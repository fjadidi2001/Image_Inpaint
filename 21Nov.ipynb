{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fjadidi2001/Image_Inpaint/blob/main/21Nov.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N8eos1QbzvIu",
        "outputId": "fc5841d5-a1d3-4881-9415-5e52dc023f16"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Nov 21 09:18:12 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   42C    P8               9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "dataset_path=kagglehub.dataset_download(\"badasstechie/celebahq-resized-256x256\")\n",
        "\n",
        "!ls {dataset_path}/celeba_hq_256 | head\n",
        "\n",
        "dataset_path=f'{dataset_path}/celeba_hq_256'"
      ],
      "metadata": {
        "id": "A1EFp-mnWfEg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "327e65c3-3a49-4ecd-da9e-c5471f270152"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/badasstechie/celebahq-resized-256x256?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 283M/283M [00:03<00:00, 75.5MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "00000.jpg\n",
            "00001.jpg\n",
            "00002.jpg\n",
            "00003.jpg\n",
            "00004.jpg\n",
            "00005.jpg\n",
            "00006.jpg\n",
            "00007.jpg\n",
            "00008.jpg\n",
            "00009.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "fzhVJLTAVOgM"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import glob\n",
        "\n",
        "\n",
        "assert(torch.cuda.is_available())\n",
        "torch.set_default_device('cpu')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = torch.device('cuda')\n",
        "IMG_WIDTH=256\n",
        "IMG_HEIGHT=256\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 0.001\n",
        "\n",
        "\n",
        "\n",
        "class InpaintingDataset(Dataset):\n",
        "    def __init__(self, image_paths, transform=None):\n",
        "        self.image_paths = image_paths\n",
        "        self.transform = transform\n",
        "\n",
        "    def create_mask(self):\n",
        "        mask = torch.ones((1, IMG_HEIGHT,IMG_WIDTH ))\n",
        "        y1, x1 = random.randint(0, IMG_HEIGHT - IMG_HEIGHT//3), random.randint(0, IMG_WIDTH - IMG_WIDTH//3)\n",
        "        h, w = IMG_HEIGHT//3, IMG_WIDTH//3\n",
        "        mask[:, y1:y1+h, x1:x1+w] = 0\n",
        "        return mask\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        img = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        mask = self.create_mask()\n",
        "        masked_img = img * mask\n",
        "\n",
        "        # print(f\"Image device: {img.device}\")\n",
        "        # print(f\"Mask device: {mask.device}\")\n",
        "        # print(f\"Masked image device: {masked_img.device}\")\n",
        "\n",
        "        return masked_img, mask, img\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(UNet, self).__init__()\n",
        "\n",
        "        # Encoder\n",
        "        self.enc1 = self.double_conv(3, 64)\n",
        "        self.enc2 = self.double_conv(64, 128)\n",
        "        self.enc3 = self.double_conv(128, 256)\n",
        "\n",
        "        # Decoder\n",
        "        self.dec3 = self.double_conv(256 + 128, 128)\n",
        "        self.dec2 = self.double_conv(128 + 64, 64)\n",
        "        self.dec1 = nn.Conv2d(64, 3, kernel_size=1)\n",
        "\n",
        "        self.pool = nn.MaxPool2d(2)\n",
        "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "\n",
        "    def double_conv(self, in_channels, out_channels):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        e1 = self.enc1(x)\n",
        "        e2 = self.enc2(self.pool(e1))\n",
        "        e3 = self.enc3(self.pool(e2))\n",
        "\n",
        "        # Decoder\n",
        "        d3 = self.dec3(torch.cat([self.upsample(e3), e2], dim=1))\n",
        "        d2 = self.dec2(torch.cat([self.upsample(d3), e1], dim=1))\n",
        "        return torch.sigmoid(self.dec1(d2))\n",
        "\n",
        "class HINT(nn.Module):\n",
        "    def __init__(self, dim=64, num_heads=4):\n",
        "        super(HINT, self).__init__()\n",
        "\n",
        "        self.input_conv = nn.Conv2d(4, dim, 3, padding=1)  # 3 channels + 1 mask channel\n",
        "\n",
        "        # Transformer blocks\n",
        "        self.transformer_blocks = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.MultiheadAttention(dim, num_heads),\n",
        "                nn.LayerNorm(dim),\n",
        "                nn.Linear(dim, dim * 2),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(dim * 2, dim),\n",
        "                nn.LayerNorm(dim)\n",
        "            ) for _ in range(3)\n",
        "        ])\n",
        "\n",
        "        self.output_conv = nn.Conv2d(dim, 3, 1)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        # Combine input and mask\n",
        "        x = torch.cat([x, mask], dim=1)\n",
        "        x = self.input_conv(x)\n",
        "\n",
        "        # Reshape for transformer\n",
        "        b, c, h, w = x.shape\n",
        "        x = x.flatten(2).permute(2, 0, 1)  # (h*w, batch, channels)\n",
        "\n",
        "        # Apply transformer blocks\n",
        "        for block in self.transformer_blocks:\n",
        "            attn, _ = block[0](x, x, x)\n",
        "            x = x + attn\n",
        "            x = block[1](x)\n",
        "            x = x + block[4](block[3](block[2](x)))\n",
        "            x = block[5](x)\n",
        "\n",
        "        # Reshape back\n",
        "        x = x.permute(1, 2, 0).view(b, c, h, w)\n",
        "        x = self.output_conv(x)\n",
        "        return torch.sigmoid(x)\n",
        "\n",
        "class XLSTM(nn.Module):\n",
        "    def __init__(self, hidden_size=64):\n",
        "        super(XLSTM, self).__init__()\n",
        "\n",
        "        self.conv_init = nn.Conv2d(3, hidden_size, 3, padding=1)\n",
        "\n",
        "        # LSTM cells for different directions\n",
        "        self.lstm_h = nn.LSTMCell(hidden_size, hidden_size)\n",
        "        self.lstm_v = nn.LSTMCell(hidden_size, hidden_size)\n",
        "        self.lstm_d = nn.LSTMCell(hidden_size, hidden_size)\n",
        "\n",
        "        self.conv_out = nn.Conv2d(hidden_size, 3, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, _, h, w = x.shape\n",
        "        hidden_size = 64\n",
        "\n",
        "        # Initial feature extraction\n",
        "        feat = self.conv_init(x)\n",
        "\n",
        "        # Initialize hidden states\n",
        "        h_state = torch.zeros(b, hidden_size, device=x.device)\n",
        "        c_state = torch.zeros(b, hidden_size, device=x.device)\n",
        "\n",
        "        # Process in different directions\n",
        "        for i in range(h):\n",
        "            for j in range(w):\n",
        "                feat_ij = feat[:, :, i, j]\n",
        "                h_state, c_state = self.lstm_h(feat_ij, (h_state, c_state))\n",
        "                feat[:, :, i, j] = h_state\n",
        "\n",
        "        # Vertical processing\n",
        "        for j in range(w):\n",
        "            for i in range(h):\n",
        "                feat_ij = feat[:, :, i, j]\n",
        "                h_state, c_state = self.lstm_v(feat_ij, (h_state, c_state))\n",
        "                feat[:, :, i, j] = h_state\n",
        "\n",
        "        # Diagonal processing\n",
        "        for k in range(h + w - 1):\n",
        "            for i in range(max(0, k-w+1), min(h, k+1)):\n",
        "                j = k - i\n",
        "                if j < w:\n",
        "                    feat_ij = feat[:, :, i, j]\n",
        "                    h_state, c_state = self.lstm_d(feat_ij, (h_state, c_state))\n",
        "                    feat[:, :, i, j] = h_state\n",
        "\n",
        "        return torch.sigmoid(self.conv_out(feat))\n",
        "\n",
        "class CombinedModel(nn.Module):\n",
        "    def __init__(self, unet, hint, xlstm):\n",
        "        super(CombinedModel, self).__init__()\n",
        "        self.unet = unet\n",
        "        self.hint = hint\n",
        "        self.xlstm = xlstm\n",
        "        self.weights = nn.Parameter(torch.ones(3))  # Learnable weights for each model\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        unet_out = self.unet(x)\n",
        "        hint_out = self.hint(x, mask)\n",
        "        xlstm_out = self.xlstm(x)\n",
        "\n",
        "        weights = torch.softmax(self.weights, dim=0)\n",
        "        combined = (weights[0] * unet_out +\n",
        "                   weights[1] * hint_out +\n",
        "                   weights[2] * xlstm_out)\n",
        "        return combined\n",
        "\n",
        "\n",
        "\n",
        "def train_model(model, train_loader, criterion, optimizer, epoch: int, model_name):\n",
        "    model.train()\n",
        "\n",
        "    epoch_loss = 0\n",
        "    for masked_imgs, masks, original_imgs in tqdm(train_loader):\n",
        "        masked_imgs = masked_imgs.to(DEVICE)\n",
        "        masks = masks.to(DEVICE)\n",
        "        original_imgs = original_imgs.to(DEVICE)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if isinstance(model, HINT):\n",
        "            outputs = model(masked_imgs, masks)\n",
        "        elif isinstance(model, (UNet, XLSTM)):\n",
        "            outputs = model(masked_imgs)\n",
        "        else:  # Combined model\n",
        "            outputs = model(masked_imgs, masks)\n",
        "\n",
        "        loss = criterion(outputs, original_imgs)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        # print(f'Loss: {loss.item():.4f}')\n",
        "\n",
        "\n",
        "    avg_loss = epoch_loss / len(train_loader)\n",
        "    print(f'Epoch {epoch}, Loss: {avg_loss:.4f}')\n",
        "\n",
        "    return avg_loss\n",
        "\n",
        "def evaluate_models(models, test_loader):\n",
        "    metrics = {\n",
        "        'PSNR': [],\n",
        "        'SSIM': [],\n",
        "        'L1_Loss': []\n",
        "    }\n",
        "\n",
        "    for name, model in models.items():\n",
        "        model.eval()\n",
        "        psnr_values = []\n",
        "        ssim_values = []\n",
        "        l1_values = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for masked_imgs, masks, original_imgs in test_loader:\n",
        "                masked_imgs = masked_imgs.to(DEVICE)\n",
        "                masks = masks.to(DEVICE)\n",
        "                original_imgs = original_imgs.to(DEVICE)\n",
        "\n",
        "                if isinstance(model, HINT):\n",
        "                    outputs = model(masked_imgs, masks)\n",
        "                elif isinstance(model, (UNet, XLSTM)):\n",
        "                    outputs = model(masked_imgs)\n",
        "                else:  # Combined model\n",
        "                    outputs = model(masked_imgs, masks)\n",
        "\n",
        "                # Calculate metrics\n",
        "                psnr = -10 * torch.log10(torch.mean((original_imgs - outputs) ** 2))\n",
        "                l1 = torch.mean(torch.abs(original_imgs - outputs))\n",
        "\n",
        "                psnr_values.append(psnr.item())\n",
        "                l1_values.append(l1.item())\n",
        "\n",
        "        metrics['PSNR'].append(np.mean(psnr_values))\n",
        "        metrics['L1_Loss'].append(np.mean(l1_values))\n",
        "\n",
        "    return pd.DataFrame(metrics, index=list(models.keys()))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BIydTXyBVyVq"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CHECKPOINT_DIR = './checkpoints'\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "\n",
        "def save_checkpoint(model, optimizer, epoch, model_name):\n",
        "    checkpoint_path = os.path.join(CHECKPOINT_DIR, f'{model_name}_epoch_{epoch}.pth')\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "    }, checkpoint_path)\n",
        "    print(f\"ckpt saved for {model_name} at epoch {epoch}.\")\n",
        "\n",
        "def load_checkpoint(model, optimizer, model_name):\n",
        "    checkpoints = [f for f in os.listdir(CHECKPOINT_DIR) if model_name in f]\n",
        "    if not checkpoints:\n",
        "        print(f\"no ckpt found for {model_name} starting from epoch 0.\")\n",
        "        return 0\n",
        "\n",
        "    latest_checkpoint = sorted(checkpoints)[-1]\n",
        "    checkpoint_path = os.path.join(CHECKPOINT_DIR, latest_checkpoint)\n",
        "    checkpoint = torch.load(checkpoint_path)\n",
        "\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    start_epoch = checkpoint['epoch'] + 1\n",
        "    print(f\"ckpt loaded for {model_name} from {latest_checkpoint}. start from epoch {start_epoch}.\")\n",
        "\n",
        "    return start_epoch"
      ],
      "metadata": {
        "id": "UhZAUPmhA74P"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up data transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((IMG_HEIGHT, IMG_WIDTH)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "    # Load dataset\n",
        "image_paths = glob.glob(f'{dataset_path}/*.jpg')\n",
        "print(len(image_paths))\n",
        "# Split dataset\n",
        "train_paths, test_paths = train_test_split(image_paths, test_size=0.9, random_state=42)\n",
        "\n",
        "\n",
        "train_dataset = InpaintingDataset(train_paths, transform=transform)\n",
        "test_dataset = InpaintingDataset(test_paths, transform=transform)\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "\n",
        "unet = UNet().to(DEVICE)\n",
        "hint = HINT().to(DEVICE)\n",
        "xlstm = XLSTM().to(DEVICE)\n",
        "combined = CombinedModel(unet, hint, xlstm).to(DEVICE)\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "models = {\n",
        "    'unet': (unet, optim.Adam(unet.parameters(), lr=LEARNING_RATE)),\n",
        "    'hint': (hint, optim.Adam(hint.parameters(), lr=LEARNING_RATE)),\n",
        "    'xlstm': (xlstm, optim.Adam(xlstm.parameters(), lr=LEARNING_RATE)),\n",
        "    'combined': (combined, optim.Adam(combined.parameters(), lr=LEARNING_RATE))\n",
        "}\n",
        "\n",
        "for model_name, (model, optimizer) in models.items():\n",
        "    print(f\"\\nTraining {model_name}\")\n",
        "    start_epoch = load_checkpoint(model, optimizer, model_name)\n",
        "\n",
        "    losses = []\n",
        "    for epoch in range(start_epoch, 5): #assuming we wanna train for 5 epochs. likely reconfigurable later\n",
        "        model.train()\n",
        "        loss = train_model(model, train_loader, criterion, optimizer, epoch=epoch, model_name=model_name)\n",
        "        losses.append(loss)\n",
        "\n",
        "        save_checkpoint(model, optimizer, epoch, model_name)\n",
        "\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.plot(losses)\n",
        "        plt.title(f'{model_name} Training Loss')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.savefig(f'{model_name}_training_loss.png')\n",
        "        plt.close()\n",
        "\n",
        "    results = evaluate_models(\n",
        "        {name: model for name, (model, _) in models.items()},\n",
        "        test_loader\n",
        "    )\n",
        "    print(\"\\nEvaluation Results:\")\n",
        "    print(results)\n"
      ],
      "metadata": {
        "id": "4PniIxL0WO0S",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        },
        "outputId": "64277739-fc98-4a46-9bfc-b73ee1464840"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "30000\n",
            "\n",
            "Training unet\n",
            "ckpt loaded for unet from unet_epoch_4.pth. start from epoch 5.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-98e75d9302d7>:21: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(checkpoint_path)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-e235b5576f82>\u001b[0m in \u001b[0;36m<cell line: 35>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     results = evaluate_models(\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0;34m{\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mtest_loader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-ccfb16765695>\u001b[0m in \u001b[0;36mevaluate_models\u001b[0;34m(models, test_loader)\u001b[0m\n\u001b[1;32m    247\u001b[0m                 \u001b[0ml1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_imgs\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m                 \u001b[0mpsnr_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpsnr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m                 \u001b[0ml1_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_device.py\u001b[0m in \u001b[0;36m__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfunc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_device_constructors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'device'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'device'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;31m# NB: This is directly called from C++ in torch/csrc/Device.cpp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset[0]"
      ],
      "metadata": {
        "id": "WLdINc5BuLJw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FJ V"
      ],
      "metadata": {
        "id": "b7lZ_QHRKkXF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import os\n",
        "import glob\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
        "\n",
        "# Ensure reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "class EnhancedInpaintingDataset(Dataset):\n",
        "    def __init__(self, image_paths, transform=None, mask_type='random'):\n",
        "        self.image_paths = image_paths\n",
        "        self.transform = transform\n",
        "        self.mask_type = mask_type\n",
        "\n",
        "    def create_mask(self, mask_type='random'):\n",
        "        mask = torch.ones((1, IMG_HEIGHT, IMG_WIDTH))\n",
        "\n",
        "        if mask_type == 'random':\n",
        "            # Random rectangular mask\n",
        "            y1, x1 = random.randint(0, IMG_HEIGHT - IMG_HEIGHT//3), random.randint(0, IMG_WIDTH - IMG_WIDTH//3)\n",
        "            h, w = IMG_HEIGHT//3, IMG_WIDTH//3\n",
        "            mask[:, y1:y1+h, x1:x1+w] = 0\n",
        "\n",
        "        elif mask_type == 'center':\n",
        "            # Center mask\n",
        "            y1 = (IMG_HEIGHT - IMG_HEIGHT//2) // 2\n",
        "            x1 = (IMG_WIDTH - IMG_WIDTH//2) // 2\n",
        "            mask[:, y1:y1+IMG_HEIGHT//2, x1:x1+IMG_WIDTH//2] = 0\n",
        "\n",
        "        elif mask_type == 'random_scattered':\n",
        "            # Multiple small random masks\n",
        "            num_masks = random.randint(3, 7)\n",
        "            for _ in range(num_masks):\n",
        "                y1, x1 = random.randint(0, IMG_HEIGHT - IMG_HEIGHT//4), random.randint(0, IMG_WIDTH - IMG_WIDTH//4)\n",
        "                h, w = IMG_HEIGHT//4, IMG_WIDTH//4\n",
        "                mask[:, y1:y1+h, x1:x1+w] = 0\n",
        "\n",
        "        return mask\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        img = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        mask = self.create_mask(self.mask_type)\n",
        "        masked_img = img * mask\n",
        "\n",
        "        return masked_img, mask, img\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "def perform_eda(dataset_path, image_paths):\n",
        "    \"\"\"\n",
        "    Perform Exploratory Data Analysis on the image dataset\n",
        "    \"\"\"\n",
        "    # Basic dataset statistics\n",
        "    print(\"Dataset EDA:\")\n",
        "    print(f\"Total number of images: {len(image_paths)}\")\n",
        "\n",
        "    # Image size distribution\n",
        "    image_sizes = []\n",
        "    for path in image_paths[:100]:  # Sample first 100 images\n",
        "        img = Image.open(path)\n",
        "        image_sizes.append(img.size)\n",
        "\n",
        "    # Plot image size distribution\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    dataset = EnhancedInpaintingDataset(image_paths, transform=None) # Assuming no transforms needed for EDA\n",
        "\n",
        "    # Width distribution\n",
        "    plt.subplot(1, 2, 1)\n",
        "    widths = [size[0] for size in image_sizes]\n",
        "    sns.histplot(widths, kde=True)\n",
        "    plt.title('Image Width Distribution')\n",
        "    plt.xlabel('Width')\n",
        "\n",
        "    # Height distribution\n",
        "    plt.subplot(1, 2, 2)\n",
        "    heights = [size[1] for size in image_sizes]\n",
        "    sns.histplot(heights, kde=True)\n",
        "    plt.title('Image Height Distribution')\n",
        "    plt.xlabel('Height')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('image_size_distribution.png')\n",
        "    plt.close()\n",
        "\n",
        "    # Visualize mask types\n",
        "    def visualize_masks(dataset):\n",
        "        plt.figure(figsize=(15, 5))\n",
        "        mask_types = ['random', 'center', 'random_scattered']\n",
        "\n",
        "        for i, mask_type in enumerate(mask_types, 1):\n",
        "            dataset.mask_type = mask_type\n",
        "            masked_img, mask, orig_img = dataset[0]\n",
        "\n",
        "            plt.subplot(1, 3, i)\n",
        "            plt.imshow(mask.squeeze().numpy(), cmap='gray')\n",
        "            plt.title(f'{mask_type.replace(\"_\", \" \").title()} Mask')\n",
        "            plt.axis('off')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('mask_types.png')\n",
        "        plt.close()\n",
        "\n",
        "    # Call mask visualization\n",
        "    visualize_masks(dataset)\n",
        "\n",
        "def enhanced_evaluate_models(models, test_loader, device):\n",
        "    \"\"\"\n",
        "    Enhanced model evaluation with multiple metrics\n",
        "    \"\"\"\n",
        "    metrics = {\n",
        "        'PSNR': [],\n",
        "        'SSIM': [],\n",
        "        'L1_Loss': [],\n",
        "        'MSE': []\n",
        "    }\n",
        "\n",
        "    for name, model in models.items():\n",
        "        model.eval()\n",
        "        model_metrics = {\n",
        "            'PSNR': [],\n",
        "            'SSIM': [],\n",
        "            'L1_Loss': [],\n",
        "            'MSE': []\n",
        "        }\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for masked_imgs, masks, original_imgs in test_loader:\n",
        "                masked_imgs = masked_imgs.to(device)\n",
        "                masks = masks.to(device)\n",
        "                original_imgs = original_imgs.to(device)\n",
        "\n",
        "                # Predict\n",
        "                if 'hint' in name.lower():\n",
        "                    outputs = model(masked_imgs, masks)\n",
        "                elif 'unet' in name.lower() or 'xlstm' in name.lower():\n",
        "                    outputs = model(masked_imgs)\n",
        "                else:  # Combined model\n",
        "                    outputs = model(masked_imgs, masks)\n",
        "\n",
        "                # Convert to numpy for sklearn metrics\n",
        "                original_np = original_imgs.cpu().numpy()\n",
        "                outputs_np = outputs.cpu().numpy()\n",
        "\n",
        "                for i in range(len(original_np)):\n",
        "                    # Compute metrics\n",
        "                    model_metrics['PSNR'].append(\n",
        "                        psnr(original_np[i].transpose(1,2,0),\n",
        "                             outputs_np[i].transpose(1,2,0),\n",
        "                             data_range=1.0)\n",
        "                    )\n",
        "\n",
        "                    model_metrics['SSIM'].append(\n",
        "                        ssim(original_np[i].transpose(1,2,0),\n",
        "                             outputs_np[i].transpose(1,2,0),\n",
        "                             multichannel=True, data_range=1.0)\n",
        "                    )\n",
        "\n",
        "                    # Compute losses\n",
        "                    model_metrics['L1_Loss'].append(\n",
        "                        np.mean(np.abs(original_np[i] - outputs_np[i]))\n",
        "                    )\n",
        "                    model_metrics['MSE'].append(\n",
        "                        np.mean((original_np[i] - outputs_np[i]) ** 2)\n",
        "                    )\n",
        "\n",
        "        # Aggregate metrics\n",
        "        metrics['PSNR'].append(np.mean(model_metrics['PSNR']))\n",
        "        metrics['SSIM'].append(np.mean(model_metrics['SSIM']))\n",
        "        metrics['L1_Loss'].append(np.mean(model_metrics['L1_Loss']))\n",
        "        metrics['MSE'].append(np.mean(model_metrics['MSE']))\n",
        "\n",
        "    # Create DataFrame\n",
        "    results_df = pd.DataFrame(metrics, index=list(models.keys()))\n",
        "\n",
        "    # Visualization of metrics\n",
        "    plt.figure(figsize=(15, 5))\n",
        "\n",
        "    # PSNR\n",
        "    plt.subplot(1, 3, 1)\n",
        "    results_df['PSNR'].plot(kind='bar')\n",
        "    plt.title('PSNR by Model')\n",
        "    plt.ylabel('PSNR (dB)')\n",
        "\n",
        "    # SSIM\n",
        "    plt.subplot(1, 3, 2)\n",
        "    results_df['SSIM'].plot(kind='bar')\n",
        "    plt.title('SSIM by Model')\n",
        "    plt.ylabel('SSIM')\n",
        "\n",
        "    # L1 Loss\n",
        "    plt.subplot(1, 3, 3)\n",
        "    results_df['L1_Loss'].plot(kind='bar')\n",
        "    plt.title('L1 Loss by Model')\n",
        "    plt.ylabel('L1 Loss')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('model_performance_comparison.png')\n",
        "    plt.close()\n",
        "\n",
        "    return results_df\n",
        "\n",
        "def save_model_checkpoint(model, optimizer, epoch, model_name, checkpoint_dir='./model_checkpoints'):\n",
        "    \"\"\"\n",
        "    Save model checkpoint with error handling and directory creation\n",
        "    \"\"\"\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "    checkpoint_path = os.path.join(\n",
        "        checkpoint_dir,\n",
        "        f'{model_name}_epoch_{epoch}_checkpoint.pth'\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "        }, checkpoint_path)\n",
        "        print(f\"Checkpoint saved for {model_name} at epoch {epoch}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving checkpoint for {model_name}: {e}\")\n",
        "\n",
        "def main():\n",
        "    # Configuration\n",
        "    IMG_WIDTH = 256\n",
        "    IMG_HEIGHT = 256\n",
        "    BATCH_SIZE = 32\n",
        "    LEARNING_RATE = 0.001\n",
        "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # Dataset path and EDA\n",
        "    dataset_path = kagglehub.dataset_download(\"badasstechie/celebahq-resized-256x256\")\n",
        "    dataset_path = f'{dataset_path}/celeba_hq_256'\n",
        "\n",
        "    # Image paths\n",
        "    image_paths = glob.glob(f'{dataset_path}/*.jpg')\n",
        "\n",
        "    # Perform Exploratory Data Analysis\n",
        "    perform_eda(dataset_path, image_paths)\n",
        "\n",
        "    # Rest of the training logic remains similar to your original code...\n",
        "    # (Include data loading, model creation, training loop, etc.)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "9BIU9jp6Kl5_",
        "outputId": "b9249c9c-8e2c-471e-9064-01ab450e007f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset EDA:\n",
            "Total number of images: 30000\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'IMG_HEIGHT' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-7690fbe25ce5>\u001b[0m in \u001b[0;36m<cell line: 269>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-2-7690fbe25ce5>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[0;31m# Perform Exploratory Data Analysis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m     \u001b[0mperform_eda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0;31m# Rest of the training logic remains similar to your original code...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-7690fbe25ce5>\u001b[0m in \u001b[0;36mperform_eda\u001b[0;34m(dataset_path, image_paths)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;31m# Call mask visualization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m     \u001b[0mvisualize_masks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0menhanced_evaluate_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-7690fbe25ce5>\u001b[0m in \u001b[0;36mvisualize_masks\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0mmasked_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-7690fbe25ce5>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0mmasked_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-7690fbe25ce5>\u001b[0m in \u001b[0;36mcreate_mask\u001b[0;34m(self, mask_type)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreate_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'random'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIMG_HEIGHT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIMG_WIDTH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'random'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'IMG_HEIGHT' is not defined"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x500 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}