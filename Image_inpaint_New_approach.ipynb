{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fjadidi2001/Image_Inpaint/blob/main/Image_inpaint_New_approach.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fsspec==2024.10.0\n",
        "!pip install s3fs==2024.10.0\n",
        "!pip install --upgrade fsspec gcsfs\n",
        "!pip install --upgrade pip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KcMV3pt8yy3k",
        "outputId": "3d544ed6-9f9a-41f3-e4e8-813c5aa0bd99"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fsspec==2024.10.0 in /usr/local/lib/python3.10/dist-packages (2024.10.0)\n",
            "Requirement already satisfied: s3fs==2024.10.0 in /usr/local/lib/python3.10/dist-packages (2024.10.0)\n",
            "Requirement already satisfied: aiobotocore<3.0.0,>=2.5.4 in /usr/local/lib/python3.10/dist-packages (from s3fs==2024.10.0) (2.15.2)\n",
            "Requirement already satisfied: fsspec==2024.10.0.* in /usr/local/lib/python3.10/dist-packages (from s3fs==2024.10.0) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from s3fs==2024.10.0) (3.11.8)\n",
            "Requirement already satisfied: botocore<1.35.37,>=1.35.16 in /usr/local/lib/python3.10/dist-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs==2024.10.0) (1.35.36)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.10.10 in /usr/local/lib/python3.10/dist-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs==2024.10.0) (1.16.0)\n",
            "Requirement already satisfied: aioitertools<1.0.0,>=0.5.1 in /usr/local/lib/python3.10/dist-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs==2024.10.0) (0.12.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs==2024.10.0) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs==2024.10.0) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs==2024.10.0) (5.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs==2024.10.0) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs==2024.10.0) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs==2024.10.0) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs==2024.10.0) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs==2024.10.0) (1.18.0)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.35.37,>=1.35.16->aiobotocore<3.0.0,>=2.5.4->s3fs==2024.10.0) (1.0.1)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.35.37,>=1.35.16->aiobotocore<3.0.0,>=2.5.4->s3fs==2024.10.0) (2.8.2)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.10/dist-packages (from botocore<1.35.37,>=1.35.16->aiobotocore<3.0.0,>=2.5.4->s3fs==2024.10.0) (2.2.3)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp!=4.0.0a0,!=4.0.0a1->s3fs==2024.10.0) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->s3fs==2024.10.0) (3.10)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.35.37,>=1.35.16->aiobotocore<3.0.0,>=2.5.4->s3fs==2024.10.0) (1.16.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (2024.10.0)\n",
            "Requirement already satisfied: gcsfs in /usr/local/lib/python3.10/dist-packages (2024.10.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from gcsfs) (3.11.8)\n",
            "Requirement already satisfied: decorator>4.1.2 in /usr/local/lib/python3.10/dist-packages (from gcsfs) (5.1.1)\n",
            "Requirement already satisfied: google-auth>=1.2 in /usr/local/lib/python3.10/dist-packages (from gcsfs) (2.36.0)\n",
            "Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.10/dist-packages (from gcsfs) (1.2.1)\n",
            "Requirement already satisfied: google-cloud-storage in /usr/local/lib/python3.10/dist-packages (from gcsfs) (2.18.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from gcsfs) (2.32.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (5.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (1.18.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.2->gcsfs) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.2->gcsfs) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.2->gcsfs) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib->gcsfs) (2.0.0)\n",
            "Requirement already satisfied: google-api-core<3.0.0dev,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage->gcsfs) (2.23.0)\n",
            "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage->gcsfs) (2.4.1)\n",
            "Requirement already satisfied: google-resumable-media>=2.7.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage->gcsfs) (2.7.2)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage->gcsfs) (1.6.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->gcsfs) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->gcsfs) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->gcsfs) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->gcsfs) (2024.8.30)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage->gcsfs) (1.66.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage->gcsfs) (5.29.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage->gcsfs) (1.25.0)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (4.12.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.2->gcsfs) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs) (3.2.2)\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (24.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52fT7Egh0PD2",
        "outputId": "f0fd3405-645f-40ca-e387-fd53289e3e51"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Package                            Version\n",
            "---------------------------------- -------------------\n",
            "absl-py                            1.4.0\n",
            "accelerate                         1.1.1\n",
            "aiobotocore                        2.15.2\n",
            "aiohappyeyeballs                   2.4.3\n",
            "aiohttp                            3.11.8\n",
            "aioitertools                       0.12.0\n",
            "aiosignal                          1.3.1\n",
            "alabaster                          1.0.0\n",
            "albucore                           0.0.19\n",
            "albumentations                     1.4.20\n",
            "altair                             4.2.2\n",
            "annotated-types                    0.7.0\n",
            "anyio                              3.7.1\n",
            "argon2-cffi                        23.1.0\n",
            "argon2-cffi-bindings               21.2.0\n",
            "array_record                       0.5.1\n",
            "arviz                              0.20.0\n",
            "astropy                            6.1.6\n",
            "astropy-iers-data                  0.2024.11.18.0.35.2\n",
            "astunparse                         1.6.3\n",
            "async-timeout                      5.0.1\n",
            "atpublic                           4.1.0\n",
            "attrs                              24.2.0\n",
            "audioread                          3.0.1\n",
            "autograd                           1.7.0\n",
            "babel                              2.16.0\n",
            "backcall                           0.2.0\n",
            "beautifulsoup4                     4.12.3\n",
            "bigframes                          1.27.0\n",
            "bigquery-magics                    0.4.0\n",
            "bleach                             6.2.0\n",
            "blinker                            1.9.0\n",
            "blis                               0.7.11\n",
            "blosc2                             2.7.1\n",
            "bokeh                              3.6.1\n",
            "botocore                           1.35.36\n",
            "Bottleneck                         1.4.2\n",
            "bqplot                             0.12.43\n",
            "branca                             0.8.0\n",
            "CacheControl                       0.14.1\n",
            "cachetools                         5.5.0\n",
            "catalogue                          2.0.10\n",
            "certifi                            2024.8.30\n",
            "cffi                               1.17.1\n",
            "chardet                            5.2.0\n",
            "charset-normalizer                 3.4.0\n",
            "chex                               0.1.87\n",
            "clarabel                           0.9.0\n",
            "click                              8.1.7\n",
            "cloudpathlib                       0.20.0\n",
            "cloudpickle                        3.1.0\n",
            "cmake                              3.30.5\n",
            "cmdstanpy                          1.2.4\n",
            "colorcet                           3.1.0\n",
            "colorlover                         0.3.0\n",
            "colour                             0.1.5\n",
            "community                          1.0.0b1\n",
            "confection                         0.1.5\n",
            "cons                               0.4.6\n",
            "contourpy                          1.3.1\n",
            "cryptography                       43.0.3\n",
            "cuda-python                        12.2.1\n",
            "cudf-cu12                          24.10.1\n",
            "cufflinks                          0.17.3\n",
            "cupy-cuda12x                       12.2.0\n",
            "cvxopt                             1.3.2\n",
            "cvxpy                              1.5.4\n",
            "cycler                             0.12.1\n",
            "cymem                              2.0.8\n",
            "Cython                             3.0.11\n",
            "dask                               2024.10.0\n",
            "datascience                        0.17.6\n",
            "datasets                           3.1.0\n",
            "db-dtypes                          1.3.1\n",
            "dbus-python                        1.2.18\n",
            "debugpy                            1.8.0\n",
            "decorator                          5.1.1\n",
            "defusedxml                         0.7.1\n",
            "Deprecated                         1.2.15\n",
            "diffusers                          0.31.0\n",
            "dill                               0.3.8\n",
            "distro                             1.9.0\n",
            "dlib                               19.24.2\n",
            "dm-tree                            0.1.8\n",
            "docker-pycreds                     0.4.0\n",
            "docstring_parser                   0.16\n",
            "docutils                           0.21.2\n",
            "dopamine_rl                        4.0.9\n",
            "duckdb                             1.1.3\n",
            "earthengine-api                    1.2.0\n",
            "easydict                           1.13\n",
            "ecos                               2.0.14\n",
            "editdistance                       0.8.1\n",
            "eerepr                             0.0.4\n",
            "einops                             0.8.0\n",
            "en-core-web-sm                     3.7.1\n",
            "entrypoints                        0.4\n",
            "et_xmlfile                         2.0.0\n",
            "etils                              1.10.0\n",
            "etuples                            0.3.9\n",
            "eval_type_backport                 0.2.0\n",
            "exceptiongroup                     1.2.2\n",
            "fastai                             2.7.18\n",
            "fastcore                           1.7.20\n",
            "fastdownload                       0.0.7\n",
            "fastjsonschema                     2.20.0\n",
            "fastprogress                       1.0.3\n",
            "fastrlock                          0.8.2\n",
            "filelock                           3.16.1\n",
            "firebase-admin                     6.5.0\n",
            "Flask                              3.0.3\n",
            "flatbuffers                        24.3.25\n",
            "flax                               0.8.5\n",
            "folium                             0.18.0\n",
            "fonttools                          4.55.0\n",
            "frozendict                         2.4.6\n",
            "frozenlist                         1.5.0\n",
            "fsspec                             2024.9.0\n",
            "future                             1.0.0\n",
            "gast                               0.6.0\n",
            "gcsfs                              2024.10.0\n",
            "GDAL                               3.6.4\n",
            "gdown                              5.2.0\n",
            "geemap                             0.35.1\n",
            "gensim                             4.3.3\n",
            "geocoder                           1.38.1\n",
            "geographiclib                      2.0\n",
            "geopandas                          1.0.1\n",
            "geopy                              2.4.1\n",
            "gin-config                         0.5.0\n",
            "gitdb                              4.0.11\n",
            "GitPython                          3.1.43\n",
            "glob2                              0.7\n",
            "google                             2.0.3\n",
            "google-ai-generativelanguage       0.6.10\n",
            "google-api-core                    2.23.0\n",
            "google-api-python-client           2.151.0\n",
            "google-auth                        2.36.0\n",
            "google-auth-httplib2               0.2.0\n",
            "google-auth-oauthlib               1.2.1\n",
            "google-cloud-aiplatform            1.71.1\n",
            "google-cloud-bigquery              3.25.0\n",
            "google-cloud-bigquery-connection   1.16.1\n",
            "google-cloud-bigquery-storage      2.27.0\n",
            "google-cloud-bigtable              2.27.0\n",
            "google-cloud-core                  2.4.1\n",
            "google-cloud-datastore             2.20.1\n",
            "google-cloud-firestore             2.19.0\n",
            "google-cloud-functions             1.18.1\n",
            "google-cloud-iam                   2.16.1\n",
            "google-cloud-language              2.15.1\n",
            "google-cloud-pubsub                2.27.1\n",
            "google-cloud-resource-manager      1.13.1\n",
            "google-cloud-storage               2.18.2\n",
            "google-cloud-translate             3.17.0\n",
            "google-colab                       1.0.0\n",
            "google-crc32c                      1.6.0\n",
            "google-generativeai                0.8.3\n",
            "google-pasta                       0.2.0\n",
            "google-resumable-media             2.7.2\n",
            "googleapis-common-protos           1.66.0\n",
            "googledrivedownloader              0.4\n",
            "graphviz                           0.20.3\n",
            "greenlet                           3.1.1\n",
            "grpc-google-iam-v1                 0.13.1\n",
            "grpcio                             1.68.0\n",
            "grpcio-status                      1.62.3\n",
            "gspread                            6.0.2\n",
            "gspread-dataframe                  3.3.1\n",
            "gym                                0.25.2\n",
            "gym-notices                        0.0.8\n",
            "h11                                0.14.0\n",
            "h5netcdf                           1.4.1\n",
            "h5py                               3.12.1\n",
            "holidays                           0.61\n",
            "holoviews                          1.20.0\n",
            "html5lib                           1.1\n",
            "httpcore                           1.0.7\n",
            "httpimport                         1.4.0\n",
            "httplib2                           0.22.0\n",
            "httpx                              0.27.2\n",
            "huggingface-hub                    0.26.2\n",
            "humanize                           4.11.0\n",
            "hyperopt                           0.2.7\n",
            "ibis-framework                     9.2.0\n",
            "idna                               3.10\n",
            "imageio                            2.36.0\n",
            "imageio-ffmpeg                     0.5.1\n",
            "imagesize                          1.4.1\n",
            "imbalanced-learn                   0.12.4\n",
            "imgaug                             0.4.0\n",
            "immutabledict                      4.2.1\n",
            "importlib_metadata                 8.5.0\n",
            "importlib_resources                6.4.5\n",
            "imutils                            0.5.4\n",
            "inflect                            7.4.0\n",
            "iniconfig                          2.0.0\n",
            "intel-cmplr-lib-ur                 2025.0.0\n",
            "intel-openmp                       2025.0.0\n",
            "ipyevents                          2.0.2\n",
            "ipyfilechooser                     0.6.0\n",
            "ipykernel                          5.5.6\n",
            "ipyleaflet                         0.19.2\n",
            "ipyparallel                        8.8.0\n",
            "ipython                            7.34.0\n",
            "ipython-genutils                   0.2.0\n",
            "ipython-sql                        0.5.0\n",
            "ipytree                            0.2.2\n",
            "ipywidgets                         7.7.1\n",
            "itsdangerous                       2.2.0\n",
            "jax                                0.4.33\n",
            "jax-cuda12-pjrt                    0.4.33\n",
            "jax-cuda12-plugin                  0.4.33\n",
            "jaxlib                             0.4.33\n",
            "jeepney                            0.7.1\n",
            "jellyfish                          1.1.0\n",
            "jieba                              0.42.1\n",
            "Jinja2                             3.1.4\n",
            "jiter                              0.7.1\n",
            "jmespath                           1.0.1\n",
            "joblib                             1.4.2\n",
            "jsonpatch                          1.33\n",
            "jsonpickle                         4.0.0\n",
            "jsonpointer                        3.0.0\n",
            "jsonschema                         4.23.0\n",
            "jsonschema-specifications          2024.10.1\n",
            "jupyter-client                     6.1.12\n",
            "jupyter-console                    6.1.0\n",
            "jupyter_core                       5.7.2\n",
            "jupyter-leaflet                    0.19.2\n",
            "jupyter-server                     1.24.0\n",
            "jupyterlab_pygments                0.3.0\n",
            "jupyterlab_widgets                 3.0.13\n",
            "kaggle                             1.6.17\n",
            "kagglehub                          0.3.4\n",
            "keras                              3.5.0\n",
            "keyring                            23.5.0\n",
            "kiwisolver                         1.4.7\n",
            "langchain                          0.3.7\n",
            "langchain-core                     0.3.19\n",
            "langchain-text-splitters           0.3.2\n",
            "langcodes                          3.4.1\n",
            "langsmith                          0.1.143\n",
            "language_data                      1.2.0\n",
            "launchpadlib                       1.10.16\n",
            "lazr.restfulclient                 0.14.4\n",
            "lazr.uri                           1.0.6\n",
            "lazy_loader                        0.4\n",
            "libclang                           18.1.1\n",
            "libcudf-cu12                       24.10.1\n",
            "librosa                            0.10.2.post1\n",
            "lightgbm                           4.5.0\n",
            "linkify-it-py                      2.0.3\n",
            "llvmlite                           0.43.0\n",
            "locket                             1.0.0\n",
            "logical-unification                0.4.6\n",
            "lxml                               5.3.0\n",
            "marisa-trie                        1.2.1\n",
            "Markdown                           3.7\n",
            "markdown-it-py                     3.0.0\n",
            "MarkupSafe                         3.0.2\n",
            "matplotlib                         3.8.0\n",
            "matplotlib-inline                  0.1.7\n",
            "matplotlib-venn                    1.1.1\n",
            "mdit-py-plugins                    0.4.2\n",
            "mdurl                              0.1.2\n",
            "miniKanren                         1.0.3\n",
            "missingno                          0.5.2\n",
            "mistune                            3.0.2\n",
            "mizani                             0.13.0\n",
            "mkl                                2025.0.0\n",
            "ml-dtypes                          0.4.1\n",
            "mlxtend                            0.23.3\n",
            "more-itertools                     10.5.0\n",
            "moviepy                            1.0.3\n",
            "mpmath                             1.3.0\n",
            "msgpack                            1.1.0\n",
            "multidict                          6.1.0\n",
            "multipledispatch                   1.0.0\n",
            "multiprocess                       0.70.16\n",
            "multitasking                       0.0.11\n",
            "murmurhash                         1.0.10\n",
            "music21                            9.3.0\n",
            "namex                              0.0.8\n",
            "natsort                            8.4.0\n",
            "nbclassic                          1.1.0\n",
            "nbclient                           0.10.0\n",
            "nbconvert                          7.16.4\n",
            "nbformat                           5.10.4\n",
            "ndindex                            1.9.2\n",
            "nest-asyncio                       1.6.0\n",
            "networkx                           3.4.2\n",
            "nibabel                            5.3.2\n",
            "nltk                               3.9.1\n",
            "notebook                           6.5.5\n",
            "notebook_shim                      0.2.4\n",
            "numba                              0.60.0\n",
            "numexpr                            2.10.1\n",
            "numpy                              1.26.4\n",
            "nvidia-cublas-cu12                 12.6.3.3\n",
            "nvidia-cuda-cupti-cu12             12.6.80\n",
            "nvidia-cuda-nvcc-cu12              12.6.77\n",
            "nvidia-cuda-runtime-cu12           12.6.77\n",
            "nvidia-cudnn-cu12                  9.5.1.17\n",
            "nvidia-cufft-cu12                  11.3.0.4\n",
            "nvidia-curand-cu12                 10.3.7.77\n",
            "nvidia-cusolver-cu12               11.7.1.2\n",
            "nvidia-cusparse-cu12               12.5.4.2\n",
            "nvidia-nccl-cu12                   2.23.4\n",
            "nvidia-nvjitlink-cu12              12.6.77\n",
            "nvtx                               0.2.10\n",
            "nx-cugraph-cu12                    24.10.0\n",
            "oauth2client                       4.1.3\n",
            "oauthlib                           3.2.2\n",
            "openai                             1.54.4\n",
            "opencv-contrib-python              4.10.0.84\n",
            "opencv-python                      4.10.0.84\n",
            "opencv-python-headless             4.10.0.84\n",
            "openpyxl                           3.1.5\n",
            "opentelemetry-api                  1.28.2\n",
            "opentelemetry-sdk                  1.28.2\n",
            "opentelemetry-semantic-conventions 0.49b2\n",
            "opt_einsum                         3.4.0\n",
            "optax                              0.2.4\n",
            "optree                             0.13.1\n",
            "orbax-checkpoint                   0.6.4\n",
            "orjson                             3.10.11\n",
            "osqp                               0.6.7.post3\n",
            "packaging                          24.2\n",
            "pandas                             2.2.2\n",
            "pandas-datareader                  0.10.0\n",
            "pandas-gbq                         0.24.0\n",
            "pandas-stubs                       2.2.2.240909\n",
            "pandocfilters                      1.5.1\n",
            "panel                              1.5.4\n",
            "param                              2.1.1\n",
            "parso                              0.8.4\n",
            "parsy                              2.1\n",
            "partd                              1.4.2\n",
            "pathlib                            1.0.1\n",
            "patsy                              1.0.1\n",
            "peewee                             3.17.8\n",
            "peft                               0.13.2\n",
            "pexpect                            4.9.0\n",
            "pickleshare                        0.7.5\n",
            "pillow                             11.0.0\n",
            "pip                                24.3.1\n",
            "platformdirs                       4.3.6\n",
            "plotly                             5.24.1\n",
            "plotnine                           0.14.1\n",
            "pluggy                             1.5.0\n",
            "polars                             1.9.0\n",
            "pooch                              1.8.2\n",
            "portpicker                         1.5.2\n",
            "preshed                            3.0.9\n",
            "prettytable                        3.12.0\n",
            "proglog                            0.1.10\n",
            "progressbar2                       4.5.0\n",
            "prometheus_client                  0.21.0\n",
            "promise                            2.3\n",
            "prompt_toolkit                     3.0.48\n",
            "propcache                          0.2.0\n",
            "prophet                            1.1.6\n",
            "proto-plus                         1.25.0\n",
            "protobuf                           5.29.0\n",
            "psutil                             5.9.5\n",
            "psycopg2                           2.9.10\n",
            "ptyprocess                         0.7.0\n",
            "py-cpuinfo                         9.0.0\n",
            "py4j                               0.10.9.7\n",
            "pyarrow                            17.0.0\n",
            "pyarrow-hotfix                     0.6\n",
            "pyasn1                             0.6.1\n",
            "pyasn1_modules                     0.4.1\n",
            "pycocotools                        2.0.8\n",
            "pycparser                          2.22\n",
            "pydantic                           2.9.2\n",
            "pydantic_core                      2.23.4\n",
            "pydata-google-auth                 1.8.2\n",
            "pydot                              3.0.2\n",
            "pydotplus                          2.0.2\n",
            "PyDrive                            1.3.1\n",
            "PyDrive2                           1.21.1\n",
            "pyerfa                             2.0.1.5\n",
            "pygame                             2.6.1\n",
            "pygit2                             1.16.0\n",
            "Pygments                           2.18.0\n",
            "PyGObject                          3.42.1\n",
            "PyJWT                              2.10.0\n",
            "pylibcudf-cu12                     24.10.1\n",
            "pylibcugraph-cu12                  24.10.0\n",
            "pylibraft-cu12                     24.10.0\n",
            "pymc                               5.18.2\n",
            "pymystem3                          0.2.0\n",
            "pynvjitlink-cu12                   0.4.0\n",
            "pyogrio                            0.10.0\n",
            "PyOpenGL                           3.1.7\n",
            "pyOpenSSL                          24.2.1\n",
            "pyparsing                          3.2.0\n",
            "pyperclip                          1.9.0\n",
            "pyproj                             3.7.0\n",
            "pyshp                              2.3.1\n",
            "PySocks                            1.7.1\n",
            "pyspark                            3.5.3\n",
            "pytensor                           2.26.3\n",
            "pytest                             8.3.3\n",
            "python-apt                         0.0.0\n",
            "python-box                         7.2.0\n",
            "python-dateutil                    2.8.2\n",
            "python-louvain                     0.16\n",
            "python-slugify                     8.0.4\n",
            "python-utils                       3.9.0\n",
            "pytz                               2024.2\n",
            "pyviz_comms                        3.0.3\n",
            "PyYAML                             6.0.2\n",
            "pyzmq                              24.0.1\n",
            "qdldl                              0.1.7.post4\n",
            "ratelim                            0.1.6\n",
            "referencing                        0.35.1\n",
            "regex                              2024.9.11\n",
            "requests                           2.32.3\n",
            "requests-oauthlib                  2.0.0\n",
            "requests-toolbelt                  1.0.0\n",
            "requirements-parser                0.9.0\n",
            "rich                               13.9.4\n",
            "rmm-cu12                           24.10.0\n",
            "rpds-py                            0.21.0\n",
            "rpy2                               3.4.2\n",
            "rsa                                4.9\n",
            "s3fs                               2024.10.0\n",
            "safetensors                        0.4.5\n",
            "scikit-image                       0.24.0\n",
            "scikit-learn                       1.5.2\n",
            "scipy                              1.13.1\n",
            "scooby                             0.10.0\n",
            "scs                                3.2.7\n",
            "seaborn                            0.13.2\n",
            "SecretStorage                      3.3.1\n",
            "Send2Trash                         1.8.3\n",
            "sentence-transformers              3.2.1\n",
            "sentencepiece                      0.2.0\n",
            "sentry-sdk                         2.18.0\n",
            "setproctitle                       1.3.4\n",
            "setuptools                         75.1.0\n",
            "shap                               0.46.0\n",
            "shapely                            2.0.6\n",
            "shellingham                        1.5.4\n",
            "simple-parsing                     0.1.6\n",
            "six                                1.16.0\n",
            "sklearn-pandas                     2.2.0\n",
            "slicer                             0.0.8\n",
            "smart-open                         7.0.5\n",
            "smmap                              5.0.1\n",
            "sniffio                            1.3.1\n",
            "snowballstemmer                    2.2.0\n",
            "soundfile                          0.12.1\n",
            "soupsieve                          2.6\n",
            "soxr                               0.5.0.post1\n",
            "spacy                              3.7.5\n",
            "spacy-legacy                       3.0.12\n",
            "spacy-loggers                      1.0.5\n",
            "Sphinx                             8.1.3\n",
            "sphinxcontrib-applehelp            2.0.0\n",
            "sphinxcontrib-devhelp              2.0.0\n",
            "sphinxcontrib-htmlhelp             2.1.0\n",
            "sphinxcontrib-jsmath               1.0.1\n",
            "sphinxcontrib-qthelp               2.0.0\n",
            "sphinxcontrib-serializinghtml      2.0.0\n",
            "SQLAlchemy                         2.0.36\n",
            "sqlglot                            25.1.0\n",
            "sqlparse                           0.5.2\n",
            "srsly                              2.4.8\n",
            "stanio                             0.5.1\n",
            "statsmodels                        0.14.4\n",
            "StrEnum                            0.4.15\n",
            "stringzilla                        3.10.10\n",
            "sympy                              1.13.1\n",
            "tables                             3.10.1\n",
            "tabulate                           0.9.0\n",
            "tbb                                2022.0.0\n",
            "tcmlib                             1.2.0\n",
            "tenacity                           9.0.0\n",
            "tensorboard                        2.17.1\n",
            "tensorboard-data-server            0.7.2\n",
            "tensorflow                         2.17.1\n",
            "tensorflow-datasets                4.9.7\n",
            "tensorflow-hub                     0.16.1\n",
            "tensorflow-io-gcs-filesystem       0.37.1\n",
            "tensorflow-metadata                1.13.1\n",
            "tensorflow-probability             0.24.0\n",
            "tensorstore                        0.1.68\n",
            "termcolor                          2.5.0\n",
            "terminado                          0.18.1\n",
            "text-unidecode                     1.3\n",
            "textblob                           0.17.1\n",
            "tf_keras                           2.17.0\n",
            "tf-slim                            1.1.0\n",
            "thinc                              8.2.5\n",
            "threadpoolctl                      3.5.0\n",
            "tifffile                           2024.9.20\n",
            "timm                               1.0.11\n",
            "tinycss2                           1.4.0\n",
            "tokenizers                         0.20.3\n",
            "toml                               0.10.2\n",
            "tomli                              2.1.0\n",
            "toolz                              0.12.1\n",
            "torch                              2.5.1+cu121\n",
            "torchaudio                         2.5.1+cu121\n",
            "torchsummary                       1.5.1\n",
            "torchvision                        0.20.1+cu121\n",
            "tornado                            6.3.3\n",
            "tqdm                               4.66.6\n",
            "traitlets                          5.7.1\n",
            "traittypes                         0.2.1\n",
            "transformers                       4.46.2\n",
            "tweepy                             4.14.0\n",
            "typeguard                          4.4.1\n",
            "typer                              0.13.0\n",
            "types-pytz                         2024.2.0.20241003\n",
            "types-setuptools                   75.5.0.20241122\n",
            "typing_extensions                  4.12.2\n",
            "tzdata                             2024.2\n",
            "tzlocal                            5.2\n",
            "uc-micro-py                        1.0.3\n",
            "umf                                0.9.0\n",
            "uritemplate                        4.1.1\n",
            "urllib3                            2.2.3\n",
            "vega-datasets                      0.9.0\n",
            "wadllib                            1.3.6\n",
            "wandb                              0.18.7\n",
            "wasabi                             1.1.3\n",
            "wcwidth                            0.2.13\n",
            "weasel                             0.4.1\n",
            "webcolors                          24.11.1\n",
            "webencodings                       0.5.1\n",
            "websocket-client                   1.8.0\n",
            "Werkzeug                           3.1.3\n",
            "wheel                              0.45.0\n",
            "widgetsnbextension                 3.6.10\n",
            "wordcloud                          1.9.4\n",
            "wrapt                              1.16.0\n",
            "xarray                             2024.10.0\n",
            "xarray-einstats                    0.8.0\n",
            "xgboost                            2.1.2\n",
            "xlrd                               2.0.1\n",
            "xxhash                             3.5.0\n",
            "xyzservices                        2024.9.0\n",
            "yarl                               1.18.0\n",
            "yellowbrick                        1.5\n",
            "yfinance                           0.2.49\n",
            "zipp                               3.21.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall gcsfs s3fs fsspec -y\n",
        "!pip install gcsfs s3fs fsspec"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aMMPfitd0hUU",
        "outputId": "ad6f89ce-c42d-40c8-8046-512c11c30927"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: gcsfs 2024.10.0\n",
            "Uninstalling gcsfs-2024.10.0:\n",
            "  Successfully uninstalled gcsfs-2024.10.0\n",
            "Found existing installation: s3fs 2024.10.0\n",
            "Uninstalling s3fs-2024.10.0:\n",
            "  Successfully uninstalled s3fs-2024.10.0\n",
            "Found existing installation: fsspec 2024.9.0\n",
            "Uninstalling fsspec-2024.9.0:\n",
            "  Successfully uninstalled fsspec-2024.9.0\n",
            "Collecting gcsfs\n",
            "  Using cached gcsfs-2024.10.0-py2.py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting s3fs\n",
            "  Using cached s3fs-2024.10.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting fsspec\n",
            "  Using cached fsspec-2024.10.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from gcsfs) (3.11.8)\n",
            "Requirement already satisfied: decorator>4.1.2 in /usr/local/lib/python3.10/dist-packages (from gcsfs) (5.1.1)\n",
            "Requirement already satisfied: google-auth>=1.2 in /usr/local/lib/python3.10/dist-packages (from gcsfs) (2.36.0)\n",
            "Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.10/dist-packages (from gcsfs) (1.2.1)\n",
            "Requirement already satisfied: google-cloud-storage in /usr/local/lib/python3.10/dist-packages (from gcsfs) (2.18.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from gcsfs) (2.32.3)\n",
            "Requirement already satisfied: aiobotocore<3.0.0,>=2.5.4 in /usr/local/lib/python3.10/dist-packages (from s3fs) (2.15.2)\n",
            "Requirement already satisfied: botocore<1.35.37,>=1.35.16 in /usr/local/lib/python3.10/dist-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs) (1.35.36)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.10.10 in /usr/local/lib/python3.10/dist-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs) (1.16.0)\n",
            "Requirement already satisfied: aioitertools<1.0.0,>=0.5.1 in /usr/local/lib/python3.10/dist-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs) (0.12.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (5.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (1.18.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.2->gcsfs) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.2->gcsfs) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.2->gcsfs) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib->gcsfs) (2.0.0)\n",
            "Requirement already satisfied: google-api-core<3.0.0dev,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage->gcsfs) (2.23.0)\n",
            "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage->gcsfs) (2.4.1)\n",
            "Requirement already satisfied: google-resumable-media>=2.7.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage->gcsfs) (2.7.2)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage->gcsfs) (1.6.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->gcsfs) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->gcsfs) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->gcsfs) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->gcsfs) (2024.8.30)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.35.37,>=1.35.16->aiobotocore<3.0.0,>=2.5.4->s3fs) (1.0.1)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.35.37,>=1.35.16->aiobotocore<3.0.0,>=2.5.4->s3fs) (2.8.2)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage->gcsfs) (1.66.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage->gcsfs) (5.29.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage->gcsfs) (1.25.0)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (4.12.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.2->gcsfs) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs) (3.2.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.35.37,>=1.35.16->aiobotocore<3.0.0,>=2.5.4->s3fs) (1.16.0)\n",
            "Using cached gcsfs-2024.10.0-py2.py3-none-any.whl (34 kB)\n",
            "Using cached fsspec-2024.10.0-py3-none-any.whl (179 kB)\n",
            "Using cached s3fs-2024.10.0-py3-none-any.whl (29 kB)\n",
            "Installing collected packages: fsspec, s3fs, gcsfs\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datasets 3.1.0 requires fsspec[http]<=2024.9.0,>=2023.1.0, but you have fsspec 2024.10.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed fsspec-2024.10.0 gcsfs-2024.10.0 s3fs-2024.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset(\"saitsharipov/CelebA-HQ\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j8DQ7HwuyWxR",
        "outputId": "f1561909-df8c-4cc0-fa2f-e7adb6f2ea28"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
            "  Using cached fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.8)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.26.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (5.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Using cached fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "Installing collected packages: fsspec\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\n",
            "s3fs 2024.10.0 requires fsspec==2024.10.0.*, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed fsspec-2024.9.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Model\n",
        "import cv2\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import gc\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "# Enable memory growth for GPU\n",
        "physical_devices = tf.config.list_physical_devices('GPU')\n",
        "if physical_devices:\n",
        "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
        "\n",
        "# Configuration\n",
        "IMG_SIZE = 128  # Reduced from 256 due to memory constraints\n",
        "BATCH_SIZE = 4\n",
        "BUFFER_SIZE = 1000\n",
        "DATA_DIR = \"celeba_hq_256\"\n",
        "\n",
        "def load_and_analyze_dataset():\n",
        "    \"\"\"Load images and perform EDA\"\"\"\n",
        "    image_paths = [os.path.join(DATA_DIR, f) for f in os.listdir(DATA_DIR) if f.endswith(('.jpg', '.png'))]\n",
        "    print(f\"Total images found: {len(image_paths)}\")\n",
        "\n",
        "    # Sample images for analysis\n",
        "    sample_size = min(1000, len(image_paths))\n",
        "    sample_paths = random.sample(image_paths, sample_size)\n",
        "\n",
        "    # Analyze image properties\n",
        "    sizes = []\n",
        "    channels = []\n",
        "    pixel_values = []\n",
        "\n",
        "    for path in tqdm(sample_paths[:100]):  # Analyze first 100 images\n",
        "        img = cv2.imread(path)\n",
        "        sizes.append(img.shape[:2])\n",
        "        channels.append(img.shape[2])\n",
        "        pixel_values.extend(img.ravel())\n",
        "\n",
        "    # Create visualizations\n",
        "    plt.figure(figsize=(15, 5))\n",
        "\n",
        "    plt.subplot(131)\n",
        "    plt.hist(pixel_values, bins=50)\n",
        "    plt.title('Pixel Value Distribution')\n",
        "\n",
        "    plt.subplot(132)\n",
        "    sizes_np = np.array(sizes)\n",
        "    plt.scatter(sizes_np[:, 0], sizes_np[:, 1])\n",
        "    plt.title('Image Dimensions')\n",
        "\n",
        "    plt.subplot(133)\n",
        "    plt.hist(channels)\n",
        "    plt.title('Number of Channels')\n",
        "    plt.show()\n",
        "\n",
        "    return image_paths\n",
        "\n",
        "def create_mask(height, width):\n",
        "    \"\"\"Create random masks for inpainting\"\"\"\n",
        "    mask = np.ones((height, width, 1))\n",
        "\n",
        "    # Random rectangular mask\n",
        "    y1, x1 = np.random.randint(0, height - height//3), np.random.randint(0, width - width//3)\n",
        "    h, w = height//3, width//3\n",
        "    mask[y1:y1+h, x1:x1+w] = 0\n",
        "\n",
        "    return mask\n",
        "\n",
        "def preprocess_image(image_path):\n",
        "    \"\"\"Load and preprocess images\"\"\"\n",
        "    img = tf.io.read_file(image_path)\n",
        "    img = tf.image.decode_png(img, channels=3)\n",
        "    img = tf.image.resize(img, [IMG_SIZE, IMG_SIZE])\n",
        "    img = tf.cast(img, tf.float32) / 255.0\n",
        "\n",
        "    mask = create_mask(IMG_SIZE, IMG_SIZE)\n",
        "    masked_img = img * mask\n",
        "\n",
        "    return masked_img, mask, img\n",
        "\n",
        "def create_dataset(image_paths):\n",
        "    \"\"\"Create TensorFlow dataset\"\"\"\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(image_paths)\n",
        "    dataset = dataset.map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "    return dataset\n",
        "\n",
        "def unet_model():\n",
        "    \"\"\"Create U-Net model\"\"\"\n",
        "    inputs = layers.Input((IMG_SIZE, IMG_SIZE, 3))\n",
        "\n",
        "    # Encoder\n",
        "    conv1 = layers.Conv2D(64, 3, activation='relu', padding='same')(inputs)\n",
        "    conv1 = layers.Conv2D(64, 3, activation='relu', padding='same')(conv1)\n",
        "    pool1 = layers.MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "\n",
        "    conv2 = layers.Conv2D(128, 3, activation='relu', padding='same')(pool1)\n",
        "    conv2 = layers.Conv2D(128, 3, activation='relu', padding='same')(conv2)\n",
        "    pool2 = layers.MaxPooling2D(pool_size=(2, 2))(conv2)\n",
        "\n",
        "    # Bridge\n",
        "    conv3 = layers.Conv2D(256, 3, activation='relu', padding='same')(pool2)\n",
        "    conv3 = layers.Conv2D(256, 3, activation='relu', padding='same')(conv3)\n",
        "\n",
        "    # Decoder\n",
        "    up1 = layers.UpSampling2D(size=(2, 2))(conv3)\n",
        "    up1 = layers.concatenate([conv2, up1], axis=-1)\n",
        "    conv4 = layers.Conv2D(128, 3, activation='relu', padding='same')(up1)\n",
        "    conv4 = layers.Conv2D(128, 3, activation='relu', padding='same')(conv4)\n",
        "\n",
        "    up2 = layers.UpSampling2D(size=(2, 2))(conv4)\n",
        "    up2 = layers.concatenate([conv1, up2], axis=-1)\n",
        "    conv5 = layers.Conv2D(64, 3, activation='relu', padding='same')(up2)\n",
        "    conv5 = layers.Conv2D(64, 3, activation='relu', padding='same')(conv5)\n",
        "\n",
        "    outputs = layers.Conv2D(3, 1, activation='sigmoid')(conv5)\n",
        "\n",
        "    return Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "def hint_model():\n",
        "    \"\"\"Create HINT model with transformer architecture\"\"\"\n",
        "    inputs = layers.Input((IMG_SIZE, IMG_SIZE, 3))\n",
        "    mask_input = layers.Input((IMG_SIZE, IMG_SIZE, 1))\n",
        "\n",
        "    # Encode input with mask awareness\n",
        "    x = layers.concatenate([inputs, mask_input])\n",
        "    x = layers.Conv2D(64, 3, padding='same')(x)\n",
        "\n",
        "    # Transformer blocks\n",
        "    for _ in range(3):\n",
        "        # Multi-head attention\n",
        "        attention = layers.MultiHeadAttention(num_heads=4, key_dim=16)(x, x)\n",
        "        x = layers.Add()([attention, x])\n",
        "        x = layers.LayerNormalization()(x)\n",
        "\n",
        "        # FFN\n",
        "        ffn = layers.Dense(128, activation='relu')(x)\n",
        "        ffn = layers.Dense(64)(ffn)\n",
        "        x = layers.Add()([ffn, x])\n",
        "        x = layers.LayerNormalization()(x)\n",
        "\n",
        "    outputs = layers.Conv2D(3, 1, activation='sigmoid')(x)\n",
        "\n",
        "    return Model(inputs=[inputs, mask_input], outputs=outputs)\n",
        "\n",
        "def combined_model(unet, hint):\n",
        "    \"\"\"Combine U-Net and HINT models\"\"\"\n",
        "    inputs = layers.Input((IMG_SIZE, IMG_SIZE, 3))\n",
        "    mask_input = layers.Input((IMG_SIZE, IMG_SIZE, 1))\n",
        "\n",
        "    unet_output = unet(inputs)\n",
        "    hint_output = hint([inputs, mask_input])\n",
        "\n",
        "    combined = layers.Average()([unet_output, hint_output])\n",
        "\n",
        "    return Model(inputs=[inputs, mask_input], outputs=combined)\n",
        "\n",
        "def evaluate_models(models, test_dataset):\n",
        "    \"\"\"Evaluate models using various metrics\"\"\"\n",
        "    metrics = {\n",
        "        'PSNR': [],\n",
        "        'SSIM': [],\n",
        "        'L1_Loss': []\n",
        "    }\n",
        "\n",
        "    for model_name, model in models.items():\n",
        "        psnr_values = []\n",
        "        ssim_values = []\n",
        "        l1_values = []\n",
        "\n",
        "        for masked_imgs, masks, original_imgs in test_dataset:\n",
        "            if isinstance(model.input, list):\n",
        "                predictions = model([masked_imgs, masks])\n",
        "            else:\n",
        "                predictions = model(masked_imgs)\n",
        "\n",
        "            # Calculate metrics\n",
        "            psnr = tf.image.psnr(original_imgs, predictions, max_val=1.0)\n",
        "            ssim = tf.image.ssim(original_imgs, predictions, max_val=1.0)\n",
        "            l1 = tf.reduce_mean(tf.abs(original_imgs - predictions))\n",
        "\n",
        "            psnr_values.extend(psnr.numpy())\n",
        "            ssim_values.extend(ssim.numpy())\n",
        "            l1_values.append(l1.numpy())\n",
        "\n",
        "        metrics['PSNR'].append(np.mean(psnr_values))\n",
        "        metrics['SSIM'].append(np.mean(ssim_values))\n",
        "        metrics['L1_Loss'].append(np.mean(l1_values))\n",
        "\n",
        "    # Visualize metrics\n",
        "    df = pd.DataFrame(metrics, index=list(models.keys()))\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    sns.barplot(data=df)\n",
        "    plt.title('Model Comparison')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.show()\n",
        "\n",
        "    return df\n",
        "\n",
        "def main():\n",
        "    # 1. Load dataset and perform EDA\n",
        "    image_paths = load_and_analyze_dataset()\n",
        "\n",
        "    # 2. Split dataset\n",
        "    train_paths, test_paths = train_test_split(image_paths, test_size=0.2, random_state=42)\n",
        "\n",
        "    # 3. Create datasets\n",
        "    train_dataset = create_dataset(train_paths)\n",
        "    test_dataset = create_dataset(test_paths)\n",
        "\n",
        "    # 4. Create models\n",
        "    unet = unet_model()\n",
        "    hint = hint_model()\n",
        "    combined = combined_model(unet, hint)\n",
        "\n",
        "    # 5. Compile models\n",
        "    optimizer = Adam(learning_rate=0.001)\n",
        "    loss = 'mse'\n",
        "\n",
        "    unet.compile(optimizer=optimizer, loss=loss)\n",
        "    hint.compile(optimizer=optimizer, loss=loss)\n",
        "    combined.compile(optimizer=optimizer, loss=loss)\n",
        "\n",
        "    # 6. Train models (showing only structure here due to computational constraints)\n",
        "    print(\"Models created and compiled successfully\")\n",
        "\n",
        "    # 7. Evaluate models\n",
        "    models = {\n",
        "        'U-Net': unet,\n",
        "        'HINT': hint,\n",
        "        'Combined': combined\n",
        "    }\n",
        "\n",
        "    results = evaluate_models(models, test_dataset)\n",
        "    print(\"\\nEvaluation Results:\")\n",
        "    print(results)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "TjPkBht2heVs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "outputId": "f77bdbde-345e-4e4f-fe9e-9914722e0ae2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'celeba_hq_256'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-063f4cf07f4f>\u001b[0m in \u001b[0;36m<cell line: 245>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-1-063f4cf07f4f>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;31m# 1. Load dataset and perform EDA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m     \u001b[0mimage_paths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_and_analyze_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;31m# 2. Split dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-063f4cf07f4f>\u001b[0m in \u001b[0;36mload_and_analyze_dataset\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_and_analyze_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;34m\"\"\"Load images and perform EDA\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mimage_paths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_DIR\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.jpg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Total images found: {len(image_paths)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'celeba_hq_256'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Model\n",
        "import cv2\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import gc\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "from tensorflow.keras import mixed_precision\n",
        "\n",
        "# Enable memory growth for GPU and mixed precision training\n",
        "physical_devices = tf.config.list_physical_devices('GPU')\n",
        "if physical_devices:\n",
        "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
        "mixed_precision.set_global_policy('mixed_float16')\n",
        "\n",
        "# Configuration\n",
        "IMG_SIZE = 128  # Reduced from 256 due to memory constraints\n",
        "BATCH_SIZE = 4\n",
        "BUFFER_SIZE = 1000\n",
        "DATA_DIR = \"celeba_hq_256\"\n",
        "\n",
        "def load_and_analyze_dataset():\n",
        "    \"\"\"Load images and perform EDA\"\"\"\n",
        "    image_paths = [os.path.join(DATA_DIR, f) for f in os.listdir(DATA_DIR) if f.endswith(('.jpg', '.png'))]\n",
        "    print(f\"Total images found: {len(image_paths)}\")\n",
        "\n",
        "    # Sample images for analysis\n",
        "    sample_size = min(1000, len(image_paths))\n",
        "    sample_paths = random.sample(image_paths, sample_size)\n",
        "\n",
        "    # Analyze image properties\n",
        "    sizes = []\n",
        "    channels = []\n",
        "    pixel_values = []\n",
        "\n",
        "    for path in tqdm(sample_paths[:100]):  # Analyze first 100 images\n",
        "        img = cv2.imread(path)\n",
        "        sizes.append(img.shape[:2])\n",
        "        channels.append(img.shape[2])\n",
        "        pixel_values.extend(img.ravel())\n",
        "\n",
        "    # Create visualizations\n",
        "    plt.figure(figsize=(15, 5))\n",
        "\n",
        "    plt.subplot(131)\n",
        "    plt.hist(pixel_values, bins=50)\n",
        "    plt.title('Pixel Value Distribution')\n",
        "\n",
        "    plt.subplot(132)\n",
        "    sizes_np = np.array(sizes)\n",
        "    plt.scatter(sizes_np[:, 0], sizes_np[:, 1])\n",
        "    plt.title('Image Dimensions')\n",
        "\n",
        "    plt.subplot(133)\n",
        "    plt.hist(channels)\n",
        "    plt.title('Number of Channels')\n",
        "    plt.show()\n",
        "\n",
        "    return image_paths\n",
        "\n",
        "def create_mask(height, width):\n",
        "    \"\"\"Create random masks for inpainting\"\"\"\n",
        "    mask = np.ones((height, width, 1))\n",
        "    num_masks = np.random.randint(1, 4)  # Random number of masks\n",
        "    for _ in range(num_masks):\n",
        "        y1, x1 = np.random.randint(0, height - height//4), np.random.randint(0, width - width//4)\n",
        "        h, w = np.random.randint(height//8, height//4), np.random.randint(width//8, width//4)\n",
        "        mask[y1:y1+h, x1:x1+w] = 0\n",
        "    return mask\n",
        "\n",
        "def preprocess_image(image_path):\n",
        "    \"\"\"Load and preprocess images\"\"\"\n",
        "    img = tf.io.read_file(image_path)\n",
        "    img = tf.image.decode_png(img, channels=3)\n",
        "    img = tf.image.resize(img, [IMG_SIZE, IMG_SIZE])\n",
        "    img = tf.cast(img, tf.float32) / 255.0\n",
        "\n",
        "    mask = create_mask(IMG_SIZE, IMG_SIZE)\n",
        "    masked_img = img * mask\n",
        "\n",
        "    return masked_img, mask, img\n",
        "\n",
        "def augment(masked_img, mask, img):\n",
        "    img = tf.image.random_flip_left_right(img)\n",
        "    img = tf.image.random_brightness(img, max_delta=0.2)\n",
        "    img = tf.image.random_contrast(img, lower=0.8, upper=1.2)\n",
        "    masked_img = tf.image.random_flip_left_right(masked_img)\n",
        "    mask = tf.image.random_flip_left_right(mask)\n",
        "    return masked_img, mask, img\n",
        "\n",
        "def create_dataset(image_paths):\n",
        "    \"\"\"Create TensorFlow dataset\"\"\"\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(image_paths)\n",
        "    dataset = dataset.map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    dataset = dataset.map(augment, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "    return dataset\n",
        "\n",
        "def unet_model():\n",
        "    \"\"\"Create U-Net model\"\"\"\n",
        "    inputs = layers.Input((IMG_SIZE, IMG_SIZE, 3))\n",
        "\n",
        "    # Encoder\n",
        "    conv1 = layers.Conv2D(64, 3, activation='relu', padding='same')(inputs)\n",
        "    conv1 = layers.Conv2D(64, 3, activation='relu', padding='same')(conv1)\n",
        "    conv1 = layers.BatchNormalization()(conv1)\n",
        "    pool1 = layers.MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "\n",
        "    conv2 = layers.Conv2D(128, 3, activation='relu', padding='same')(pool1)\n",
        "    conv2 = layers.Conv2D(128, 3, activation='relu', padding='same')(conv2)\n",
        "    conv2 = layers.BatchNormalization()(conv2)\n",
        "    pool2 = layers.MaxPooling2D(pool_size=(2, 2))(conv2)\n",
        "\n",
        "    # Bridge\n",
        "    conv3 = layers.Conv2D(256, 3, activation='relu', padding='same')(pool2)\n",
        "    conv3 = layers.Conv2D(256, 3, activation='relu', padding='same')(conv3)\n",
        "    conv3 = layers.BatchNormalization()(conv3)\n",
        "\n",
        "    # Decoder\n",
        "    up1 = layers.UpSampling2D(size=(2, 2))(conv3)\n",
        "    up1 = layers.concatenate([conv2, up1], axis=-1)\n",
        "    conv4 = layers.Conv2D(128, 3, activation='relu', padding='same')(up1)\n",
        "    conv4 = layers.Conv2D(128, 3, activation='relu', padding='same')(conv4)\n",
        "    conv4 = layers.BatchNormalization()(conv4)\n",
        "\n",
        "    up2 = layers.UpSampling2D(size=(2, 2))(conv4)\n",
        "    up2 = layers.concatenate([conv1, up2], axis=-1)\n",
        "    conv5 = layers.Conv2D(64, 3, activation='relu', padding='same')(up2)\n",
        "    conv5 = layers.Conv2D(64, 3, activation='relu', padding='same')(conv5)\n",
        "    conv5 = layers.BatchNormalization()(conv5)\n",
        "\n",
        "    outputs = layers.Conv2D(3, 1, activation='sigmoid')(conv5)\n",
        "\n",
        "    return Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "def hint_model():\n",
        "    \"\"\"Create HINT model with transformer architecture\"\"\"\n",
        "    inputs = layers.Input((IMG_SIZE, IMG_SIZE, 3))\n",
        "    mask_input = layers.Input((IMG_SIZE, IMG_SIZE, 1))\n",
        "\n",
        "    # Encode input with mask awareness\n",
        "    x = layers.concatenate([inputs, mask_input])\n",
        "    x = layers.Conv2D(64, 3, padding='same')(x)\n",
        "\n",
        "    # Transformer blocks\n",
        "    for _ in range(3):\n",
        "        # Multi-head attention\n",
        "        attention = layers.MultiHeadAttention(num_heads=4, key_dim=16)(x, x)\n",
        "        x = layers.Add()([attention, x])\n",
        "        x = layers.LayerNormalization()(x)\n",
        "\n",
        "        # FFN\n",
        "        ffn = layers.Dense(128, activation='relu')(x)\n",
        "        ffn = layers.Dense(64)(ffn)\n",
        "        x = layers.Add()([ffn, x])\n",
        "        x = layers.LayerNormalization()(x)\n",
        "\n",
        "    outputs = layers.Conv2D(3, 1, activation='sigmoid')(x)\n",
        "\n",
        "    return Model(inputs=[inputs, mask_input], outputs=outputs)\n",
        "\n",
        "def combined_model(unet, hint):\n",
        "    \"\"\"Combine U-Net and HINT models\"\"\"\n",
        "    inputs = layers.Input((IMG_SIZE, IMG_SIZE, 3))\n",
        "    mask_input = layers.Input((IMG_SIZE, IMG_SIZE, 1))\n",
        "\n",
        "    unet_output = unet(inputs)\n",
        "    hint_output = hint([inputs, mask_input])\n",
        "\n",
        "    combined = layers.Average()([unet_output, hint_output])\n",
        "\n",
        "    return Model(inputs=[inputs, mask_input], outputs=combined)\n",
        "\n",
        "def perceptual_loss(y_true, y_pred):\n",
        "    vgg = tf.keras.applications.VGG19(include_top=False, weights='imagenet')\n",
        "    vgg.trainable = False\n",
        "    loss_model = tf.keras.Model(inputs=vgg.input, outputs=vgg.get_layer('block3_conv3').output)\n",
        "    loss_model.trainable = False\n",
        "    return tf.reduce_mean(tf.square(loss_model(y_true) - loss_model(y_pred)))\n",
        "\n",
        "def combined_loss(y_true, y_pred):\n",
        "    return tf.reduce_mean(tf.square(y_true - y_pred)) + 0.1 * perceptual_loss(y_true, y_pred)\n",
        "\n",
        "def evaluate_models(models, test_dataset):\n",
        "    \"\"\"Evaluate models using various metrics\"\"\"\n",
        "    metrics = {\n",
        "        'PSNR': [],\n",
        "        'SSIM': [],\n",
        "        'L1_Loss': []\n",
        "    }\n",
        "\n",
        "    for model_name, model in models.items():\n",
        "        psnr_values = []\n",
        "        ssim_values = []\n",
        "        l1_values = []\n",
        "\n",
        "        for masked_imgs, masks, original_imgs in test_dataset:\n",
        "            if isinstance(model.input, list):\n",
        "                predictions = model([masked_imgs, masks])\n",
        "            else:\n",
        "                predictions = model(masked_imgs)\n",
        "\n",
        "            # Calculate metrics\n",
        "            psnr = tf.image.psnr(original_imgs, predictions, max_val=1.0)\n",
        "            ssim = tf.image.ssim(original_imgs, predictions, max_val=1.0)\n",
        "            l1 = tf.reduce_mean(tf.abs(original_imgs - predictions))\n",
        "\n",
        "            psnr_values.extend(psnr.numpy())\n",
        "            ssim_values.extend(ssim.numpy())\n",
        "            l1_values.append(l1.numpy())\n",
        "\n",
        "        metrics['PSNR'].append(np.mean(psnr_values))\n",
        "        metrics['SSIM'].append(np.mean(ssim_values))\n",
        "        metrics['L1_Loss'].append(np.mean(l1_values))\n",
        "\n",
        "    # Visualize metrics\n",
        "    df = pd.DataFrame(metrics, index=list(models.keys()))\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    sns.barplot(data=df)\n",
        "    plt.title('Model Comparison')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.show()\n",
        "\n",
        "    return df\n",
        "\n",
        "def visualize_inpainting(model, test_dataset, num_samples=5):\n",
        "    for masked_imgs, masks, original_imgs in test_dataset.take(num_samples):\n",
        "        if isinstance(model.input, list):\n",
        "            predictions = model([masked_imgs, masks])\n",
        "        else:\n",
        "            predictions = model(masked_imgs)\n",
        "\n",
        "        plt.figure(figsize=(15, 10))\n",
        "        for i in range(num_samples):\n",
        "            plt.subplot(num_samples, 3, i*3 + 1)\n",
        "            plt.imshow(masked_imgs[i])\n",
        "            plt.title('Masked Image')\n",
        "            plt.axis('off')\n",
        "\n",
        "            plt.subplot(num_samples, 3, i*3 + 2)\n",
        "            plt.imshow(predictions[i])\n",
        "            plt.title('Inpainted Image')\n",
        "            plt.axis('off')\n",
        "\n",
        "            plt.subplot(num_samples, 3, i*3 + 3)\n",
        "            plt.imshow(original_imgs[i])\n",
        "            plt.title('Original Image')\n",
        "            plt.axis('off')\n",
        "        plt.show()\n",
        "\n",
        "def main():\n",
        "    # 1. Load dataset and perform EDA\n",
        "    image_paths = load_and_analyze_dataset()\n",
        "\n",
        "    # 2. Split dataset\n",
        "    train_paths, test_paths = train_test_split(image_paths, test_size=0.2, random_state=42)\n",
        "\n",
        "    # 3. Create datasets\n",
        "    train_dataset = create_dataset(train_paths)\n",
        "    test_dataset = create_dataset(test_paths)\n",
        "\n",
        "    # 4. Create models\n",
        "    unet = unet_model()\n",
        "    hint = hint_model()\n",
        "    combined = combined_model(unet, hint)\n",
        "\n",
        "    # 5. Compile models\n",
        "    optimizer = Adam(learning_rate=0.001)\n",
        "    loss = combined_loss\n",
        "\n",
        "    unet.compile(optimizer=optimizer, loss=loss)\n",
        "    hint.compile(optimizer=optimizer, loss=loss)\n",
        "    combined.compile(optimizer=optimizer, loss=loss)\n",
        "\n",
        "    # 6. Training callbacks\n",
        "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.00001)\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "    # 7. Train models (showing only structure here due to computational constraints)\n",
        "    print(\"Models created and compiled successfully\")\n",
        "\n",
        "    # 8. Evaluate models\n",
        "    models = {\n",
        "        'U-Net': unet,\n",
        "        'HINT': hint,\n",
        "        'Combined': combined\n",
        "    }\n",
        "\n",
        "    results = evaluate_models(models, test_dataset)\n",
        "    print(\"\\nEvaluation Results:\")\n",
        "    print(results)\n",
        "\n",
        "    # 9. Visualize inpainting results\n",
        "    visualize_inpainting(combined, test_dataset)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "kPtFgPACZLrv",
        "outputId": "d3cf71a6-c647-4f2f-9c22-bfb78e0cda21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'celeba_hq_256'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-4a3a9d60367a>\u001b[0m in \u001b[0;36m<cell line: 303>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-2-4a3a9d60367a>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;31m# 1. Load dataset and perform EDA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m     \u001b[0mimage_paths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_and_analyze_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[0;31m# 2. Split dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-4a3a9d60367a>\u001b[0m in \u001b[0;36mload_and_analyze_dataset\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_and_analyze_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;34m\"\"\"Load images and perform EDA\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mimage_paths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_DIR\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.jpg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Total images found: {len(image_paths)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'celeba_hq_256'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Model\n",
        "import cv2\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import gc\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "from tensorflow.keras import mixed_precision\n",
        "\n",
        "# Enable memory growth for GPU and mixed precision training\n",
        "physical_devices = tf.config.list_physical_devices('GPU')\n",
        "if physical_devices:\n",
        "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
        "mixed_precision.set_global_policy('mixed_float16')\n",
        "\n",
        "# Configuration\n",
        "IMG_SIZE = 128  # Reduced from 256 due to memory constraints\n",
        "BATCH_SIZE = 4\n",
        "BUFFER_SIZE = 1000\n",
        "DATA_DIR = \"celeba_hq_256\"  # Update this path to the correct directory\n",
        "\n",
        "def load_and_analyze_dataset():\n",
        "    \"\"\"Load images and perform EDA\"\"\"\n",
        "    if not os.path.exists(DATA_DIR):\n",
        "        raise FileNotFoundError(f\"The directory {DATA_DIR} does not exist. Please ensure the dataset is downloaded and extracted to the correct location.\")\n",
        "\n",
        "    image_paths = [os.path.join(DATA_DIR, f) for f in os.listdir(DATA_DIR) if f.endswith(('.jpg', '.png'))]\n",
        "    print(f\"Total images found: {len(image_paths)}\")\n",
        "\n",
        "    # Sample images for analysis\n",
        "    sample_size = min(1000, len(image_paths))\n",
        "    sample_paths = random.sample(image_paths, sample_size)\n",
        "\n",
        "    # Analyze image properties\n",
        "    sizes = []\n",
        "    channels = []\n",
        "    pixel_values = []\n",
        "\n",
        "    for path in tqdm(sample_paths[:100]):  # Analyze first 100 images\n",
        "        img = cv2.imread(path)\n",
        "        sizes.append(img.shape[:2])\n",
        "        channels.append(img.shape[2])\n",
        "        pixel_values.extend(img.ravel())\n",
        "\n",
        "    # Create visualizations\n",
        "    plt.figure(figsize=(15, 5))\n",
        "\n",
        "    plt.subplot(131)\n",
        "    plt.hist(pixel_values, bins=50)\n",
        "    plt.title('Pixel Value Distribution')\n",
        "\n",
        "    plt.subplot(132)\n",
        "    sizes_np = np.array(sizes)\n",
        "    plt.scatter(sizes_np[:, 0], sizes_np[:, 1])\n",
        "    plt.title('Image Dimensions')\n",
        "\n",
        "    plt.subplot(133)\n",
        "    plt.hist(channels)\n",
        "    plt.title('Number of Channels')\n",
        "    plt.show()\n",
        "\n",
        "    return image_paths\n",
        "\n",
        "def create_mask(height, width):\n",
        "    \"\"\"Create random masks for inpainting\"\"\"\n",
        "    mask = np.ones((height, width, 1))\n",
        "    num_masks = np.random.randint(1, 4)  # Random number of masks\n",
        "    for _ in range(num_masks):\n",
        "        y1, x1 = np.random.randint(0, height - height//4), np.random.randint(0, width - width//4)\n",
        "        h, w = np.random.randint(height//8, height//4), np.random.randint(width//8, width//4)\n",
        "        mask[y1:y1+h, x1:x1+w] = 0\n",
        "    return mask\n",
        "\n",
        "def preprocess_image(image_path):\n",
        "    \"\"\"Load and preprocess images\"\"\"\n",
        "    img = tf.io.read_file(image_path)\n",
        "    img = tf.image.decode_png(img, channels=3)\n",
        "    img = tf.image.resize(img, [IMG_SIZE, IMG_SIZE])\n",
        "    img = tf.cast(img, tf.float32) / 255.0\n",
        "\n",
        "    mask = create_mask(IMG_SIZE, IMG_SIZE)\n",
        "    masked_img = img * mask\n",
        "\n",
        "    return masked_img, mask, img\n",
        "\n",
        "def augment(masked_img, mask, img):\n",
        "    img = tf.image.random_flip_left_right(img)\n",
        "    img = tf.image.random_brightness(img, max_delta=0.2)\n",
        "    img = tf.image.random_contrast(img, lower=0.8, upper=1.2)\n",
        "    masked_img = tf.image.random_flip_left_right(masked_img)\n",
        "    mask = tf.image.random_flip_left_right(mask)\n",
        "    return masked_img, mask, img\n",
        "\n",
        "def create_dataset(image_paths):\n",
        "    \"\"\"Create TensorFlow dataset\"\"\"\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(image_paths)\n",
        "    dataset = dataset.map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    dataset = dataset.map(augment, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "    return dataset\n",
        "\n",
        "def unet_model():\n",
        "    \"\"\"Create U-Net model\"\"\"\n",
        "    inputs = layers.Input((IMG_SIZE, IMG_SIZE, 3))\n",
        "\n",
        "    # Encoder\n",
        "    conv1 = layers.Conv2D(64, 3, activation='relu', padding='same')(inputs)\n",
        "    conv1 = layers.Conv2D(64, 3, activation='relu', padding='same')(conv1)\n",
        "    conv1 = layers.BatchNormalization()(conv1)\n",
        "    pool1 = layers.MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "\n",
        "    conv2 = layers.Conv2D(128, 3, activation='relu', padding='same')(pool1)\n",
        "    conv2 = layers.Conv2D(128, 3, activation='relu', padding='same')(conv2)\n",
        "    conv2 = layers.BatchNormalization()(conv2)\n",
        "    pool2 = layers.MaxPooling2D(pool_size=(2, 2))(conv2)\n",
        "\n",
        "    # Bridge\n",
        "    conv3 = layers.Conv2D(256, 3, activation='relu', padding='same')(pool2)\n",
        "    conv3 = layers.Conv2D(256, 3, activation='relu', padding='same')(conv3)\n",
        "    conv3 = layers.BatchNormalization()(conv3)\n",
        "\n",
        "    # Decoder\n",
        "    up1 = layers.UpSampling2D(size=(2, 2))(conv3)\n",
        "    up1 = layers.concatenate([conv2, up1], axis=-1)\n",
        "    conv4 = layers.Conv2D(128, 3, activation='relu', padding='same')(up1)\n",
        "    conv4 = layers.Conv2D(128, 3, activation='relu', padding='same')(conv4)\n",
        "    conv4 = layers.BatchNormalization()(conv4)\n",
        "\n",
        "    up2 = layers.UpSampling2D(size=(2, 2))(conv4)\n",
        "    up2 = layers.concatenate([conv1, up2], axis=-1)\n",
        "    conv5 = layers.Conv2D(64, 3, activation='relu', padding='same')(up2)\n",
        "    conv5 = layers.Conv2D(64, 3, activation='relu', padding='same')(conv5)\n",
        "    conv5 = layers.BatchNormalization()(conv5)\n",
        "\n",
        "    outputs = layers.Conv2D(3, 1, activation='sigmoid')(conv5)\n",
        "\n",
        "    return Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "def hint_model():\n",
        "    \"\"\"Create HINT model with transformer architecture\"\"\"\n",
        "    inputs = layers.Input((IMG_SIZE, IMG_SIZE, 3))\n",
        "    mask_input = layers.Input((IMG_SIZE, IMG_SIZE, 1))\n",
        "\n",
        "    # Encode input with mask awareness\n",
        "    x = layers.concatenate([inputs, mask_input])\n",
        "    x = layers.Conv2D(64, 3, padding='same')(x)\n",
        "\n",
        "    # Transformer blocks\n",
        "    for _ in range(3):\n",
        "        # Multi-head attention\n",
        "        attention = layers.MultiHeadAttention(num_heads=4, key_dim=16)(x, x)\n",
        "        x = layers.Add()([attention, x])\n",
        "        x = layers.LayerNormalization()(x)\n",
        "\n",
        "        # FFN\n",
        "        ffn = layers.Dense(128, activation='relu')(x)\n",
        "        ffn = layers.Dense(64)(ffn)\n",
        "        x = layers.Add()([ffn, x])\n",
        "        x = layers.LayerNormalization()(x)\n",
        "\n",
        "    outputs = layers.Conv2D(3, 1, activation='sigmoid')(x)\n",
        "\n",
        "    return Model(inputs=[inputs, mask_input], outputs=outputs)\n",
        "\n",
        "def combined_model(unet, hint):\n",
        "    \"\"\"Combine U-Net and HINT models\"\"\"\n",
        "    inputs = layers.Input((IMG_SIZE, IMG_SIZE, 3))\n",
        "    mask_input = layers.Input((IMG_SIZE, IMG_SIZE, 1))\n",
        "\n",
        "    unet_output = unet(inputs)\n",
        "    hint_output = hint([inputs, mask_input])\n",
        "\n",
        "    combined = layers.Average()([unet_output, hint_output])\n",
        "\n",
        "    return Model(inputs=[inputs, mask_input], outputs=combined)\n",
        "\n",
        "def perceptual_loss(y_true, y_pred):\n",
        "    vgg = tf.keras.applications.VGG19(include_top=False, weights='imagenet')\n",
        "    vgg.trainable = False\n",
        "    loss_model = tf.keras.Model(inputs=vgg.input, outputs=vgg.get_layer('block3_conv3').output)\n",
        "    loss_model.trainable = False\n",
        "    return tf.reduce_mean(tf.square(loss_model(y_true) - loss_model(y_pred)))\n",
        "\n",
        "def combined_loss(y_true, y_pred):\n",
        "    return tf.reduce_mean(tf.square(y_true - y_pred)) + 0.1 * perceptual_loss(y_true, y_pred)\n",
        "\n",
        "def evaluate_models(models, test_dataset):\n",
        "    \"\"\"Evaluate models using various metrics\"\"\"\n",
        "    metrics = {\n",
        "        'PSNR': [],\n",
        "        'SSIM': [],\n",
        "        'L1_Loss': []\n",
        "    }\n",
        "\n",
        "    for model_name, model in models.items():\n",
        "        psnr_values = []\n",
        "        ssim_values = []\n",
        "        l1_values = []\n",
        "\n",
        "        for masked_imgs, masks, original_imgs in test_dataset:\n",
        "            if isinstance(model.input, list):\n",
        "                predictions = model([masked_imgs, masks])\n",
        "            else:\n",
        "                predictions = model(masked_imgs)\n",
        "\n",
        "            # Calculate metrics\n",
        "            psnr = tf.image.psnr(original_imgs, predictions, max_val=1.0)\n",
        "            ssim = tf.image.ssim(original_imgs, predictions, max_val=1.0)\n",
        "            l1 = tf.reduce_mean(tf.abs(original_imgs - predictions))\n",
        "\n",
        "            psnr_values.extend(psnr.numpy())\n",
        "            ssim_values.extend(ssim.numpy())\n",
        "            l1_values.append(l1.numpy())\n",
        "\n",
        "        metrics['PSNR'].append(np.mean(psnr_values))\n",
        "        metrics['SSIM'].append(np.mean(ssim_values))\n",
        "        metrics['L1_Loss'].append(np.mean(l1_values))\n",
        "\n",
        "    # Visualize metrics\n",
        "    df = pd.DataFrame(metrics, index=list(models.keys()))\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    sns.barplot(data=df)\n",
        "    plt.title('Model Comparison')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.show()\n",
        "\n",
        "    return df\n",
        "\n",
        "def visualize_inpainting(model, test_dataset, num_samples=5):\n",
        "    for masked_imgs, masks, original_imgs in test_dataset.take(num_samples):\n",
        "        if isinstance(model.input, list):\n",
        "            predictions = model([masked_imgs, masks])\n",
        "        else:\n",
        "            predictions = model(masked_imgs)\n",
        "\n",
        "        plt.figure(figsize=(15, 10))\n",
        "        for i in range(num_samples):\n",
        "            plt.subplot(num_samples, 3, i*3 + 1)\n",
        "            plt.imshow(masked_imgs[i])\n",
        "            plt.title('Masked Image')\n",
        "            plt.axis('off')\n",
        "\n",
        "            plt.subplot(num_samples, 3, i*3 + 2)\n",
        "            plt.imshow(predictions[i])\n",
        "            plt.title('Inpainted Image')\n",
        "            plt.axis('off')\n",
        "\n",
        "            plt.subplot(num_samples, 3, i*3 + 3)\n",
        "            plt.imshow(original_imgs[i])\n",
        "            plt.title('Original Image')\n",
        "            plt.axis('off')\n",
        "        plt.show()\n",
        "\n",
        "def main():\n",
        "    # 1. Load dataset and perform EDA\n",
        "    try:\n",
        "        image_paths = load_and_analyze_dataset()\n",
        "    except FileNotFoundError as e:\n",
        "        print(e)\n",
        "        return\n",
        "\n",
        "    # 2. Split dataset\n",
        "    train_paths, test_paths = train_test_split(image_paths, test_size=0.2, random_state=42)\n",
        "\n",
        "    # 3. Create datasets\n",
        "    train_dataset = create_dataset(train_paths)\n",
        "    test_dataset = create_dataset(test_paths)\n",
        "\n",
        "    # 4. Create models\n",
        "    unet = unet_model()\n",
        "    hint = hint_model()\n",
        "    combined = combined_model(unet, hint)\n",
        "\n",
        "    # 5. Compile models\n",
        "    optimizer = Adam(learning_rate=0.001)\n",
        "    loss = combined_loss\n",
        "\n",
        "    unet.compile(optimizer=optimizer, loss=loss)\n",
        "    hint.compile(optimizer=optimizer, loss=loss)\n",
        "    combined.compile(optimizer=optimizer, loss=loss)\n",
        "\n",
        "    # 6. Training callbacks\n",
        "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.00001)\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "    # 7. Train models (showing only structure here due to computational constraints)\n",
        "    print(\"Models created and compiled successfully\")\n",
        "\n",
        "    # 8. Evaluate models\n",
        "    models = {\n",
        "        'U-Net': unet,\n",
        "        'HINT': hint,\n",
        "        'Combined': combined\n",
        "    }\n",
        "\n",
        "    results = evaluate_models(models, test_dataset)\n",
        "    print(\"\\nEvaluation Results:\")\n",
        "    print(results)\n",
        "\n",
        "    # 9. Visualize inpainting results\n",
        "    visualize_inpainting(combined, test_dataset)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Nw9Nrsix2q-",
        "outputId": "57b64318-e025-463e-da44-d5e0900163d5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The directory celeba_hq_256 does not exist. Please ensure the dataset is downloaded and extracted to the correct location.\n"
          ]
        }
      ]
    }
  ]
}