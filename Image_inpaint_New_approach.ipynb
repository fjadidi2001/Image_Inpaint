{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fjadidi2001/Image_Inpaint/blob/main/Image_inpaint_New_approach.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"badasstechie/celebahq-resized-256x256\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "id": "UnxJfW2N7I7l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76b36ae9-f823-40a3-ff5d-4c65656b5faf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/badasstechie/celebahq-resized-256x256?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 283M/283M [00:02<00:00, 100MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/badasstechie/celebahq-resized-256x256/versions/1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import layers, Model\n",
        "import cv2\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Limit GPU memory usage\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        tf.config.experimental.set_virtual_device_configuration(\n",
        "            gpus[0],\n",
        "            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4096)])\n",
        "    except RuntimeError as e:\n",
        "        print(e)\n",
        "\n",
        "class ImageInpainting:\n",
        "    def __init__(self, data_path, img_size=256, batch_size=8):\n",
        "        self.data_path = data_path\n",
        "        self.img_size = img_size\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def load_dataset(self):\n",
        "        \"\"\"Load and prepare the dataset\"\"\"\n",
        "        images = []\n",
        "        for img_path in tqdm(os.listdir(self.data_path)[:1000]):  # Limiting to 1000 images for memory\n",
        "            if img_path.endswith(('.jpg', '.png')):\n",
        "                img = cv2.imread(os.path.join(self.data_path, img_path))\n",
        "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "                img = cv2.resize(img, (self.img_size, self.img_size))\n",
        "                images.append(img)\n",
        "        return np.array(images)\n",
        "\n",
        "    def perform_eda(self, images):\n",
        "        \"\"\"Perform exploratory data analysis\"\"\"\n",
        "        plt.figure(figsize=(15, 5))\n",
        "        for i in range(5):\n",
        "            plt.subplot(1, 5, i+1)\n",
        "            plt.imshow(images[i])\n",
        "            plt.axis('off')\n",
        "        plt.show()\n",
        "\n",
        "        print(f\"Dataset shape: {images.shape}\")\n",
        "        print(f\"Data type: {images.dtype}\")\n",
        "        print(f\"Min value: {images.min()}, Max value: {images.max()}\")\n",
        "\n",
        "    def preprocess_images(self, images):\n",
        "        \"\"\"Preprocess the images\"\"\"\n",
        "        # Normalize to [-1, 1]\n",
        "        images = (images.astype('float32') - 127.5) / 127.5\n",
        "        return images\n",
        "\n",
        "    def create_masks(self, shape):\n",
        "        \"\"\"Create random masks for inpainting\"\"\"\n",
        "        masks = []\n",
        "        for _ in range(shape[0]):\n",
        "            mask = np.ones((self.img_size, self.img_size, 1))\n",
        "            # Random rectangular masks\n",
        "            y1, x1 = np.random.randint(0, self.img_size-64, 2)\n",
        "            mask[y1:y1+64, x1:x1+64] = 0\n",
        "            masks.append(mask)\n",
        "        return np.array(masks)\n",
        "\n",
        "    def build_unet(self):\n",
        "        \"\"\"Build U-Net model\"\"\"\n",
        "        def conv_block(x, filters, kernel_size=3):\n",
        "            x = layers.Conv2D(filters, kernel_size, padding='same')(x)\n",
        "            x = layers.BatchNormalization()(x)\n",
        "            x = layers.ReLU()(x)\n",
        "            return x\n",
        "\n",
        "        inputs = layers.Input((self.img_size, self.img_size, 3))\n",
        "        mask = layers.Input((self.img_size, self.img_size, 1))\n",
        "\n",
        "        # Concatenate mask with input\n",
        "        x = layers.Concatenate()([inputs, mask])\n",
        "\n",
        "        # Encoder\n",
        "        e1 = conv_block(x, 64)\n",
        "        e2 = conv_block(layers.MaxPooling2D()(e1), 128)\n",
        "        e3 = conv_block(layers.MaxPooling2D()(e2), 256)\n",
        "\n",
        "        # Bridge\n",
        "        b = conv_block(layers.MaxPooling2D()(e3), 512)\n",
        "\n",
        "        # Decoder\n",
        "        d3 = conv_block(layers.UpSampling2D()(b), 256)\n",
        "        d3 = layers.Concatenate()([d3, e3])\n",
        "\n",
        "        d2 = conv_block(layers.UpSampling2D()(d3), 128)\n",
        "        d2 = layers.Concatenate()([d2, e2])\n",
        "\n",
        "        d1 = conv_block(layers.UpSampling2D()(d2), 64)\n",
        "        d1 = layers.Concatenate()([d1, e1])\n",
        "\n",
        "        outputs = layers.Conv2D(3, 1, activation='tanh')(d1)\n",
        "\n",
        "        return Model([inputs, mask], outputs)\n",
        "\n",
        "    def build_hint(self):\n",
        "        \"\"\"Build simplified HINT model for limited resources\"\"\"\n",
        "        def transformer_block(x, filters):\n",
        "            # Self-attention\n",
        "            attention = layers.MultiHeadAttention(\n",
        "                num_heads=4, key_dim=filters//4)(x, x, x)\n",
        "            x = layers.Add()([x, attention])\n",
        "            x = layers.LayerNormalization()(x)\n",
        "\n",
        "            # FFN\n",
        "            ffn = layers.Dense(filters*2)(x)\n",
        "            ffn = layers.ReLU()(ffn)\n",
        "            ffn = layers.Dense(filters)(ffn)\n",
        "\n",
        "            x = layers.Add()([x, ffn])\n",
        "            x = layers.LayerNormalization()(x)\n",
        "            return x\n",
        "\n",
        "        inputs = layers.Input((self.img_size, self.img_size, 3))\n",
        "        mask = layers.Input((self.img_size, self.img_size, 1))\n",
        "\n",
        "        x = layers.Concatenate()([inputs, mask])\n",
        "\n",
        "        # Simplified architecture for limited resources\n",
        "        x = conv_block(x, 64)\n",
        "        x = layers.Reshape((self.img_size * self.img_size, 64))(x)\n",
        "        x = transformer_block(x, 64)\n",
        "        x = layers.Reshape((self.img_size, self.img_size, 64))(x)\n",
        "\n",
        "        outputs = layers.Conv2D(3, 1, activation='tanh')(x)\n",
        "\n",
        "        return Model([inputs, mask], outputs)\n",
        "\n",
        "    def combined_model(self):\n",
        "        \"\"\"Combine U-Net and HINT models\"\"\"\n",
        "        inputs = layers.Input((self.img_size, self.img_size, 3))\n",
        "        mask = layers.Input((self.img_size, self.img_size, 1))\n",
        "\n",
        "        unet = self.build_unet()\n",
        "        hint = self.build_hint()\n",
        "\n",
        "        unet_out = unet([inputs, mask])\n",
        "        hint_out = hint([inputs, mask])\n",
        "\n",
        "        # Weighted combination\n",
        "        alpha = 0.7  # Weight for U-Net\n",
        "        outputs = layers.Lambda(\n",
        "            lambda x: alpha * x[0] + (1-alpha) * x[1])([unet_out, hint_out])\n",
        "\n",
        "        return Model([inputs, mask], outputs)\n",
        "\n",
        "    def evaluate_model(self, model, test_images, test_masks):\n",
        "        \"\"\"Evaluate model using various metrics\"\"\"\n",
        "        predictions = model.predict([test_images, test_masks])\n",
        "\n",
        "        # Calculate metrics\n",
        "        mse = np.mean((test_images - predictions) ** 2)\n",
        "        psnr = 20 * np.log10(2.0 / np.sqrt(mse))  # Assuming normalized [-1, 1]\n",
        "\n",
        "        # Visualize results\n",
        "        plt.figure(figsize=(15, 5))\n",
        "        for i in range(3):\n",
        "            plt.subplot(1, 3, i*3 + 1)\n",
        "            plt.imshow((test_images[i] + 1) / 2)\n",
        "            plt.title('Original')\n",
        "            plt.axis('off')\n",
        "\n",
        "            plt.subplot(1, 3, i*3 + 2)\n",
        "            masked = test_images[i] * test_masks[i]\n",
        "            plt.imshow((masked + 1) / 2)\n",
        "            plt.title('Masked')\n",
        "            plt.axis('off')\n",
        "\n",
        "            plt.subplot(1, 3, i*3 + 3)\n",
        "            plt.imshow((predictions[i] + 1) / 2)\n",
        "            plt.title('Inpainted')\n",
        "            plt.axis('off')\n",
        "\n",
        "        plt.show()\n",
        "        print(f\"MSE: {mse:.4f}\")\n",
        "        print(f\"PSNR: {psnr:.2f} dB\")\n",
        "\n",
        "    def train(self, epochs=10):\n",
        "        \"\"\"Train the model\"\"\"\n",
        "        # Load and prepare data\n",
        "        print(\"Loading dataset...\")\n",
        "        images = self.load_dataset()\n",
        "        self.perform_eda(images)\n",
        "\n",
        "        print(\"\\nPreprocessing images...\")\n",
        "        images = self.preprocess_images(images)\n",
        "        masks = self.create_masks(images.shape)\n",
        "\n",
        "        # Split dataset\n",
        "        train_images, test_images, train_masks, test_masks = train_test_split(\n",
        "            images, masks, test_size=0.2, random_state=42)\n",
        "\n",
        "        # Build and compile model\n",
        "        print(\"\\nBuilding model...\")\n",
        "        model = self.combined_model()\n",
        "        model.compile(\n",
        "            optimizer=tf.keras.optimizers.Adam(1e-4),\n",
        "            loss='mse',\n",
        "            metrics=['mae']\n",
        "        )\n",
        "\n",
        "        # Train\n",
        "        print(\"\\nTraining model...\")\n",
        "        history = model.fit(\n",
        "            [train_images, train_masks],\n",
        "            train_images,\n",
        "            batch_size=self.batch_size,\n",
        "            epochs=epochs,\n",
        "            validation_split=0.2\n",
        "        )\n",
        "\n",
        "        # Evaluate\n",
        "        print(\"\\nEvaluating model...\")\n",
        "        self.evaluate_model(model, test_images, test_masks)\n",
        "\n",
        "        return model, history\n",
        "\n",
        "# Usage example\n",
        "if __name__ == \"__main__\":\n",
        "    path = kagglehub.dataset_download(\"badasstechie/celebahq-resized-256x256\")\n",
        "\n",
        "    inpainting = ImageInpainting(\"path_to_dataset\")\n",
        "    model, history = inpainting.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "LzXTCC7b-hxJ",
        "outputId": "93325df7-7aac-4079-fe88-055c1e030669"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading dataset...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'path_to_dataset'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-ad0818ff53bf>\u001b[0m in \u001b[0;36m<cell line: 228>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m     \u001b[0minpainting\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImageInpainting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"path_to_dataset\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minpainting\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-3-ad0818ff53bf>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, epochs)\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;31m# Load and prepare data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading dataset...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperform_eda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-ad0818ff53bf>\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;34m\"\"\"Load and prepare the dataset\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mimg_path\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Limiting to 1000 images for memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mimg_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.jpg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'path_to_dataset'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "{\n",
        "\t\"name\": \"ImportError\",\n",
        "\t\"message\": \"Traceback (most recent call last):\n",
        "  File \\\"c:\\\\Users\\\\DRpeyvandi\\\\Downloads\\\\myenv\\\\Lib\\\\site-packages\\\\tensorflow\\\\python\\\\pywrap_tensorflow.py\\\", line 70, in <module>\n",
        "    from tensorflow.python._pywrap_tensorflow_internal import *\n",
        "ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.\n",
        "\n",
        "\n",
        "Failed to load the native TensorFlow runtime.\n",
        "See https://www.tensorflow.org/install/errors for some common causes and solutions.\n",
        "If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.\",\n",
        "\t\"stack\": \"---------------------------------------------------------------------------\n",
        "ImportError                               Traceback (most recent call last)\n",
        "File c:\\\\Users\\\\DRpeyvandi\\\\Downloads\\\\myenv\\\\Lib\\\\site-packages\\\\tensorflow\\\\python\\\\pywrap_tensorflow.py:70\n",
        "     69 try:\n",
        "---> 70   from tensorflow.python._pywrap_tensorflow_internal import *\n",
        "     71 # This try catch logic is because there is no bazel equivalent for py_extension.\n",
        "     72 # Externally in opensource we must enable exceptions to load the shared object\n",
        "     73 # by exposing the PyInit symbols with pybind. This error will only be\n",
        "     74 # caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.\n",
        "     75\n",
        "     76 # This logic is used in other internal projects using py_extension.\n",
        "\n",
        "ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.\n",
        "\n",
        "During handling of the above exception, another exception occurred:\n",
        "\n",
        "ImportError                               Traceback (most recent call last)\n",
        "Cell In[3], line 3\n",
        "      1 import os\n",
        "      2 import numpy as np\n",
        "----> 3 import tensorflow as tf\n",
        "      4 import matplotlib.pyplot as plt\n",
        "      5 from tensorflow.keras import layers, Model\n",
        "\n",
        "File c:\\\\Users\\\\DRpeyvandi\\\\Downloads\\\\myenv\\\\Lib\\\\site-packages\\\\tensorflow\\\\__init__.py:40\n",
        "     37 _os.environ.setdefault(\\\"ENABLE_RUNTIME_UPTIME_TELEMETRY\\\", \\\"1\\\")\n",
        "     39 # Do not remove this line; See https://github.com/tensorflow/tensorflow/issues/42596\n",
        "---> 40 from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow  # pylint: disable=unused-import\n",
        "     41 from tensorflow.python.tools import module_util as _module_util\n",
        "     42 from tensorflow.python.util.lazy_loader import KerasLazyLoader as _KerasLazyLoader\n",
        "\n",
        "File c:\\\\Users\\\\DRpeyvandi\\\\Downloads\\\\myenv\\\\Lib\\\\site-packages\\\\tensorflow\\\\python\\\\pywrap_tensorflow.py:85\n",
        "     83     sys.setdlopenflags(_default_dlopen_flags)\n",
        "     84 except ImportError:\n",
        "---> 85   raise ImportError(\n",
        "     86       f'{traceback.format_exc()}'\n",
        "     87       f'\\\n",
        "\\\n",
        "Failed to load the native TensorFlow runtime.\\\n",
        "'\n",
        "     88       f'See https://www.tensorflow.org/install/errors '\n",
        "     89       f'for some common causes and solutions.\\\n",
        "'\n",
        "     90       f'If you need help, create an issue '\n",
        "     91       f'at https://github.com/tensorflow/tensorflow/issues '\n",
        "     92       f'and include the entire stack trace above this error message.')\n",
        "     94 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\n",
        "\n",
        "ImportError: Traceback (most recent call last):\n",
        "  File \\\"c:\\\\Users\\\\DRpeyvandi\\\\Downloads\\\\myenv\\\\Lib\\\\site-packages\\\\tensorflow\\\\python\\\\pywrap_tensorflow.py\\\", line 70, in <module>\n",
        "    from tensorflow.python._pywrap_tensorflow_internal import *\n",
        "ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.\n",
        "\n",
        "\n",
        "Failed to load the native TensorFlow runtime.\n",
        "See https://www.tensorflow.org/install/errors for some common causes and solutions.\n",
        "If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.\"\n",
        "}"
      ],
      "metadata": {
        "id": "_BVpnZSXUtXR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import cv2\n",
        "from datetime import datetime\n",
        "\n",
        "class InpaintingDataset(Dataset):\n",
        "    def __init__(self, data_path, img_size=256, transform=None):\n",
        "        self.data_path = data_path\n",
        "        self.img_size = img_size\n",
        "        self.transform = transform\n",
        "        self.image_files = [f for f in os.listdir(data_path)\n",
        "                          if f.endswith(('.jpg', '.png'))][:1000]  # Limit to 1000 images\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def create_mask(self):\n",
        "        \"\"\"Create random rectangular mask\"\"\"\n",
        "        mask = torch.ones(1, self.img_size, self.img_size)\n",
        "        y1, x1 = torch.randint(0, self.img_size-64, (2,))\n",
        "        mask[:, y1:y1+64, x1:x1+64] = 0\n",
        "        return mask\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.data_path, self.image_files[idx])\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        mask = self.create_mask()\n",
        "        masked_image = image * mask\n",
        "\n",
        "        return image, masked_image, mask\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(UNet, self).__init__()\n",
        "\n",
        "        # Encoder\n",
        "        self.enc1 = self.conv_block(4, 64)    # +1 channel for mask\n",
        "        self.enc2 = self.conv_block(64, 128)\n",
        "        self.enc3 = self.conv_block(128, 256)\n",
        "\n",
        "        # Bridge\n",
        "        self.bridge = self.conv_block(256, 512)\n",
        "\n",
        "        # Decoder\n",
        "        self.up3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
        "        self.dec3 = self.conv_block(512, 256)  # 512 = 256 + 256 (skip connection)\n",
        "\n",
        "        self.up2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
        "        self.dec2 = self.conv_block(256, 128)\n",
        "\n",
        "        self.up1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
        "        self.dec1 = self.conv_block(128, 64)\n",
        "\n",
        "        self.final = nn.Conv2d(64, 3, kernel_size=1)\n",
        "\n",
        "    def conv_block(self, in_ch, out_ch):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        # Concatenate input image and mask\n",
        "        x = torch.cat([x, mask], dim=1)\n",
        "\n",
        "        # Encoder\n",
        "        e1 = self.enc1(x)\n",
        "        e2 = self.enc2(nn.MaxPool2d(2)(e1))\n",
        "        e3 = self.enc3(nn.MaxPool2d(2)(e2))\n",
        "\n",
        "        # Bridge\n",
        "        b = self.bridge(nn.MaxPool2d(2)(e3))\n",
        "\n",
        "        # Decoder with skip connections\n",
        "        d3 = self.dec3(torch.cat([self.up3(b), e3], dim=1))\n",
        "        d2 = self.dec2(torch.cat([self.up2(d3), e2], dim=1))\n",
        "        d1 = self.dec1(torch.cat([self.up1(d2), e1], dim=1))\n",
        "\n",
        "        return torch.tanh(self.final(d1))\n",
        "\n",
        "class InpaintingTrainer:\n",
        "    def __init__(self, data_path, img_size=256, batch_size=8):\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.img_size = img_size\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        # Data preparation\n",
        "        transform = transforms.Compose([\n",
        "            transforms.Resize((img_size, img_size)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "        ])\n",
        "\n",
        "        self.dataset = InpaintingDataset(data_path, img_size, transform)\n",
        "        self.dataloader = DataLoader(\n",
        "            self.dataset,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=True,\n",
        "            num_workers=2\n",
        "        )\n",
        "\n",
        "        # Model setup\n",
        "        self.model = UNet().to(self.device)\n",
        "        self.criterion = nn.MSELoss()\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=0.0002)\n",
        "\n",
        "        # Create directory for saving results\n",
        "        self.save_dir = f'inpainting_results_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}'\n",
        "        os.makedirs(self.save_dir, exist_ok=True)\n",
        "        os.makedirs(os.path.join(self.save_dir, 'checkpoints'), exist_ok=True)\n",
        "        os.makedirs(os.path.join(self.save_dir, 'samples'), exist_ok=True)\n",
        "\n",
        "    def save_images(self, original, masked, output, epoch, batch_idx):\n",
        "        \"\"\"Save sample images during training\"\"\"\n",
        "        def to_image(tensor):\n",
        "            return ((tensor.cpu().detach().numpy().transpose(1, 2, 0) + 1) * 127.5).astype(np.uint8)\n",
        "\n",
        "        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "        axes[0].imshow(to_image(original[0]))\n",
        "        axes[0].set_title('Original')\n",
        "        axes[0].axis('off')\n",
        "\n",
        "        axes[1].imshow(to_image(masked[0]))\n",
        "        axes[1].set_title('Masked')\n",
        "        axes[1].axis('off')\n",
        "\n",
        "        axes[2].imshow(to_image(output[0]))\n",
        "        axes[2].set_title('Inpainted')\n",
        "        axes[2].axis('off')\n",
        "\n",
        "        plt.savefig(os.path.join(self.save_dir, 'samples', f'epoch_{epoch}_batch_{batch_idx}.png'))\n",
        "        plt.close()\n",
        "\n",
        "    def train(self, num_epochs=100):\n",
        "        losses = []\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            epoch_losses = []\n",
        "            pbar = tqdm(self.dataloader, desc=f'Epoch {epoch+1}/{num_epochs}')\n",
        "\n",
        "            for batch_idx, (original, masked, mask) in enumerate(pbar):\n",
        "                original = original.to(self.device)\n",
        "                masked = masked.to(self.device)\n",
        "                mask = mask.to(self.device)\n",
        "\n",
        "                # Forward pass\n",
        "                output = self.model(masked, mask)\n",
        "                loss = self.criterion(output, original)\n",
        "\n",
        "                # Backward pass\n",
        "                self.optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "                # Record loss\n",
        "                epoch_losses.append(loss.item())\n",
        "                pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
        "\n",
        "                # Save sample images\n",
        "                if batch_idx % 100 == 0:\n",
        "                    self.save_images(original, masked, output, epoch+1, batch_idx)\n",
        "\n",
        "            # Save model checkpoint every 5 epochs\n",
        "            if (epoch + 1) % 5 == 0:\n",
        "                checkpoint_path = os.path.join(\n",
        "                    self.save_dir, 'checkpoints', f'model_epoch_{epoch+1}.pth'\n",
        "                )\n",
        "                torch.save({\n",
        "                    'epoch': epoch + 1,\n",
        "                    'model_state_dict': self.model.state_dict(),\n",
        "                    'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "                    'loss': np.mean(epoch_losses),\n",
        "                }, checkpoint_path)\n",
        "\n",
        "            # Record and plot average epoch loss\n",
        "            avg_loss = np.mean(epoch_losses)\n",
        "            losses.append(avg_loss)\n",
        "\n",
        "            # Plot loss curve\n",
        "            plt.figure(figsize=(10, 5))\n",
        "            plt.plot(losses)\n",
        "            plt.title('Training Loss')\n",
        "            plt.xlabel('Epoch')\n",
        "            plt.ylabel('Loss')\n",
        "            plt.savefig(os.path.join(self.save_dir, 'loss_curve.png'))\n",
        "            plt.close()\n",
        "\n",
        "            print(f'Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}')\n",
        "\n",
        "# Usage example\n",
        "if __name__ == \"__main__\":\n",
        "    trainer = InpaintingTrainer(\"path_to_dataset\")\n",
        "    trainer.train(num_epochs=100)"
      ],
      "metadata": {
        "id": "b_R1Fz5jb5nY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "outputId": "435215c6-eb63-4c32-b65b-0892d3eb6b90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'path_to_dataset'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-42cab1268ebe>\u001b[0m in \u001b[0;36m<cell line: 207>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;31m# Usage example\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m     \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInpaintingTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"path_to_dataset\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-42cab1268ebe>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_path, img_size, batch_size)\u001b[0m\n\u001b[1;32m    108\u001b[0m         ])\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInpaintingDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m         self.dataloader = DataLoader(\n\u001b[1;32m    112\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-42cab1268ebe>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_path, img_size, transform)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         self.image_files = [f for f in os.listdir(data_path)\n\u001b[0m\u001b[1;32m     20\u001b[0m                           if f.endswith(('.jpg', '.png'))][:1000]  # Limit to 1000 images\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'path_to_dataset'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "import zipfile\n",
        "from tqdm import tqdm\n",
        "import shutil\n",
        "\n",
        "class DatasetPreparation:\n",
        "    def __init__(self):\n",
        "        self.base_dir = os.path.join(os.path.expanduser('~'), 'inpainting_data')\n",
        "        self.dataset_dir = os.path.join(self.base_dir, 'images')\n",
        "\n",
        "    def download_test_dataset(self):\n",
        "        \"\"\"Download a small test dataset of face images\"\"\"\n",
        "        # Create directories if they don't exist\n",
        "        os.makedirs(self.base_dir, exist_ok=True)\n",
        "        os.makedirs(self.dataset_dir, exist_ok=True)\n",
        "\n",
        "        # Download CelebA-HQ test dataset (small subset)\n",
        "        url = \"https://drive.google.com/uc?id=1badu11NqxGf6qM3PTTooQDJvQbejgbTv\"\n",
        "\n",
        "        print(\"Downloading test dataset...\")\n",
        "        response = requests.get(url, stream=True)\n",
        "        total_size = int(response.headers.get('content-length', 0))\n",
        "\n",
        "        zip_path = os.path.join(self.base_dir, 'test_dataset.zip')\n",
        "\n",
        "        with open(zip_path, 'wb') as file, tqdm(\n",
        "            desc=\"Downloading\",\n",
        "            total=total_size,\n",
        "            unit='iB',\n",
        "            unit_scale=True,\n",
        "            unit_divisor=1024,\n",
        "        ) as pbar:\n",
        "            for data in response.iter_content(chunk_size=1024):\n",
        "                size = file.write(data)\n",
        "                pbar.update(size)\n",
        "\n",
        "        # Extract dataset\n",
        "        print(\"Extracting files...\")\n",
        "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall(self.dataset_dir)\n",
        "\n",
        "        # Clean up\n",
        "        os.remove(zip_path)\n",
        "        print(f\"Dataset prepared at: {self.dataset_dir}\")\n",
        "        return self.dataset_dir\n",
        "\n",
        "    def prepare_custom_dataset(self, source_path):\n",
        "        \"\"\"Prepare dataset from a custom folder of images\"\"\"\n",
        "        os.makedirs(self.dataset_dir, exist_ok=True)\n",
        "\n",
        "        # Copy images from source to dataset directory\n",
        "        print(f\"Copying images from {source_path}...\")\n",
        "        for filename in tqdm(os.listdir(source_path)):\n",
        "            if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                src = os.path.join(source_path, filename)\n",
        "                dst = os.path.join(self.dataset_dir, filename)\n",
        "                shutil.copy2(src, dst)\n",
        "\n",
        "        print(f\"Dataset prepared at: {self.dataset_dir}\")\n",
        "        return self.dataset_dir\n",
        "\n",
        "    def cleanup(self):\n",
        "        \"\"\"Remove the dataset directory\"\"\"\n",
        "        if os.path.exists(self.base_dir):\n",
        "            shutil.rmtree(self.base_dir)\n",
        "            print(\"Dataset cleaned up successfully\")\n",
        "\n",
        "# Modified training script\n",
        "if __name__ == \"__main__\":\n",
        "    # Prepare dataset\n",
        "    data_prep = DatasetPreparation()\n",
        "\n",
        "    # Choose one of these options:\n",
        "\n",
        "    # Option 1: Download test dataset\n",
        "    dataset_path = data_prep.download_test_dataset()\n",
        "\n",
        "    # Option 2: Use custom dataset (uncomment and modify path)\n",
        "    # dataset_path = data_prep.prepare_custom_dataset(\"path/to/your/images\")\n",
        "\n",
        "    # Initialize and train the model\n",
        "    from inpainting_model import InpaintingTrainer  # Assuming previous code is saved as inpainting_model.py\n",
        "\n",
        "    trainer = InpaintingTrainer(dataset_path)\n",
        "    try:\n",
        "        trainer.train(num_epochs=100)\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nTraining interrupted. Cleaning up...\")\n",
        "    finally:\n",
        "        # Uncomment the following line if you want to clean up the dataset after training\n",
        "        data_prep.cleanup()"
      ],
      "metadata": {
        "id": "qKi9PNBMeVmv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "f635e688-d162-4dc1-9912-5decbb597684"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading test dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: 100%|██████████| 2.32k/2.32k [00:00<00:00, 2.40MiB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "BadZipFile",
          "evalue": "File is not a zip file",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mBadZipFile\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-9d8e80362711>\u001b[0m in \u001b[0;36m<cell line: 70>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;31m# Option 1: Download test dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m     \u001b[0mdataset_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_prep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_test_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;31m# Option 2: Use custom dataset (uncomment and modify path)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-9d8e80362711>\u001b[0m in \u001b[0;36mdownload_test_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m# Extract dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Extracting files...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mzip_ref\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m             \u001b[0mzip_ref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/zipfile.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps)\u001b[0m\n\u001b[1;32m   1270\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1272\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_RealGetContents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1273\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'x'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m                 \u001b[0;31m# set the modified flag so central directory gets written\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/zipfile.py\u001b[0m in \u001b[0;36m_RealGetContents\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1337\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mBadZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"File is not a zip file\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1338\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mendrec\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1339\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mBadZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"File is not a zip file\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1340\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1341\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendrec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mBadZipFile\u001b[0m: File is not a zip file"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ah, I see the issue. You need to specify the correct path to your dataset. Let's create a simple script to prepare the dataset using either direct images from a local folder or by downloading a small test dataset.\n",
        "\n",
        "\n",
        "\n",
        "```python\n",
        "import os\n",
        "import requests\n",
        "import zipfile\n",
        "from tqdm import tqdm\n",
        "import shutil\n",
        "\n",
        "class DatasetPreparation:\n",
        "    def __init__(self):\n",
        "        self.base_dir = os.path.join(os.path.expanduser('~'), 'inpainting_data')\n",
        "        self.dataset_dir = os.path.join(self.base_dir, 'images')\n",
        "        \n",
        "    def download_test_dataset(self):\n",
        "        \"\"\"Download a small test dataset of face images\"\"\"\n",
        "        # Create directories if they don't exist\n",
        "        os.makedirs(self.base_dir, exist_ok=True)\n",
        "        os.makedirs(self.dataset_dir, exist_ok=True)\n",
        "        \n",
        "        # Download CelebA-HQ test dataset (small subset)\n",
        "        url = \"https://drive.google.com/uc?id=1badu11NqxGf6qM3PTTooQDJvQbejgbTv\"\n",
        "        \n",
        "        print(\"Downloading test dataset...\")\n",
        "        response = requests.get(url, stream=True)\n",
        "        total_size = int(response.headers.get('content-length', 0))\n",
        "        \n",
        "        zip_path = os.path.join(self.base_dir, 'test_dataset.zip')\n",
        "        \n",
        "        with open(zip_path, 'wb') as file, tqdm(\n",
        "            desc=\"Downloading\",\n",
        "            total=total_size,\n",
        "            unit='iB',\n",
        "            unit_scale=True,\n",
        "            unit_divisor=1024,\n",
        "        ) as pbar:\n",
        "            for data in response.iter_content(chunk_size=1024):\n",
        "                size = file.write(data)\n",
        "                pbar.update(size)\n",
        "        \n",
        "        # Extract dataset\n",
        "        print(\"Extracting files...\")\n",
        "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall(self.dataset_dir)\n",
        "        \n",
        "        # Clean up\n",
        "        os.remove(zip_path)\n",
        "        print(f\"Dataset prepared at: {self.dataset_dir}\")\n",
        "        return self.dataset_dir\n",
        "    \n",
        "    def prepare_custom_dataset(self, source_path):\n",
        "        \"\"\"Prepare dataset from a custom folder of images\"\"\"\n",
        "        os.makedirs(self.dataset_dir, exist_ok=True)\n",
        "        \n",
        "        # Copy images from source to dataset directory\n",
        "        print(f\"Copying images from {source_path}...\")\n",
        "        for filename in tqdm(os.listdir(source_path)):\n",
        "            if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                src = os.path.join(source_path, filename)\n",
        "                dst = os.path.join(self.dataset_dir, filename)\n",
        "                shutil.copy2(src, dst)\n",
        "        \n",
        "        print(f\"Dataset prepared at: {self.dataset_dir}\")\n",
        "        return self.dataset_dir\n",
        "    \n",
        "    def cleanup(self):\n",
        "        \"\"\"Remove the dataset directory\"\"\"\n",
        "        if os.path.exists(self.base_dir):\n",
        "            shutil.rmtree(self.base_dir)\n",
        "            print(\"Dataset cleaned up successfully\")\n",
        "\n",
        "# Modified training script\n",
        "if __name__ == \"__main__\":\n",
        "    # Prepare dataset\n",
        "    data_prep = DatasetPreparation()\n",
        "    \n",
        "    # Choose one of these options:\n",
        "    \n",
        "    # Option 1: Download test dataset\n",
        "    dataset_path = data_prep.download_test_dataset()\n",
        "    \n",
        "    # Option 2: Use custom dataset (uncomment and modify path)\n",
        "    # dataset_path = data_prep.prepare_custom_dataset(\"path/to/your/images\")\n",
        "    \n",
        "    # Initialize and train the model\n",
        "    from inpainting_model import InpaintingTrainer  # Assuming previous code is saved as inpainting_model.py\n",
        "    \n",
        "    trainer = InpaintingTrainer(dataset_path)\n",
        "    try:\n",
        "        trainer.train(num_epochs=100)\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nTraining interrupted. Cleaning up...\")\n",
        "    finally:\n",
        "        # Uncomment the following line if you want to clean up the dataset after training\n",
        "        # data_prep.cleanup()\n",
        "\n",
        "```\n",
        "\n",
        "To use this code:\n",
        "\n",
        "1. First, save the previous inpainting model code as `inpainting_model.py` in your working directory.\n",
        "\n",
        "2. Then create a new file named `train.py` with this new code.\n",
        "\n",
        "3. Install additional required packages:\n",
        "```bash\n",
        "pip install requests tqdm\n",
        "```\n",
        "\n",
        "4. Run the training script:\n",
        "```bash\n",
        "python train.py\n",
        "```\n",
        "\n",
        "The script provides two options:\n",
        "\n",
        "1. Automatically download a small test dataset (recommended for testing)\n",
        "2. Use your own images by specifying the path to your image folder\n",
        "\n",
        "The benefits of this approach:\n",
        "- Creates a proper directory structure\n",
        "- Downloads a test dataset if needed\n",
        "- Handles cleanup\n",
        "- Shows progress bars for downloads and copying\n",
        "- Provides clear error messages\n",
        "\n",
        "If you want to use your own images:\n",
        "1. Comment out the test dataset download line\n",
        "2. Uncomment the custom dataset line\n",
        "3. Provide the path to your image folder\n",
        "\n",
        "The dataset will be organized in:\n",
        "```\n",
        "~/inpainting_data/\n",
        "    └── images/\n",
        "        ├── image1.jpg\n",
        "        ├── image2.jpg\n",
        "        └── ...\n",
        "```\n",
        "\n",
        "Would you like me to:\n",
        "1. Modify the test dataset source?\n",
        "2. Add more dataset preparation options?\n",
        "3. Implement additional data preprocessing steps?"
      ],
      "metadata": {
        "id": "aeAlEXGreYfW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The error you're encountering is related to TensorFlow installation. Let's fix that and then create a version of the code that uses PyTorch instead, which often has fewer installation issues on Windows. I'll also add visualization and model checkpointing.\n",
        "\n",
        "First, to fix your TensorFlow installation:\n",
        "1. Uninstall TensorFlow: `pip uninstall tensorflow`\n",
        "2. Install Microsoft Visual C++ Redistributable\n",
        "3. Reinstall TensorFlow: `pip install tensorflow==2.10.0`\n",
        "\n",
        "However, let's use PyTorch for better compatibility:\n",
        "\n",
        "\n",
        "\n",
        "```python\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import cv2\n",
        "from datetime import datetime\n",
        "\n",
        "class InpaintingDataset(Dataset):\n",
        "    def __init__(self, data_path, img_size=256, transform=None):\n",
        "        self.data_path = data_path\n",
        "        self.img_size = img_size\n",
        "        self.transform = transform\n",
        "        self.image_files = [f for f in os.listdir(data_path)\n",
        "                          if f.endswith(('.jpg', '.png'))][:1000]  # Limit to 1000 images\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "    \n",
        "    def create_mask(self):\n",
        "        \"\"\"Create random rectangular mask\"\"\"\n",
        "        mask = torch.ones(1, self.img_size, self.img_size)\n",
        "        y1, x1 = torch.randint(0, self.img_size-64, (2,))\n",
        "        mask[:, y1:y1+64, x1:x1+64] = 0\n",
        "        return mask\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.data_path, self.image_files[idx])\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        \n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        \n",
        "        mask = self.create_mask()\n",
        "        masked_image = image * mask\n",
        "        \n",
        "        return image, masked_image, mask\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(UNet, self).__init__()\n",
        "        \n",
        "        # Encoder\n",
        "        self.enc1 = self.conv_block(4, 64)    # +1 channel for mask\n",
        "        self.enc2 = self.conv_block(64, 128)\n",
        "        self.enc3 = self.conv_block(128, 256)\n",
        "        \n",
        "        # Bridge\n",
        "        self.bridge = self.conv_block(256, 512)\n",
        "        \n",
        "        # Decoder\n",
        "        self.up3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
        "        self.dec3 = self.conv_block(512, 256)  # 512 = 256 + 256 (skip connection)\n",
        "        \n",
        "        self.up2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
        "        self.dec2 = self.conv_block(256, 128)\n",
        "        \n",
        "        self.up1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
        "        self.dec1 = self.conv_block(128, 64)\n",
        "        \n",
        "        self.final = nn.Conv2d(64, 3, kernel_size=1)\n",
        "        \n",
        "    def conv_block(self, in_ch, out_ch):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x, mask):\n",
        "        # Concatenate input image and mask\n",
        "        x = torch.cat([x, mask], dim=1)\n",
        "        \n",
        "        # Encoder\n",
        "        e1 = self.enc1(x)\n",
        "        e2 = self.enc2(nn.MaxPool2d(2)(e1))\n",
        "        e3 = self.enc3(nn.MaxPool2d(2)(e2))\n",
        "        \n",
        "        # Bridge\n",
        "        b = self.bridge(nn.MaxPool2d(2)(e3))\n",
        "        \n",
        "        # Decoder with skip connections\n",
        "        d3 = self.dec3(torch.cat([self.up3(b), e3], dim=1))\n",
        "        d2 = self.dec2(torch.cat([self.up2(d3), e2], dim=1))\n",
        "        d1 = self.dec1(torch.cat([self.up1(d2), e1], dim=1))\n",
        "        \n",
        "        return torch.tanh(self.final(d1))\n",
        "\n",
        "class InpaintingTrainer:\n",
        "    def __init__(self, data_path, img_size=256, batch_size=8):\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.img_size = img_size\n",
        "        self.batch_size = batch_size\n",
        "        \n",
        "        # Data preparation\n",
        "        transform = transforms.Compose([\n",
        "            transforms.Resize((img_size, img_size)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "        ])\n",
        "        \n",
        "        self.dataset = InpaintingDataset(data_path, img_size, transform)\n",
        "        self.dataloader = DataLoader(\n",
        "            self.dataset,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=True,\n",
        "            num_workers=2\n",
        "        )\n",
        "        \n",
        "        # Model setup\n",
        "        self.model = UNet().to(self.device)\n",
        "        self.criterion = nn.MSELoss()\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=0.0002)\n",
        "        \n",
        "        # Create directory for saving results\n",
        "        self.save_dir = f'inpainting_results_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}'\n",
        "        os.makedirs(self.save_dir, exist_ok=True)\n",
        "        os.makedirs(os.path.join(self.save_dir, 'checkpoints'), exist_ok=True)\n",
        "        os.makedirs(os.path.join(self.save_dir, 'samples'), exist_ok=True)\n",
        "        \n",
        "    def save_images(self, original, masked, output, epoch, batch_idx):\n",
        "        \"\"\"Save sample images during training\"\"\"\n",
        "        def to_image(tensor):\n",
        "            return ((tensor.cpu().detach().numpy().transpose(1, 2, 0) + 1) * 127.5).astype(np.uint8)\n",
        "        \n",
        "        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "        axes[0].imshow(to_image(original[0]))\n",
        "        axes[0].set_title('Original')\n",
        "        axes[0].axis('off')\n",
        "        \n",
        "        axes[1].imshow(to_image(masked[0]))\n",
        "        axes[1].set_title('Masked')\n",
        "        axes[1].axis('off')\n",
        "        \n",
        "        axes[2].imshow(to_image(output[0]))\n",
        "        axes[2].set_title('Inpainted')\n",
        "        axes[2].axis('off')\n",
        "        \n",
        "        plt.savefig(os.path.join(self.save_dir, 'samples', f'epoch_{epoch}_batch_{batch_idx}.png'))\n",
        "        plt.close()\n",
        "    \n",
        "    def train(self, num_epochs=100):\n",
        "        losses = []\n",
        "        \n",
        "        for epoch in range(num_epochs):\n",
        "            epoch_losses = []\n",
        "            pbar = tqdm(self.dataloader, desc=f'Epoch {epoch+1}/{num_epochs}')\n",
        "            \n",
        "            for batch_idx, (original, masked, mask) in enumerate(pbar):\n",
        "                original = original.to(self.device)\n",
        "                masked = masked.to(self.device)\n",
        "                mask = mask.to(self.device)\n",
        "                \n",
        "                # Forward pass\n",
        "                output = self.model(masked, mask)\n",
        "                loss = self.criterion(output, original)\n",
        "                \n",
        "                # Backward pass\n",
        "                self.optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "                \n",
        "                # Record loss\n",
        "                epoch_losses.append(loss.item())\n",
        "                pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
        "                \n",
        "                # Save sample images\n",
        "                if batch_idx % 100 == 0:\n",
        "                    self.save_images(original, masked, output, epoch+1, batch_idx)\n",
        "            \n",
        "            # Save model checkpoint every 5 epochs\n",
        "            if (epoch + 1) % 5 == 0:\n",
        "                checkpoint_path = os.path.join(\n",
        "                    self.save_dir, 'checkpoints', f'model_epoch_{epoch+1}.pth'\n",
        "                )\n",
        "                torch.save({\n",
        "                    'epoch': epoch + 1,\n",
        "                    'model_state_dict': self.model.state_dict(),\n",
        "                    'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "                    'loss': np.mean(epoch_losses),\n",
        "                }, checkpoint_path)\n",
        "            \n",
        "            # Record and plot average epoch loss\n",
        "            avg_loss = np.mean(epoch_losses)\n",
        "            losses.append(avg_loss)\n",
        "            \n",
        "            # Plot loss curve\n",
        "            plt.figure(figsize=(10, 5))\n",
        "            plt.plot(losses)\n",
        "            plt.title('Training Loss')\n",
        "            plt.xlabel('Epoch')\n",
        "            plt.ylabel('Loss')\n",
        "            plt.savefig(os.path.join(self.save_dir, 'loss_curve.png'))\n",
        "            plt.close()\n",
        "            \n",
        "            print(f'Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}')\n",
        "\n",
        "# Usage example\n",
        "if __name__ == \"__main__\":\n",
        "    trainer = InpaintingTrainer(\"path_to_dataset\")\n",
        "    trainer.train(num_epochs=100)\n",
        "\n",
        "```\n",
        "\n",
        "This updated version includes several improvements:\n",
        "\n",
        "1. Uses PyTorch instead of TensorFlow for better Windows compatibility\n",
        "2. Adds comprehensive visualization:\n",
        "   - Saves sample images every 100 batches\n",
        "   - Plots and saves loss curves after each epoch\n",
        "   - Creates organized output directories with timestamps\n",
        "\n",
        "3. Implements checkpoint saving:\n",
        "   - Saves model every 5 epochs\n",
        "   - Includes model state, optimizer state, and current loss\n",
        "   - Organizes checkpoints in a dedicated directory\n",
        "\n",
        "4. Memory optimization:\n",
        "   - Batch size of 8 for your GPU\n",
        "   - Limited to 1000 images\n",
        "   - Efficient data loading with PyTorch DataLoader\n",
        "\n",
        "To run this code:\n",
        "\n",
        "1. Install requirements:\n",
        "```bash\n",
        "pip install torch torchvision tqdm Pillow matplotlib numpy opencv-python\n",
        "```\n",
        "\n",
        "2. Update the data path in the usage example to point to your dataset directory.\n",
        "\n",
        "3. The code will automatically create directories for:\n",
        "   - Checkpoints (`checkpoints/`)\n",
        "   - Sample images (`samples/`)\n",
        "   - Loss curve plots\n",
        "\n",
        "The visualizations will help you monitor:\n",
        "- Training progress through the loss curve\n",
        "- Quality of inpainting at different stages\n",
        "- Model improvement over epochs\n",
        "\n",
        "Would you like me to:\n",
        "1. Add more visualization types?\n",
        "2. Modify the architecture for your specific GPU?\n",
        "3. Add additional evaluation metrics?"
      ],
      "metadata": {
        "id": "gjfj5PujefIo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "WpL2CoXseglK"
      }
    }
  ]
}