{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPhnkzFyIBfKcyEk4vSOWwr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fjadidi2001/Image_Inpaint/blob/main/Image_inpaint_New_approach.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"badasstechie/celebahq-resized-256x256\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "id": "UnxJfW2N7I7l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f5fe568-5076-4270-f2c6-29479b98a782"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/badasstechie/celebahq-resized-256x256?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 283M/283M [00:14<00:00, 21.1MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/badasstechie/celebahq-resized-256x256/versions/1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import layers, Model\n",
        "import cv2\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Limit GPU memory usage\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        tf.config.experimental.set_virtual_device_configuration(\n",
        "            gpus[0],\n",
        "            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4096)])\n",
        "    except RuntimeError as e:\n",
        "        print(e)\n",
        "\n",
        "class ImageInpainting:\n",
        "    def __init__(self, data_path, img_size=256, batch_size=8):\n",
        "        self.data_path = data_path\n",
        "        self.img_size = img_size\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def load_dataset(self):\n",
        "        \"\"\"Load and prepare the dataset\"\"\"\n",
        "        images = []\n",
        "        for img_path in tqdm(os.listdir(self.data_path)[:1000]):  # Limiting to 1000 images for memory\n",
        "            if img_path.endswith(('.jpg', '.png')):\n",
        "                img = cv2.imread(os.path.join(self.data_path, img_path))\n",
        "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "                img = cv2.resize(img, (self.img_size, self.img_size))\n",
        "                images.append(img)\n",
        "        return np.array(images)\n",
        "\n",
        "    def perform_eda(self, images):\n",
        "        \"\"\"Perform exploratory data analysis\"\"\"\n",
        "        plt.figure(figsize=(15, 5))\n",
        "        for i in range(5):\n",
        "            plt.subplot(1, 5, i+1)\n",
        "            plt.imshow(images[i])\n",
        "            plt.axis('off')\n",
        "        plt.show()\n",
        "\n",
        "        print(f\"Dataset shape: {images.shape}\")\n",
        "        print(f\"Data type: {images.dtype}\")\n",
        "        print(f\"Min value: {images.min()}, Max value: {images.max()}\")\n",
        "\n",
        "    def preprocess_images(self, images):\n",
        "        \"\"\"Preprocess the images\"\"\"\n",
        "        # Normalize to [-1, 1]\n",
        "        images = (images.astype('float32') - 127.5) / 127.5\n",
        "        return images\n",
        "\n",
        "    def create_masks(self, shape):\n",
        "        \"\"\"Create random masks for inpainting\"\"\"\n",
        "        masks = []\n",
        "        for _ in range(shape[0]):\n",
        "            mask = np.ones((self.img_size, self.img_size, 1))\n",
        "            # Random rectangular masks\n",
        "            y1, x1 = np.random.randint(0, self.img_size-64, 2)\n",
        "            mask[y1:y1+64, x1:x1+64] = 0\n",
        "            masks.append(mask)\n",
        "        return np.array(masks)\n",
        "\n",
        "    def build_unet(self):\n",
        "        \"\"\"Build U-Net model\"\"\"\n",
        "        def conv_block(x, filters, kernel_size=3):\n",
        "            x = layers.Conv2D(filters, kernel_size, padding='same')(x)\n",
        "            x = layers.BatchNormalization()(x)\n",
        "            x = layers.ReLU()(x)\n",
        "            return x\n",
        "\n",
        "        inputs = layers.Input((self.img_size, self.img_size, 3))\n",
        "        mask = layers.Input((self.img_size, self.img_size, 1))\n",
        "\n",
        "        # Concatenate mask with input\n",
        "        x = layers.Concatenate()([inputs, mask])\n",
        "\n",
        "        # Encoder\n",
        "        e1 = conv_block(x, 64)\n",
        "        e2 = conv_block(layers.MaxPooling2D()(e1), 128)\n",
        "        e3 = conv_block(layers.MaxPooling2D()(e2), 256)\n",
        "\n",
        "        # Bridge\n",
        "        b = conv_block(layers.MaxPooling2D()(e3), 512)\n",
        "\n",
        "        # Decoder\n",
        "        d3 = conv_block(layers.UpSampling2D()(b), 256)\n",
        "        d3 = layers.Concatenate()([d3, e3])\n",
        "\n",
        "        d2 = conv_block(layers.UpSampling2D()(d3), 128)\n",
        "        d2 = layers.Concatenate()([d2, e2])\n",
        "\n",
        "        d1 = conv_block(layers.UpSampling2D()(d2), 64)\n",
        "        d1 = layers.Concatenate()([d1, e1])\n",
        "\n",
        "        outputs = layers.Conv2D(3, 1, activation='tanh')(d1)\n",
        "\n",
        "        return Model([inputs, mask], outputs)\n",
        "\n",
        "    def build_hint(self):\n",
        "        \"\"\"Build simplified HINT model for limited resources\"\"\"\n",
        "        def transformer_block(x, filters):\n",
        "            # Self-attention\n",
        "            attention = layers.MultiHeadAttention(\n",
        "                num_heads=4, key_dim=filters//4)(x, x, x)\n",
        "            x = layers.Add()([x, attention])\n",
        "            x = layers.LayerNormalization()(x)\n",
        "\n",
        "            # FFN\n",
        "            ffn = layers.Dense(filters*2)(x)\n",
        "            ffn = layers.ReLU()(ffn)\n",
        "            ffn = layers.Dense(filters)(ffn)\n",
        "\n",
        "            x = layers.Add()([x, ffn])\n",
        "            x = layers.LayerNormalization()(x)\n",
        "            return x\n",
        "\n",
        "        inputs = layers.Input((self.img_size, self.img_size, 3))\n",
        "        mask = layers.Input((self.img_size, self.img_size, 1))\n",
        "\n",
        "        x = layers.Concatenate()([inputs, mask])\n",
        "\n",
        "        # Simplified architecture for limited resources\n",
        "        x = conv_block(x, 64)\n",
        "        x = layers.Reshape((self.img_size * self.img_size, 64))(x)\n",
        "        x = transformer_block(x, 64)\n",
        "        x = layers.Reshape((self.img_size, self.img_size, 64))(x)\n",
        "\n",
        "        outputs = layers.Conv2D(3, 1, activation='tanh')(x)\n",
        "\n",
        "        return Model([inputs, mask], outputs)\n",
        "\n",
        "    def combined_model(self):\n",
        "        \"\"\"Combine U-Net and HINT models\"\"\"\n",
        "        inputs = layers.Input((self.img_size, self.img_size, 3))\n",
        "        mask = layers.Input((self.img_size, self.img_size, 1))\n",
        "\n",
        "        unet = self.build_unet()\n",
        "        hint = self.build_hint()\n",
        "\n",
        "        unet_out = unet([inputs, mask])\n",
        "        hint_out = hint([inputs, mask])\n",
        "\n",
        "        # Weighted combination\n",
        "        alpha = 0.7  # Weight for U-Net\n",
        "        outputs = layers.Lambda(\n",
        "            lambda x: alpha * x[0] + (1-alpha) * x[1])([unet_out, hint_out])\n",
        "\n",
        "        return Model([inputs, mask], outputs)\n",
        "\n",
        "    def evaluate_model(self, model, test_images, test_masks):\n",
        "        \"\"\"Evaluate model using various metrics\"\"\"\n",
        "        predictions = model.predict([test_images, test_masks])\n",
        "\n",
        "        # Calculate metrics\n",
        "        mse = np.mean((test_images - predictions) ** 2)\n",
        "        psnr = 20 * np.log10(2.0 / np.sqrt(mse))  # Assuming normalized [-1, 1]\n",
        "\n",
        "        # Visualize results\n",
        "        plt.figure(figsize=(15, 5))\n",
        "        for i in range(3):\n",
        "            plt.subplot(1, 3, i*3 + 1)\n",
        "            plt.imshow((test_images[i] + 1) / 2)\n",
        "            plt.title('Original')\n",
        "            plt.axis('off')\n",
        "\n",
        "            plt.subplot(1, 3, i*3 + 2)\n",
        "            masked = test_images[i] * test_masks[i]\n",
        "            plt.imshow((masked + 1) / 2)\n",
        "            plt.title('Masked')\n",
        "            plt.axis('off')\n",
        "\n",
        "            plt.subplot(1, 3, i*3 + 3)\n",
        "            plt.imshow((predictions[i] + 1) / 2)\n",
        "            plt.title('Inpainted')\n",
        "            plt.axis('off')\n",
        "\n",
        "        plt.show()\n",
        "        print(f\"MSE: {mse:.4f}\")\n",
        "        print(f\"PSNR: {psnr:.2f} dB\")\n",
        "\n",
        "    def train(self, epochs=10):\n",
        "        \"\"\"Train the model\"\"\"\n",
        "        # Load and prepare data\n",
        "        print(\"Loading dataset...\")\n",
        "        images = self.load_dataset()\n",
        "        self.perform_eda(images)\n",
        "\n",
        "        print(\"\\nPreprocessing images...\")\n",
        "        images = self.preprocess_images(images)\n",
        "        masks = self.create_masks(images.shape)\n",
        "\n",
        "        # Split dataset\n",
        "        train_images, test_images, train_masks, test_masks = train_test_split(\n",
        "            images, masks, test_size=0.2, random_state=42)\n",
        "\n",
        "        # Build and compile model\n",
        "        print(\"\\nBuilding model...\")\n",
        "        model = self.combined_model()\n",
        "        model.compile(\n",
        "            optimizer=tf.keras.optimizers.Adam(1e-4),\n",
        "            loss='mse',\n",
        "            metrics=['mae']\n",
        "        )\n",
        "\n",
        "        # Train\n",
        "        print(\"\\nTraining model...\")\n",
        "        history = model.fit(\n",
        "            [train_images, train_masks],\n",
        "            train_images,\n",
        "            batch_size=self.batch_size,\n",
        "            epochs=epochs,\n",
        "            validation_split=0.2\n",
        "        )\n",
        "\n",
        "        # Evaluate\n",
        "        print(\"\\nEvaluating model...\")\n",
        "        self.evaluate_model(model, test_images, test_masks)\n",
        "\n",
        "        return model, history\n",
        "\n",
        "# Usage example\n",
        "if __name__ == \"__main__\":\n",
        "    path = kagglehub.dataset_download(\"badasstechie/celebahq-resized-256x256\")\n",
        "\n",
        "    inpainting = ImageInpainting(\"path_to_dataset\")\n",
        "    model, history = inpainting.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "id": "LzXTCC7b-hxJ",
        "outputId": "aad3bbbf-48c5-40fe-e19f-bf53ab748db4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Cannot set memory growth on device when virtual devices configured",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-e8ffa33e2957>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mgpu\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgpus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_memory_growth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         tf.config.experimental.set_virtual_device_configuration(\n\u001b[1;32m     17\u001b[0m             \u001b[0mgpus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/config.py\u001b[0m in \u001b[0;36mset_memory_growth\u001b[0;34m(device, enable)\u001b[0m\n\u001b[1;32m    752\u001b[0m     \u001b[0mRuntimeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRuntime\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0malready\u001b[0m \u001b[0minitialized\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m   \"\"\"\n\u001b[0;32m--> 754\u001b[0;31m   \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_memory_growth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mset_memory_growth\u001b[0;34m(self, dev, enable)\u001b[0m\n\u001b[1;32m   1835\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1836\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdev\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_virtual_device_map\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1837\u001b[0;31m       raise ValueError(\n\u001b[0m\u001b[1;32m   1838\u001b[0m           \"Cannot set memory growth on device when virtual devices configured\")\n\u001b[1;32m   1839\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Cannot set memory growth on device when virtual devices configured"
          ]
        }
      ]
    }
  ]
}