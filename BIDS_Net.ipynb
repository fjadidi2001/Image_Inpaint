{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPmIBT5tsCkWtMeWPA5/0p+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fjadidi2001/Image_Inpaint/blob/main/BIDS_Net.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bidirectional Interaction Dual-Stream Network (BIDS-Net)"
      ],
      "metadata": {
        "id": "kJsyOcVtV11y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> The article proposes a novel approach for image inpainting called the Bidirectional Interaction Dual-Stream Network (BIDS-Net), integrating CNN and Transformer models to enhance inpainting quality by leveraging their complementary strengths."
      ],
      "metadata": {
        "id": "Ql2YbLBmWBQM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Methodology Overview\n",
        "1. Dual-Stream Structure:\n",
        "\n",
        "- CNN Stream: Captures rich local patterns and refines details.\n",
        "- Transformer Stream: Models long-range contextual correlations for global information.\n",
        "- Both streams are based on a U-shaped encoder-decoder structure to facilitate efficient multi-scale context reasoning.\n",
        "\n",
        "2. Bidirectional Feature Interaction (BFI):\n",
        "\n",
        "Implements **bidirectional feature alignment and fusion** between the CNN and Transformer streams.\n",
        "Employs **Selective Feature Fusion (SFF)** for adaptive feature integration by learning channel weights.\n",
        "3. Fast Global Self-Attention:\n",
        "\n",
        "Utilizes a **kernelizable fast-attention mechanism** for the Transformer, reducing computational complexity to linear.\n",
        "4. Loss Functions:\n",
        "\n",
        "Combines pixel-wise reconstruction, adversarial, perceptual, and style losses to ensure inpainting quality and perceptual consistency."
      ],
      "metadata": {
        "id": "Ub2RaBV8WZ-L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Channel allocation: Optimal performance when CNN and Transformer streams have equal importance.\n",
        "- Fusion methods: Bidirectional fusion outperforms unidirectional and unified-path approaches.\n",
        "- Specific fusion techniques: SFF surpasses element-wise addition and concatenation.\n",
        "- Number of random features: Optimal trade-off achieved with 72 orthogonal random features."
      ],
      "metadata": {
        "id": "RnduiCJ-ZoCn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Mask Creation Process**\n",
        "\n",
        "#### 1. **Purpose of Masking in Image Inpainting**\n",
        "   - Masks simulate corrupted regions by marking areas of an image for restoration.\n",
        "   - Masks represent regions with **value 1** (corrupted) and **value 0** (uncorrupted), facilitating selective processing during training.\n",
        "\n",
        "#### 2. **Mask Datasets**\n",
        "   - **Mask Set I**: Contains irregular shapes with various hole-to-image area ratios (10%–60%) to simulate real-world image corruption scenarios.\n",
        "   - **Mask Set II**: Focuses on **large-scale corruptions**, derived from a large mask sampling strategy, targeting challenges in **large-hole inpainting**.\n",
        "\n",
        "#### 3. **Techniques for Mask Creation**\n",
        "   - **Random Irregular Masks**:\n",
        "     - Generated using freehand-like curves and random polygons.\n",
        "     - Often involve **random rotations** and **flipping** for augmentation.\n",
        "   - **Large-Hole Masks**:\n",
        "     - Created by sampling large continuous regions, ensuring high diversity in shape and size.\n",
        "   - **Tools and Libraries**:\n",
        "     - Python libraries like **OpenCV** and **NumPy** for procedural generation of irregular shapes.\n",
        "     - **External mask datasets** for additional diversity, e.g., Mask datasets from previous works such as [29].\n",
        "\n",
        "---\n",
        "\n",
        "### **Model Architecture: BIDS-Net**\n",
        "\n",
        "#### 1. **Overall Structure**\n",
        "   - A **dual-stream network** combining **CNN** and **Transformer** models in a parallel design.\n",
        "   - Built on a **U-shaped encoder-decoder structure** for multi-scale feature extraction.\n",
        "\n",
        "#### 2. **Key Components**\n",
        "   - **CNN Stream**:\n",
        "     - Focus: Capturing **local patterns** for texture refinement.\n",
        "     - Built with **pre-activation residual blocks** for efficient and robust feature learning.\n",
        "   - **Transformer Stream**:\n",
        "     - Focus: Modeling **long-range contextual correlations**.\n",
        "     - Uses **fast global self-attention** for scalability and reduced computational overhead.\n",
        "   - **Bidirectional Feature Interaction (BFI)**:\n",
        "     - Bridges the CNN and Transformer streams with **feature alignment** and **adaptive fusion**.\n",
        "\n",
        "#### 3. **Detailed Implementation Steps**\n",
        "   - **Input Projection**:\n",
        "     - Corrupted images and masks are projected into separate feature spaces for the CNN and Transformer streams.\n",
        "     - Transformer features are downsampled to balance computational cost and performance.\n",
        "   - **Encoding Stage**:\n",
        "     - Each stream extracts features using **convolutional blocks (CNN)** and **Transformer blocks**.\n",
        "     - Features are fused bidirectionally via the **BFI module**.\n",
        "   - **Bottleneck Stage**:\n",
        "     - Features from both streams interact for enhanced context reasoning at the lowest spatial resolution.\n",
        "   - **Decoding Stage**:\n",
        "     - Outputs from both streams are upsampled and concatenated for final refinement.\n",
        "   - **Output Projection**:\n",
        "     - Combined features are transformed back to the image space for inpainting results.\n",
        "\n",
        "---\n",
        "\n",
        "### **Relevant Techniques and Algorithms**\n",
        "\n",
        "#### 1. **Fast Global Self-Attention**\n",
        "   - Reduces standard attention's quadratic complexity to linear using:\n",
        "     - **Kernelizable Attention**: Positive orthogonal random features replace softmax attention.\n",
        "     - Ensures **scalability** and efficiency for high-resolution images.\n",
        "\n",
        "#### 2. **Selective Feature Fusion (SFF)**\n",
        "   - Adapts weights for each channel during fusion, ensuring:\n",
        "     - CNN benefits from Transformer’s global context.\n",
        "     - Transformer incorporates CNN’s local details.\n",
        "   - Based on the **Selective Kernel Convolution** technique.\n",
        "\n",
        "#### 3. **Loss Functions**\n",
        "   - **Pixel-wise Reconstruction Loss**: Ensures pixel-level consistency.\n",
        "   - **Adversarial Loss**: Improves texture realism by incorporating a discriminator network.\n",
        "   - **Perceptual Loss**: Derived from a pre-trained VGG-19, enhancing perceptual similarity.\n",
        "   - **Style Loss**: Preserves stylistic details using Gram matrices.\n",
        "\n",
        "---\n",
        "\n",
        "### **Tools and Libraries**\n",
        "   - **Frameworks**: PyTorch (1.10.1), TensorFlow for alternate implementations.\n",
        "   - **Visualization**: Matplotlib or OpenCV for displaying masks and inpainted results.\n",
        "   - **GPU Hardware**: Tested on NVIDIA GeForce RTX 3090 for performance.\n",
        "\n",
        "---\n",
        "\n",
        "### **Considerations**\n",
        "   - **Mask Diversity**: Critical for generalization across various corruption scenarios.\n",
        "   - **Computational Efficiency**: Striking a balance between accuracy and runtime, particularly with Transformer integration.\n",
        "   - **Evaluation Metrics**:\n",
        "     - Quantitative: PSNR, SSIM, FID, LPIPS.\n",
        "     - Qualitative: Visual coherence and texture consistency.\n",
        "\n"
      ],
      "metadata": {
        "id": "8WqwVU4dfuIh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset"
      ],
      "metadata": {
        "id": "Ax71wBu70bTv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets -q\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sFeJjh1M0dJO",
        "outputId": "f92c19be-e536-4cda-be16-1b47cbd07886"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/480.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m471.0/480.6 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/116.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/179.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import random\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset,random_split\n",
        "from datasets import Dataset, DatasetDict\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "5BNpU_sl0joJ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name=\"caltech256-BIDS\"\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "\n",
        "CHECKPOINTS_DIR = '/content/drive/MyDrive/ckpts'\n",
        "\n",
        "def save_checkpoint(model, optimizer, epoch):\n",
        "    os.makedirs(CHECKPOINTS_DIR, exist_ok=True)\n",
        "    checkpoint_path = f'{CHECKPOINTS_DIR}/{model_name}.pth'\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "    }, checkpoint_path)\n",
        "    print(f\"ckpt saved for {model_name} at epoch {epoch}.\")\n",
        "\n",
        "def load_checkpoint(model, optimizer):\n",
        "    ckpt_path = f'{CHECKPOINTS_DIR}/{model_name}.pth'\n",
        "    if not os.path.exists(ckpt_path):\n",
        "        print(f\"no ckpt found for {model_name} starting from epoch 0.\")\n",
        "        return 0\n",
        "\n",
        "    checkpoint = torch.load(ckpt_path)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    start_epoch = checkpoint['epoch'] + 1\n",
        "    print(f\"ckpt loaded for {model_name} from {ckpt_path}. resuming from epoch {start_epoch}.\")\n",
        "\n",
        "    return start_epoch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IN73lAG-0ux8",
        "outputId": "56cb0cb7-1b19-4165-a7e8-be3feb5ed602"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"jessicali9530/caltech256\")\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hdg8NxId0utK",
        "outputId": "4200b806-95d8-4ee5-8a8d-d88f0b6ca29f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/jessicali9530/caltech256?dataset_version_number=2...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2.12G/2.12G [00:28<00:00, 79.4MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/jessicali9530/caltech256/versions/2\n"
          ]
        }
      ]
    }
  ]
}