{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM8yMbFayOkUgm31HgJyV5E",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fjadidi2001/Image_Inpaint/blob/main/BIDS_Net.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bidirectional Interaction Dual-Stream Network (BIDS-Net)"
      ],
      "metadata": {
        "id": "kJsyOcVtV11y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> The article proposes a novel approach for image inpainting called the Bidirectional Interaction Dual-Stream Network (BIDS-Net), integrating CNN and Transformer models to enhance inpainting quality by leveraging their complementary strengths."
      ],
      "metadata": {
        "id": "Ql2YbLBmWBQM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Methodology Overview\n",
        "1. Dual-Stream Structure:\n",
        "\n",
        "- CNN Stream: Captures rich local patterns and refines details.\n",
        "- Transformer Stream: Models long-range contextual correlations for global information.\n",
        "- Both streams are based on a U-shaped encoder-decoder structure to facilitate efficient multi-scale context reasoning.\n",
        "\n",
        "2. Bidirectional Feature Interaction (BFI):\n",
        "\n",
        "Implements **bidirectional feature alignment and fusion** between the CNN and Transformer streams.\n",
        "Employs **Selective Feature Fusion (SFF)** for adaptive feature integration by learning channel weights.\n",
        "3. Fast Global Self-Attention:\n",
        "\n",
        "Utilizes a **kernelizable fast-attention mechanism** for the Transformer, reducing computational complexity to linear.\n",
        "4. Loss Functions:\n",
        "\n",
        "Combines pixel-wise reconstruction, adversarial, perceptual, and style losses to ensure inpainting quality and perceptual consistency."
      ],
      "metadata": {
        "id": "Ub2RaBV8WZ-L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Channel allocation: Optimal performance when CNN and Transformer streams have equal importance.\n",
        "- Fusion methods: Bidirectional fusion outperforms unidirectional and unified-path approaches.\n",
        "- Specific fusion techniques: SFF surpasses element-wise addition and concatenation.\n",
        "- Number of random features: Optimal trade-off achieved with 72 orthogonal random features."
      ],
      "metadata": {
        "id": "RnduiCJ-ZoCn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Mask Creation Process**\n",
        "\n",
        "#### 1. **Purpose of Masking in Image Inpainting**\n",
        "   - Masks simulate corrupted regions by marking areas of an image for restoration.\n",
        "   - Masks represent regions with **value 1** (corrupted) and **value 0** (uncorrupted), facilitating selective processing during training.\n",
        "\n",
        "#### 2. **Mask Datasets**\n",
        "   - **Mask Set I**: Contains irregular shapes with various hole-to-image area ratios (10%–60%) to simulate real-world image corruption scenarios.\n",
        "   - **Mask Set II**: Focuses on **large-scale corruptions**, derived from a large mask sampling strategy, targeting challenges in **large-hole inpainting**.\n",
        "\n",
        "#### 3. **Techniques for Mask Creation**\n",
        "   - **Random Irregular Masks**:\n",
        "     - Generated using freehand-like curves and random polygons.\n",
        "     - Often involve **random rotations** and **flipping** for augmentation.\n",
        "   - **Large-Hole Masks**:\n",
        "     - Created by sampling large continuous regions, ensuring high diversity in shape and size.\n",
        "   - **Tools and Libraries**:\n",
        "     - Python libraries like **OpenCV** and **NumPy** for procedural generation of irregular shapes.\n",
        "     - **External mask datasets** for additional diversity, e.g., Mask datasets from previous works such as [29].\n",
        "\n",
        "---\n",
        "\n",
        "### **Model Architecture: BIDS-Net**\n",
        "\n",
        "#### 1. **Overall Structure**\n",
        "   - A **dual-stream network** combining **CNN** and **Transformer** models in a parallel design.\n",
        "   - Built on a **U-shaped encoder-decoder structure** for multi-scale feature extraction.\n",
        "\n",
        "#### 2. **Key Components**\n",
        "   - **CNN Stream**:\n",
        "     - Focus: Capturing **local patterns** for texture refinement.\n",
        "     - Built with **pre-activation residual blocks** for efficient and robust feature learning.\n",
        "   - **Transformer Stream**:\n",
        "     - Focus: Modeling **long-range contextual correlations**.\n",
        "     - Uses **fast global self-attention** for scalability and reduced computational overhead.\n",
        "   - **Bidirectional Feature Interaction (BFI)**:\n",
        "     - Bridges the CNN and Transformer streams with **feature alignment** and **adaptive fusion**.\n",
        "\n",
        "#### 3. **Detailed Implementation Steps**\n",
        "   - **Input Projection**:\n",
        "     - Corrupted images and masks are projected into separate feature spaces for the CNN and Transformer streams.\n",
        "     - Transformer features are downsampled to balance computational cost and performance.\n",
        "   - **Encoding Stage**:\n",
        "     - Each stream extracts features using **convolutional blocks (CNN)** and **Transformer blocks**.\n",
        "     - Features are fused bidirectionally via the **BFI module**.\n",
        "   - **Bottleneck Stage**:\n",
        "     - Features from both streams interact for enhanced context reasoning at the lowest spatial resolution.\n",
        "   - **Decoding Stage**:\n",
        "     - Outputs from both streams are upsampled and concatenated for final refinement.\n",
        "   - **Output Projection**:\n",
        "     - Combined features are transformed back to the image space for inpainting results.\n",
        "\n",
        "---\n",
        "\n",
        "### **Relevant Techniques and Algorithms**\n",
        "\n",
        "#### 1. **Fast Global Self-Attention**\n",
        "   - Reduces standard attention's quadratic complexity to linear using:\n",
        "     - **Kernelizable Attention**: Positive orthogonal random features replace softmax attention.\n",
        "     - Ensures **scalability** and efficiency for high-resolution images.\n",
        "\n",
        "#### 2. **Selective Feature Fusion (SFF)**\n",
        "   - Adapts weights for each channel during fusion, ensuring:\n",
        "     - CNN benefits from Transformer’s global context.\n",
        "     - Transformer incorporates CNN’s local details.\n",
        "   - Based on the **Selective Kernel Convolution** technique.\n",
        "\n",
        "#### 3. **Loss Functions**\n",
        "   - **Pixel-wise Reconstruction Loss**: Ensures pixel-level consistency.\n",
        "   - **Adversarial Loss**: Improves texture realism by incorporating a discriminator network.\n",
        "   - **Perceptual Loss**: Derived from a pre-trained VGG-19, enhancing perceptual similarity.\n",
        "   - **Style Loss**: Preserves stylistic details using Gram matrices.\n",
        "\n",
        "---\n",
        "\n",
        "### **Tools and Libraries**\n",
        "   - **Frameworks**: PyTorch (1.10.1), TensorFlow for alternate implementations.\n",
        "   - **Visualization**: Matplotlib or OpenCV for displaying masks and inpainted results.\n",
        "   - **GPU Hardware**: Tested on NVIDIA GeForce RTX 3090 for performance.\n",
        "\n",
        "---\n",
        "\n",
        "### **Considerations**\n",
        "   - **Mask Diversity**: Critical for generalization across various corruption scenarios.\n",
        "   - **Computational Efficiency**: Striking a balance between accuracy and runtime, particularly with Transformer integration.\n",
        "   - **Evaluation Metrics**:\n",
        "     - Quantitative: PSNR, SSIM, FID, LPIPS.\n",
        "     - Qualitative: Visual coherence and texture consistency.\n",
        "\n"
      ],
      "metadata": {
        "id": "8WqwVU4dfuIh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GdTr4zy-Vp7o"
      },
      "outputs": [],
      "source": []
    }
  ]
}