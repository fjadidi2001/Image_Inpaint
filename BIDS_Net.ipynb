{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPfU9yzVlzSnQ2vy0vnFTQU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fjadidi2001/Image_Inpaint/blob/main/BIDS_Net.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bidirectional Interaction Dual-Stream Network (BIDS-Net)"
      ],
      "metadata": {
        "id": "kJsyOcVtV11y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> The article proposes a novel approach for image inpainting called the Bidirectional Interaction Dual-Stream Network (BIDS-Net), integrating CNN and Transformer models to enhance inpainting quality by leveraging their complementary strengths."
      ],
      "metadata": {
        "id": "Ql2YbLBmWBQM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Methodology Overview\n",
        "1. Dual-Stream Structure:\n",
        "\n",
        "- CNN Stream: Captures rich local patterns and refines details.\n",
        "- Transformer Stream: Models long-range contextual correlations for global information.\n",
        "- Both streams are based on a U-shaped encoder-decoder structure to facilitate efficient multi-scale context reasoning.\n",
        "\n",
        "2. Bidirectional Feature Interaction (BFI):\n",
        "\n",
        "Implements **bidirectional feature alignment and fusion** between the CNN and Transformer streams.\n",
        "Employs **Selective Feature Fusion (SFF)** for adaptive feature integration by learning channel weights.\n",
        "3. Fast Global Self-Attention:\n",
        "\n",
        "Utilizes a **kernelizable fast-attention mechanism** for the Transformer, reducing computational complexity to linear.\n",
        "4. Loss Functions:\n",
        "\n",
        "Combines pixel-wise reconstruction, adversarial, perceptual, and style losses to ensure inpainting quality and perceptual consistency."
      ],
      "metadata": {
        "id": "Ub2RaBV8WZ-L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Channel allocation: Optimal performance when CNN and Transformer streams have equal importance.\n",
        "- Fusion methods: Bidirectional fusion outperforms unidirectional and unified-path approaches.\n",
        "- Specific fusion techniques: SFF surpasses element-wise addition and concatenation.\n",
        "- Number of random features: Optimal trade-off achieved with 72 orthogonal random features."
      ],
      "metadata": {
        "id": "RnduiCJ-ZoCn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Mask Creation Process**\n",
        "\n",
        "#### 1. **Purpose of Masking in Image Inpainting**\n",
        "   - Masks simulate corrupted regions by marking areas of an image for restoration.\n",
        "   - Masks represent regions with **value 1** (corrupted) and **value 0** (uncorrupted), facilitating selective processing during training.\n",
        "\n",
        "#### 2. **Mask Datasets**\n",
        "   - **Mask Set I**: Contains irregular shapes with various hole-to-image area ratios (10%–60%) to simulate real-world image corruption scenarios.\n",
        "   - **Mask Set II**: Focuses on **large-scale corruptions**, derived from a large mask sampling strategy, targeting challenges in **large-hole inpainting**.\n",
        "\n",
        "#### 3. **Techniques for Mask Creation**\n",
        "   - **Random Irregular Masks**:\n",
        "     - Generated using freehand-like curves and random polygons.\n",
        "     - Often involve **random rotations** and **flipping** for augmentation.\n",
        "   - **Large-Hole Masks**:\n",
        "     - Created by sampling large continuous regions, ensuring high diversity in shape and size.\n",
        "   - **Tools and Libraries**:\n",
        "     - Python libraries like **OpenCV** and **NumPy** for procedural generation of irregular shapes.\n",
        "     - **External mask datasets** for additional diversity, e.g., Mask datasets from previous works such as [29].\n",
        "\n",
        "---\n",
        "\n",
        "### **Model Architecture: BIDS-Net**\n",
        "\n",
        "#### 1. **Overall Structure**\n",
        "   - A **dual-stream network** combining **CNN** and **Transformer** models in a parallel design.\n",
        "   - Built on a **U-shaped encoder-decoder structure** for multi-scale feature extraction.\n",
        "\n",
        "#### 2. **Key Components**\n",
        "   - **CNN Stream**:\n",
        "     - Focus: Capturing **local patterns** for texture refinement.\n",
        "     - Built with **pre-activation residual blocks** for efficient and robust feature learning.\n",
        "   - **Transformer Stream**:\n",
        "     - Focus: Modeling **long-range contextual correlations**.\n",
        "     - Uses **fast global self-attention** for scalability and reduced computational overhead.\n",
        "   - **Bidirectional Feature Interaction (BFI)**:\n",
        "     - Bridges the CNN and Transformer streams with **feature alignment** and **adaptive fusion**.\n",
        "\n",
        "#### 3. **Detailed Implementation Steps**\n",
        "   - **Input Projection**:\n",
        "     - Corrupted images and masks are projected into separate feature spaces for the CNN and Transformer streams.\n",
        "     - Transformer features are downsampled to balance computational cost and performance.\n",
        "   - **Encoding Stage**:\n",
        "     - Each stream extracts features using **convolutional blocks (CNN)** and **Transformer blocks**.\n",
        "     - Features are fused bidirectionally via the **BFI module**.\n",
        "   - **Bottleneck Stage**:\n",
        "     - Features from both streams interact for enhanced context reasoning at the lowest spatial resolution.\n",
        "   - **Decoding Stage**:\n",
        "     - Outputs from both streams are upsampled and concatenated for final refinement.\n",
        "   - **Output Projection**:\n",
        "     - Combined features are transformed back to the image space for inpainting results.\n",
        "\n",
        "---\n",
        "\n",
        "### **Relevant Techniques and Algorithms**\n",
        "\n",
        "#### 1. **Fast Global Self-Attention**\n",
        "   - Reduces standard attention's quadratic complexity to linear using:\n",
        "     - **Kernelizable Attention**: Positive orthogonal random features replace softmax attention.\n",
        "     - Ensures **scalability** and efficiency for high-resolution images.\n",
        "\n",
        "#### 2. **Selective Feature Fusion (SFF)**\n",
        "   - Adapts weights for each channel during fusion, ensuring:\n",
        "     - CNN benefits from Transformer’s global context.\n",
        "     - Transformer incorporates CNN’s local details.\n",
        "   - Based on the **Selective Kernel Convolution** technique.\n",
        "\n",
        "#### 3. **Loss Functions**\n",
        "   - **Pixel-wise Reconstruction Loss**: Ensures pixel-level consistency.\n",
        "   - **Adversarial Loss**: Improves texture realism by incorporating a discriminator network.\n",
        "   - **Perceptual Loss**: Derived from a pre-trained VGG-19, enhancing perceptual similarity.\n",
        "   - **Style Loss**: Preserves stylistic details using Gram matrices.\n",
        "\n",
        "---\n",
        "\n",
        "### **Tools and Libraries**\n",
        "   - **Frameworks**: PyTorch (1.10.1), TensorFlow for alternate implementations.\n",
        "   - **Visualization**: Matplotlib or OpenCV for displaying masks and inpainted results.\n",
        "   - **GPU Hardware**: Tested on NVIDIA GeForce RTX 3090 for performance.\n",
        "\n",
        "---\n",
        "\n",
        "### **Considerations**\n",
        "   - **Mask Diversity**: Critical for generalization across various corruption scenarios.\n",
        "   - **Computational Efficiency**: Striking a balance between accuracy and runtime, particularly with Transformer integration.\n",
        "   - **Evaluation Metrics**:\n",
        "     - Quantitative: PSNR, SSIM, FID, LPIPS.\n",
        "     - Qualitative: Visual coherence and texture consistency.\n",
        "\n"
      ],
      "metadata": {
        "id": "8WqwVU4dfuIh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset"
      ],
      "metadata": {
        "id": "Ax71wBu70bTv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets -q\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sFeJjh1M0dJO",
        "outputId": "fe723869-0b98-4395-f4eb-fd81075ae15b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/480.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/116.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/179.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import random\n",
        "from PIL import Image\n",
        "from datasets import Dataset, DatasetDict\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchvision import transforms\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "import wandb"
      ],
      "metadata": {
        "id": "5BNpU_sl0joJ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name=\"caltech256-BIDS\"\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "\n",
        "CHECKPOINTS_DIR = '/content/drive/MyDrive/ckpts'\n",
        "\n",
        "def save_checkpoint(model, optimizer, epoch):\n",
        "    os.makedirs(CHECKPOINTS_DIR, exist_ok=True)\n",
        "    checkpoint_path = f'{CHECKPOINTS_DIR}/{model_name}.pth'\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "    }, checkpoint_path)\n",
        "    print(f\"ckpt saved for {model_name} at epoch {epoch}.\")\n",
        "\n",
        "def load_checkpoint(model, optimizer):\n",
        "    ckpt_path = f'{CHECKPOINTS_DIR}/{model_name}.pth'\n",
        "    if not os.path.exists(ckpt_path):\n",
        "        print(f\"no ckpt found for {model_name} starting from epoch 0.\")\n",
        "        return 0\n",
        "\n",
        "    checkpoint = torch.load(ckpt_path)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    start_epoch = checkpoint['epoch'] + 1\n",
        "    print(f\"ckpt loaded for {model_name} from {ckpt_path}. resuming from epoch {start_epoch}.\")\n",
        "\n",
        "    return start_epoch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IN73lAG-0ux8",
        "outputId": "fd483d27-9faa-46ac-83c0-8ed214bd18a5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"jessicali9530/caltech256\")\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hdg8NxId0utK",
        "outputId": "e283f933-69cd-471f-8e37-9685fc920f67"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/jessicali9530/caltech256?dataset_version_number=2...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2.12G/2.12G [00:25<00:00, 88.8MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/jessicali9530/caltech256/versions/2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Complete Image Inpainting Implementation\n",
        "\n"
      ],
      "metadata": {
        "id": "6PmoDqKU5X76"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fLrToOiI6tTN",
        "outputId": "a6fd8a2b-26d2-4fce-fa67-2f2de61df5f0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.19.1)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.25.5)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.10.4)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.19.2)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (75.1.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.12.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.6->wandb) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.12.14)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Constants and Configurations\n"
      ],
      "metadata": {
        "id": "UwXKh2fvB6OO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CONFIG = {\n",
        "    'BATCH_SIZE': 16,\n",
        "    'EPOCHS': 100,\n",
        "    'LEARNING_RATE': 0.001,\n",
        "    'IMAGE_SIZE': 256,\n",
        "    'TRAIN_SPLIT': 0.8,\n",
        "    'SAVE_INTERVAL': 5\n",
        "}\n"
      ],
      "metadata": {
        "id": "ymLw4YDBB4PR"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Dataset and Mask Generation"
      ],
      "metadata": {
        "id": "9vDfz6YkCIac"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Caltech256Dataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        # self.image_size = image_size\n",
        "        self.image_paths = glob.glob(os.path.join(root_dir, \"**/*.jpg\"), recursive=True)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        mask = generate_random_mask(image.shape[1], image.shape[2])\n",
        "        mask = torch.from_numpy(mask).float()\n",
        "\n",
        "        return image, mask\n",
        "\n",
        "def generate_random_mask(height, width):\n",
        "    if random.random() > 0.5:\n",
        "        return generate_irregular_mask(height, width)\n",
        "    return generate_large_hole_mask(height, width)\n",
        "\n",
        "def generate_irregular_mask(height, width, max_vertices=12, max_brush_width=50):\n",
        "    mask = np.zeros((height, width), dtype=np.uint8)\n",
        "    num_vertices = random.randint(3, max_vertices)\n",
        "    vertices = np.array([[\n",
        "        random.randint(0, width),\n",
        "        random.randint(0, height)\n",
        "    ] for _ in range(num_vertices)], dtype=np.int32)\n",
        "    cv2.fillPoly(mask, [vertices], 1)\n",
        "\n",
        "    for _ in range(random.randint(1, 5)):\n",
        "        start_point = (random.randint(0, width), random.randint(0, height))\n",
        "        end_point = (random.randint(0, width), random.randint(0, height))\n",
        "        thickness = random.randint(10, max_brush_width)\n",
        "        cv2.line(mask, start_point, end_point, 1, thickness)\n",
        "\n",
        "    return mask\n",
        "\n",
        "def generate_large_hole_mask(height, width, min_size=0.3, max_size=0.6):\n",
        "    mask = np.zeros((height, width), dtype=np.uint8)\n",
        "    hole_size = random.uniform(min_size, max_size)\n",
        "    hole_height = int(height * hole_size)\n",
        "    hole_width = int(width * hole_size)\n",
        "\n",
        "    x = random.randint(0, width - hole_width)\n",
        "    y = random.randint(0, height - hole_height)\n",
        "    cv2.rectangle(mask, (x, y), (x + hole_width, y + hole_height), 1, -1)\n",
        "\n",
        "    return mask\n"
      ],
      "metadata": {
        "id": "3RPPYyEkCEW-"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Model Architecture\n",
        "class PreActResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(PreActResidualBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
        "        self.norm1 = nn.BatchNorm2d(in_channels)\n",
        "        self.norm2 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        # Skip connection for channel dimension mismatch\n",
        "        self.skip = nn.Conv2d(in_channels, out_channels, kernel_size=1) if in_channels != out_channels else nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = self.skip(x)\n",
        "\n",
        "        out = self.norm1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv1(out)\n",
        "\n",
        "        out = self.norm2(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "\n",
        "        return out + identity\n",
        "\n",
        "class FastGlobalSelfAttention(nn.Module):\n",
        "    def __init__(self, dim, num_heads):\n",
        "        super(FastGlobalSelfAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.scale = (dim // num_heads) ** -0.5\n",
        "        self.to_qkv = nn.Linear(dim, dim * 3)\n",
        "        self.to_out = nn.Linear(dim, dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        x = x.flatten(2).transpose(1, 2)  # (B, H*W, C)\n",
        "\n",
        "        qkv = self.to_qkv(x).chunk(3, dim=-1)\n",
        "        q, k, v = map(lambda t: t.view(B, -1, self.num_heads, C // self.num_heads).transpose(1, 2), qkv)\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "\n",
        "        out = (attn @ v).transpose(1, 2).reshape(B, H * W, C)\n",
        "        out = self.to_out(out)\n",
        "        out = out.transpose(1, 2).view(B, C, H, W)\n",
        "\n",
        "        return out\n",
        "\n",
        "class BFI(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super(BFI, self).__init__()\n",
        "        self.cnn_norm = nn.BatchNorm2d(channels)\n",
        "        self.trans_norm = nn.BatchNorm2d(channels)\n",
        "        self.fusion = nn.Conv2d(channels * 2, channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, cnn_feat, trans_feat):\n",
        "        cnn_feat = self.cnn_norm(cnn_feat)\n",
        "        trans_feat = self.trans_norm(trans_feat)\n",
        "        fused = torch.cat([cnn_feat, trans_feat], dim=1)\n",
        "        return self.fusion(fused)\n",
        "\n",
        "class BIDS_Net(nn.Module):\n",
        "    def __init__(self, in_channels=3, out_channels=3, num_heads=8):\n",
        "        super(BIDS_Net, self).__init__()\n",
        "\n",
        "        # CNN Stream\n",
        "        self.cnn_stream = nn.ModuleList([\n",
        "            PreActResidualBlock(in_channels, 64),\n",
        "            PreActResidualBlock(64, 128),\n",
        "            PreActResidualBlock(128, 256)\n",
        "        ])\n",
        "\n",
        "        # Transformer Stream\n",
        "        self.trans_proj = nn.Conv2d(in_channels, 64, kernel_size=1)\n",
        "        self.trans_stream = nn.ModuleList([\n",
        "            FastGlobalSelfAttention(64, num_heads),\n",
        "            FastGlobalSelfAttention(128, num_heads),\n",
        "            FastGlobalSelfAttention(256, num_heads)\n",
        "        ])\n",
        "\n",
        "        # Channel Adjustments for Transformer Stream\n",
        "        self.trans_ch_adj = nn.ModuleList([\n",
        "            nn.Conv2d(64, 128, kernel_size=1),\n",
        "            nn.Conv2d(128, 256, kernel_size=1)\n",
        "        ])\n",
        "\n",
        "        # BFI Modules\n",
        "        self.bfi1 = BFI(64)\n",
        "        self.bfi2 = BFI(128)\n",
        "        self.bfi3 = BFI(256)\n",
        "\n",
        "        # Output Projection\n",
        "        self.output = nn.Sequential(\n",
        "            nn.Conv2d(256, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, out_channels, kernel_size=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        # CNN Stream\n",
        "        cnn_feat1 = self.cnn_stream[0](x)\n",
        "        cnn_feat2 = self.cnn_stream[1](cnn_feat1)\n",
        "        cnn_feat3 = self.cnn_stream[2](cnn_feat2)\n",
        "\n",
        "        # Transformer Stream\n",
        "        trans_feat = self.trans_proj(x)\n",
        "        trans_feat1 = self.trans_stream[0](trans_feat)\n",
        "\n",
        "        trans_feat = self.trans_ch_adj[0](trans_feat1)\n",
        "        trans_feat2 = self.trans_stream[1](trans_feat)\n",
        "\n",
        "        trans_feat = self.trans_ch_adj[1](trans_feat2)\n",
        "        trans_feat3 = self.trans_stream[2](trans_feat)\n",
        "\n",
        "        # BFI Fusion\n",
        "        fused1 = self.bfi1(cnn_feat1, trans_feat1)\n",
        "        fused2 = self.bfi2(cnn_feat2, trans_feat2)\n",
        "        fused3 = self.bfi3(cnn_feat3, trans_feat3)\n",
        "\n",
        "        # Output\n",
        "        out = self.output(fused3)\n",
        "        return out * mask + x * (1 - mask)"
      ],
      "metadata": {
        "id": "7avkeJ_cCk24"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Training and Evaluation Functions\n",
        "def train_one_epoch(model, dataloader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    pbar = tqdm(dataloader, desc='Training')\n",
        "\n",
        "    for images, masks in pbar:\n",
        "        images = images.to(device)\n",
        "        masks = masks.to(device)\n",
        "\n",
        "        masked_images = images * (1 - masks.unsqueeze(1))\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(masked_images, masks.unsqueeze(1))\n",
        "        loss = criterion(outputs, images)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        pbar.set_postfix({'loss': loss.item()})\n",
        "\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "def validate(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, masks in dataloader:\n",
        "            images = images.to(device)\n",
        "            masks = masks.to(device)\n",
        "\n",
        "            masked_images = images * (1 - masks.unsqueeze(1))\n",
        "            outputs = model(masked_images, masks.unsqueeze(1))\n",
        "            loss = criterion(outputs, images)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(dataloader)"
      ],
      "metadata": {
        "id": "JT7b3onAC4CL"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Checkpoint Functions\n",
        "def save_checkpoint(model, optimizer, epoch, best_loss, path):\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'best_loss': best_loss\n",
        "    }, path)\n",
        "\n",
        "def load_checkpoint(model, optimizer, path):\n",
        "    if not os.path.exists(path):\n",
        "        return 0, float('inf')\n",
        "\n",
        "    checkpoint = torch.load(path)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    return checkpoint['epoch'] + 1, checkpoint['best_loss']"
      ],
      "metadata": {
        "id": "aBcxQrthDFnu"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Visualization Functions\n",
        "def visualize_results(model, dataloader, device, save_path=None):\n",
        "    model.eval()\n",
        "    denorm = transforms.Normalize(\n",
        "        mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],\n",
        "        std=[1/0.229, 1/0.224, 1/0.225]\n",
        "    )\n",
        "\n",
        "    with torch.no_grad():\n",
        "        images, masks = next(iter(dataloader))\n",
        "        images = images[:5].to(device)  # Take first 5 samples\n",
        "        masks = masks[:5].to(device)\n",
        "\n",
        "        masked_images = images * (1 - masks.unsqueeze(1))\n",
        "        outputs = model(masked_images, masks.unsqueeze(1))\n",
        "\n",
        "        # Denormalize images\n",
        "        images = denorm(images).cpu()\n",
        "        masked_images = denorm(masked_images).cpu()\n",
        "        outputs = denorm(outputs).cpu()\n",
        "\n",
        "        # Create visualization\n",
        "        fig, axes = plt.subplots(5, 3, figsize=(12, 20))\n",
        "\n",
        "        for i in range(5):\n",
        "            axes[i, 0].imshow(images[i].permute(1, 2, 0).clip(0, 1))\n",
        "            axes[i, 0].set_title('Original')\n",
        "            axes[i, 1].imshow(masked_images[i].permute(1, 2, 0).clip(0, 1))\n",
        "            axes[i, 1].set_title('Masked')\n",
        "            axes[i, 2].imshow(outputs[i].permute(1, 2, 0).clip(0, 1))\n",
        "            axes[i, 2].set_title('Inpainted')\n",
        "\n",
        "            for ax in axes[i]:\n",
        "                ax.axis('off')\n",
        "\n",
        "        plt.tight_layout()\n",
        "\n",
        "        if save_path:\n",
        "            plt.savefig(save_path)\n",
        "            plt.close()\n",
        "        else:\n",
        "            plt.show()\n"
      ],
      "metadata": {
        "id": "YX2M-KKTDS_A"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = kagglehub.dataset_download(\"jessicali9530/caltech256\")\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1UfTZZzjFMJo",
        "outputId": "c777e753-1a38-4029-857e-e13ba07146ab"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/jessicali9530/caltech256/versions/2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gp1X40NLFSno",
        "outputId": "229b53be-2da3-443e-d01f-21fafc135cb2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/root/.cache/kagglehub/datasets/jessicali9530/caltech256/versions/2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # Memory optimization settings\n",
        "    torch.cuda.empty_cache()  # Clear cache before starting\n",
        "    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
        "\n",
        "    # Reduced hyperparameters\n",
        "    config = {\n",
        "        'batch_size': 4,  # Reduced from 16\n",
        "        'epochs': 100,\n",
        "        'learning_rate': 0.001,\n",
        "        'image_size': 192,  # Reduced from 256\n",
        "        'num_workers': 2,   # Reduced from 4\n",
        "        'checkpoint_dir': 'checkpoints',\n",
        "        'results_dir': 'results',\n",
        "        'accumulation_steps': 4  # New parameter for gradient accumulation\n",
        "    }\n",
        "\n",
        "    # Initialize wandb with lower log frequency\n",
        "    wandb.init(project=\"image-inpainting\",\n",
        "               name=\"BIDS-Net-training\",\n",
        "               config=config)\n",
        "\n",
        "    # Data transforms with smaller image size\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((config['image_size'], config['image_size'])),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                           std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    # Create datasets and dataloaders with memory-efficient settings\n",
        "    dataset = Caltech256Dataset(\n",
        "        root_dir=\"/root/.cache/kagglehub/datasets/jessicali9530/caltech256/versions/2\",\n",
        "        transform=transform,\n",
        "        image_size=config['image_size']\n",
        "    )\n",
        "\n",
        "    train_size = int(0.8 * len(dataset))\n",
        "    val_size = len(dataset) - train_size\n",
        "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=config['batch_size'],\n",
        "        shuffle=True,\n",
        "        num_workers=config['num_workers'],\n",
        "        pin_memory=False,  # Disabled pin_memory to reduce memory usage\n",
        "        persistent_workers=True  # Keep workers alive between batches\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=config['batch_size'],\n",
        "        shuffle=False,\n",
        "        num_workers=config['num_workers'],\n",
        "        pin_memory=False,\n",
        "        persistent_workers=True\n",
        "    )\n",
        "\n",
        "    # Initialize model with memory optimizations\n",
        "    model = BIDS_Net().to(device)\n",
        "    if torch.cuda.device_count() > 1:\n",
        "        model = nn.DataParallel(model)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
        "    criterion = nn.L1Loss()\n",
        "    scaler = torch.cuda.amp.GradScaler()  # For mixed precision training\n",
        "\n",
        "    # Modified training loop with memory optimizations\n",
        "    for epoch in range(config['epochs']):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        for i, (images, masks) in enumerate(train_loader):\n",
        "            images = images.to(device)\n",
        "            masks = masks.to(device)\n",
        "\n",
        "            # Mixed precision training\n",
        "            with torch.cuda.amp.autocast():\n",
        "                masked_images = images * (1 - masks.unsqueeze(1))\n",
        "                outputs = model(masked_images, masks.unsqueeze(1))\n",
        "                loss = criterion(outputs, images)\n",
        "                loss = loss / config['accumulation_steps']  # Normalize loss\n",
        "\n",
        "            # Gradient accumulation\n",
        "            scaler.scale(loss).backward()\n",
        "\n",
        "            if (i + 1) % config['accumulation_steps'] == 0:\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "            running_loss += loss.item() * config['accumulation_steps']\n",
        "\n",
        "            # Clear cache periodically\n",
        "            if i % 10 == 0:\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "            # Log less frequently\n",
        "            if i % 100 == 0:\n",
        "                print(f'Epoch [{epoch+1}/{config[\"epochs\"]}], '\n",
        "                      f'Step [{i+1}/{len(train_loader)}], '\n",
        "                      f'Loss: {running_loss/(i+1):.4f}')\n",
        "\n",
        "        # Validation with memory optimization\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for images, masks in val_loader:\n",
        "                images = images.to(device)\n",
        "                masks = masks.to(device)\n",
        "\n",
        "                with torch.cuda.amp.autocast():\n",
        "                    masked_images = images * (1 - masks.unsqueeze(1))\n",
        "                    outputs = model(masked_images, masks.unsqueeze(1))\n",
        "                    loss = criterion(outputs, images)\n",
        "\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                # Clear cache periodically\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "        val_loss /= len(val_loader)\n",
        "\n",
        "        # Save checkpoint with reduced frequency\n",
        "        if (epoch + 1) % 5 == 0:\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'loss': running_loss / len(train_loader),\n",
        "            }, f'{config[\"checkpoint_dir\"]}/checkpoint_epoch_{epoch+1}.pth')\n",
        "\n",
        "        # Visualize results less frequently\n",
        "        if (epoch + 1) % 10 == 0:  # Changed from 5 to 10\n",
        "            visualize_results(model, val_loader, device, epoch, config['results_dir'])\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "JAeq1Awg5bwo",
        "outputId": "d3534a3c-9223-41af-ddd0-608bde16dd5a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Caltech256Dataset.__init__() got an unexpected keyword argument 'image_size'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-7efd2463d638>\u001b[0m in \u001b[0;36m<cell line: 139>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-21-7efd2463d638>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;31m# Create datasets and dataloaders with memory-efficient settings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     dataset = Caltech256Dataset(\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mroot_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/root/.cache/kagglehub/datasets/jessicali9530/caltech256/versions/2\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Caltech256Dataset.__init__() got an unexpected keyword argument 'image_size'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5ck7OTfTDxg7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}