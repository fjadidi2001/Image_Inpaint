{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNXCaHWfrQLHzp48MF8CGec",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fjadidi2001/Image_Inpaint/blob/main/BIDS_Net.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bidirectional Interaction Dual-Stream Network (BIDS-Net)"
      ],
      "metadata": {
        "id": "kJsyOcVtV11y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> The article proposes a novel approach for image inpainting called the Bidirectional Interaction Dual-Stream Network (BIDS-Net), integrating CNN and Transformer models to enhance inpainting quality by leveraging their complementary strengths."
      ],
      "metadata": {
        "id": "Ql2YbLBmWBQM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Methodology Overview\n",
        "1. Dual-Stream Structure:\n",
        "\n",
        "- CNN Stream: Captures rich local patterns and refines details.\n",
        "- Transformer Stream: Models long-range contextual correlations for global information.\n",
        "- Both streams are based on a U-shaped encoder-decoder structure to facilitate efficient multi-scale context reasoning.\n",
        "\n",
        "2. Bidirectional Feature Interaction (BFI):\n",
        "\n",
        "Implements **bidirectional feature alignment and fusion** between the CNN and Transformer streams.\n",
        "Employs **Selective Feature Fusion (SFF)** for adaptive feature integration by learning channel weights.\n",
        "3. Fast Global Self-Attention:\n",
        "\n",
        "Utilizes a **kernelizable fast-attention mechanism** for the Transformer, reducing computational complexity to linear.\n",
        "4. Loss Functions:\n",
        "\n",
        "Combines pixel-wise reconstruction, adversarial, perceptual, and style losses to ensure inpainting quality and perceptual consistency."
      ],
      "metadata": {
        "id": "Ub2RaBV8WZ-L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Channel allocation: Optimal performance when CNN and Transformer streams have equal importance.\n",
        "- Fusion methods: Bidirectional fusion outperforms unidirectional and unified-path approaches.\n",
        "- Specific fusion techniques: SFF surpasses element-wise addition and concatenation.\n",
        "- Number of random features: Optimal trade-off achieved with 72 orthogonal random features."
      ],
      "metadata": {
        "id": "RnduiCJ-ZoCn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Mask Creation Process**\n",
        "\n",
        "#### 1. **Purpose of Masking in Image Inpainting**\n",
        "   - Masks simulate corrupted regions by marking areas of an image for restoration.\n",
        "   - Masks represent regions with **value 1** (corrupted) and **value 0** (uncorrupted), facilitating selective processing during training.\n",
        "\n",
        "#### 2. **Mask Datasets**\n",
        "   - **Mask Set I**: Contains irregular shapes with various hole-to-image area ratios (10%–60%) to simulate real-world image corruption scenarios.\n",
        "   - **Mask Set II**: Focuses on **large-scale corruptions**, derived from a large mask sampling strategy, targeting challenges in **large-hole inpainting**.\n",
        "\n",
        "#### 3. **Techniques for Mask Creation**\n",
        "   - **Random Irregular Masks**:\n",
        "     - Generated using freehand-like curves and random polygons.\n",
        "     - Often involve **random rotations** and **flipping** for augmentation.\n",
        "   - **Large-Hole Masks**:\n",
        "     - Created by sampling large continuous regions, ensuring high diversity in shape and size.\n",
        "   - **Tools and Libraries**:\n",
        "     - Python libraries like **OpenCV** and **NumPy** for procedural generation of irregular shapes.\n",
        "     - **External mask datasets** for additional diversity, e.g., Mask datasets from previous works such as [29].\n",
        "\n",
        "---\n",
        "\n",
        "### **Model Architecture: BIDS-Net**\n",
        "\n",
        "#### 1. **Overall Structure**\n",
        "   - A **dual-stream network** combining **CNN** and **Transformer** models in a parallel design.\n",
        "   - Built on a **U-shaped encoder-decoder structure** for multi-scale feature extraction.\n",
        "\n",
        "#### 2. **Key Components**\n",
        "   - **CNN Stream**:\n",
        "     - Focus: Capturing **local patterns** for texture refinement.\n",
        "     - Built with **pre-activation residual blocks** for efficient and robust feature learning.\n",
        "   - **Transformer Stream**:\n",
        "     - Focus: Modeling **long-range contextual correlations**.\n",
        "     - Uses **fast global self-attention** for scalability and reduced computational overhead.\n",
        "   - **Bidirectional Feature Interaction (BFI)**:\n",
        "     - Bridges the CNN and Transformer streams with **feature alignment** and **adaptive fusion**.\n",
        "\n",
        "#### 3. **Detailed Implementation Steps**\n",
        "   - **Input Projection**:\n",
        "     - Corrupted images and masks are projected into separate feature spaces for the CNN and Transformer streams.\n",
        "     - Transformer features are downsampled to balance computational cost and performance.\n",
        "   - **Encoding Stage**:\n",
        "     - Each stream extracts features using **convolutional blocks (CNN)** and **Transformer blocks**.\n",
        "     - Features are fused bidirectionally via the **BFI module**.\n",
        "   - **Bottleneck Stage**:\n",
        "     - Features from both streams interact for enhanced context reasoning at the lowest spatial resolution.\n",
        "   - **Decoding Stage**:\n",
        "     - Outputs from both streams are upsampled and concatenated for final refinement.\n",
        "   - **Output Projection**:\n",
        "     - Combined features are transformed back to the image space for inpainting results.\n",
        "\n",
        "---\n",
        "\n",
        "### **Relevant Techniques and Algorithms**\n",
        "\n",
        "#### 1. **Fast Global Self-Attention**\n",
        "   - Reduces standard attention's quadratic complexity to linear using:\n",
        "     - **Kernelizable Attention**: Positive orthogonal random features replace softmax attention.\n",
        "     - Ensures **scalability** and efficiency for high-resolution images.\n",
        "\n",
        "#### 2. **Selective Feature Fusion (SFF)**\n",
        "   - Adapts weights for each channel during fusion, ensuring:\n",
        "     - CNN benefits from Transformer’s global context.\n",
        "     - Transformer incorporates CNN’s local details.\n",
        "   - Based on the **Selective Kernel Convolution** technique.\n",
        "\n",
        "#### 3. **Loss Functions**\n",
        "   - **Pixel-wise Reconstruction Loss**: Ensures pixel-level consistency.\n",
        "   - **Adversarial Loss**: Improves texture realism by incorporating a discriminator network.\n",
        "   - **Perceptual Loss**: Derived from a pre-trained VGG-19, enhancing perceptual similarity.\n",
        "   - **Style Loss**: Preserves stylistic details using Gram matrices.\n",
        "\n",
        "---\n",
        "\n",
        "### **Tools and Libraries**\n",
        "   - **Frameworks**: PyTorch (1.10.1), TensorFlow for alternate implementations.\n",
        "   - **Visualization**: Matplotlib or OpenCV for displaying masks and inpainted results.\n",
        "   - **GPU Hardware**: Tested on NVIDIA GeForce RTX 3090 for performance.\n",
        "\n",
        "---\n",
        "\n",
        "### **Considerations**\n",
        "   - **Mask Diversity**: Critical for generalization across various corruption scenarios.\n",
        "   - **Computational Efficiency**: Striking a balance between accuracy and runtime, particularly with Transformer integration.\n",
        "   - **Evaluation Metrics**:\n",
        "     - Quantitative: PSNR, SSIM, FID, LPIPS.\n",
        "     - Qualitative: Visual coherence and texture consistency.\n",
        "\n"
      ],
      "metadata": {
        "id": "8WqwVU4dfuIh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "GdTr4zy-Vp7o"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "def generate_irregular_mask(height, width, max_vertices=12, max_brush_width=50):\n",
        "    \"\"\"\n",
        "    Generates an irregular mask with random shapes.\n",
        "    Args:\n",
        "        height (int): Height of the mask.\n",
        "        width (int): Width of the mask.\n",
        "        max_vertices (int): Maximum number of vertices for random polygons.\n",
        "        max_brush_width (int): Maximum brush width for freehand-like curves.\n",
        "    Returns:\n",
        "        mask (np.array): Binary mask with irregular shapes.\n",
        "    \"\"\"\n",
        "    mask = np.zeros((height, width), dtype=np.uint8)\n",
        "    num_vertices = random.randint(3, max_vertices)\n",
        "    vertices = np.array([[\n",
        "        random.randint(0, width),\n",
        "        random.randint(0, height)\n",
        "    ] for _ in range(num_vertices)], dtype=np.int32)\n",
        "    cv2.fillPoly(mask, [vertices], 1)\n",
        "\n",
        "    # Add freehand-like curves\n",
        "    for _ in range(random.randint(1, 5)):\n",
        "        start_point = (random.randint(0, width), random.randint(0, height))\n",
        "        end_point = (random.randint(0, width), random.randint(0, height))\n",
        "        thickness = random.randint(10, max_brush_width)\n",
        "        cv2.line(mask, start_point, end_point, 1, thickness)\n",
        "\n",
        "    return mask\n",
        "\n",
        "def generate_large_hole_mask(height, width, min_size=0.3, max_size=0.6):\n",
        "    \"\"\"\n",
        "    Generates a mask with large holes.\n",
        "    Args:\n",
        "        height (int): Height of the mask.\n",
        "        width (int): Width of the mask.\n",
        "        min_size (float): Minimum size of the hole relative to the image.\n",
        "        max_size (float): Maximum size of the hole relative to the image.\n",
        "    Returns:\n",
        "        mask (np.array): Binary mask with large holes.\n",
        "    \"\"\"\n",
        "    mask = np.zeros((height, width), dtype=np.uint8)\n",
        "    hole_size = random.uniform(min_size, max_size)\n",
        "    hole_height = int(height * hole_size)\n",
        "    hole_width = int(width * hole_size)\n",
        "\n",
        "    x = random.randint(0, width - hole_width)\n",
        "    y = random.randint(0, height - hole_height)\n",
        "    cv2.rectangle(mask, (x, y), (x + hole_width, y + hole_height), 1, -1)\n",
        "\n",
        "    return mask"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class PreActResidualBlock(nn.Module):\n",
        "    \"\"\"Pre-activation Residual Block for CNN stream.\"\"\"\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(PreActResidualBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
        "        self.norm1 = nn.BatchNorm2d(in_channels)\n",
        "        self.norm2 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "        out = self.norm1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv1(out)\n",
        "        out = self.norm2(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        return out + identity\n",
        "\n",
        "class FastGlobalSelfAttention(nn.Module):\n",
        "    \"\"\"Fast Global Self-Attention for Transformer stream.\"\"\"\n",
        "    def __init__(self, dim, num_heads):\n",
        "        super(FastGlobalSelfAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.scale = (dim // num_heads) ** -0.5\n",
        "        self.to_qkv = nn.Linear(dim, dim * 3)\n",
        "        self.to_out = nn.Linear(dim, dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        x = x.flatten(2).transpose(1, 2)  # (B, H*W, C)\n",
        "        qkv = self.to_qkv(x).chunk(3, dim=-1)\n",
        "        q, k, v = map(lambda t: t.view(B, -1, self.num_heads, C // self.num_heads).transpose(1, 2), qkv)\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        out = (attn @ v).transpose(1, 2).reshape(B, H * W, C)\n",
        "        out = self.to_out(out)\n",
        "        out = out.transpose(1, 2).view(B, C, H, W)\n",
        "        return out\n",
        "\n",
        "class BFI(nn.Module):\n",
        "    \"\"\"Bidirectional Feature Interaction module.\"\"\"\n",
        "    def __init__(self, channels):\n",
        "        super(BFI, self).__init__()\n",
        "        self.cnn_norm = nn.BatchNorm2d(channels)\n",
        "        self.trans_norm = nn.BatchNorm2d(channels)\n",
        "        self.fusion = nn.Conv2d(channels * 2, channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, cnn_feat, trans_feat):\n",
        "        cnn_feat = self.cnn_norm(cnn_feat)\n",
        "        trans_feat = self.trans_norm(trans_feat)\n",
        "        fused = torch.cat([cnn_feat, trans_feat], dim=1)\n",
        "        return self.fusion(fused)\n",
        "\n",
        "class BIDS_Net(nn.Module):\n",
        "    \"\"\"BIDS-Net architecture.\"\"\"\n",
        "    def __init__(self, in_channels=3, out_channels=3, num_heads=8):\n",
        "        super(BIDS_Net, self).__init__()\n",
        "        # CNN Stream\n",
        "        self.cnn_stream = nn.Sequential(\n",
        "            PreActResidualBlock(in_channels, 64),\n",
        "            PreActResidualBlock(64, 128),\n",
        "            PreActResidualBlock(128, 256)\n",
        "        )\n",
        "        # Transformer Stream\n",
        "        self.trans_stream = nn.Sequential(\n",
        "            FastGlobalSelfAttention(64, num_heads),\n",
        "            FastGlobalSelfAttention(128, num_heads),\n",
        "            FastGlobalSelfAttention(256, num_heads)\n",
        "        )\n",
        "        # BFI Modules\n",
        "        self.bfi1 = BFI(64)\n",
        "        self.bfi2 = BFI(128)\n",
        "        self.bfi3 = BFI(256)\n",
        "        # Output Projection\n",
        "        self.output = nn.Conv2d(256, out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        # CNN Stream\n",
        "        cnn_feat1 = self.cnn_stream[0](x)\n",
        "        cnn_feat2 = self.cnn_stream[1](cnn_feat1)\n",
        "        cnn_feat3 = self.cnn_stream[2](cnn_feat2)\n",
        "        # Transformer Stream\n",
        "        trans_feat1 = self.trans_stream[0](x)\n",
        "        trans_feat2 = self.trans_stream[1](trans_feat1)\n",
        "        trans_feat3 = self.trans_stream[2](trans_feat2)\n",
        "        # BFI Fusion\n",
        "        fused1 = self.bfi1(cnn_feat1, trans_feat1)\n",
        "        fused2 = self.bfi2(cnn_feat2, trans_feat2)\n",
        "        fused3 = self.bfi3(cnn_feat3, trans_feat3)\n",
        "        # Output\n",
        "        out = self.output(fused3)\n",
        "        return out * mask + x * (1 - mask)"
      ],
      "metadata": {
        "id": "Hn0aCoFpqVmi"
      },
      "execution_count": 2,
      "outputs": []
    }
  ]
}