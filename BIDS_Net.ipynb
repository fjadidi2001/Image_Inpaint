{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNOXaL86NQCeu5XOs7/Zbvs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fjadidi2001/Image_Inpaint/blob/main/BIDS_Net.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bidirectional Interaction Dual-Stream Network (BIDS-Net)"
      ],
      "metadata": {
        "id": "kJsyOcVtV11y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> The article proposes a novel approach for image inpainting called the Bidirectional Interaction Dual-Stream Network (BIDS-Net), integrating CNN and Transformer models to enhance inpainting quality by leveraging their complementary strengths."
      ],
      "metadata": {
        "id": "Ql2YbLBmWBQM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Methodology Overview\n",
        "1. Dual-Stream Structure:\n",
        "\n",
        "- CNN Stream: Captures rich local patterns and refines details.\n",
        "- Transformer Stream: Models long-range contextual correlations for global information.\n",
        "- Both streams are based on a U-shaped encoder-decoder structure to facilitate efficient multi-scale context reasoning.\n",
        "\n",
        "2. Bidirectional Feature Interaction (BFI):\n",
        "\n",
        "Implements **bidirectional feature alignment and fusion** between the CNN and Transformer streams.\n",
        "Employs **Selective Feature Fusion (SFF)** for adaptive feature integration by learning channel weights.\n",
        "3. Fast Global Self-Attention:\n",
        "\n",
        "Utilizes a **kernelizable fast-attention mechanism** for the Transformer, reducing computational complexity to linear.\n",
        "4. Loss Functions:\n",
        "\n",
        "Combines pixel-wise reconstruction, adversarial, perceptual, and style losses to ensure inpainting quality and perceptual consistency."
      ],
      "metadata": {
        "id": "Ub2RaBV8WZ-L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Channel allocation: Optimal performance when CNN and Transformer streams have equal importance.\n",
        "- Fusion methods: Bidirectional fusion outperforms unidirectional and unified-path approaches.\n",
        "- Specific fusion techniques: SFF surpasses element-wise addition and concatenation.\n",
        "- Number of random features: Optimal trade-off achieved with 72 orthogonal random features."
      ],
      "metadata": {
        "id": "RnduiCJ-ZoCn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Mask Creation Process**\n",
        "\n",
        "#### 1. **Purpose of Masking in Image Inpainting**\n",
        "   - Masks simulate corrupted regions by marking areas of an image for restoration.\n",
        "   - Masks represent regions with **value 1** (corrupted) and **value 0** (uncorrupted), facilitating selective processing during training.\n",
        "\n",
        "#### 2. **Mask Datasets**\n",
        "   - **Mask Set I**: Contains irregular shapes with various hole-to-image area ratios (10%–60%) to simulate real-world image corruption scenarios.\n",
        "   - **Mask Set II**: Focuses on **large-scale corruptions**, derived from a large mask sampling strategy, targeting challenges in **large-hole inpainting**.\n",
        "\n",
        "#### 3. **Techniques for Mask Creation**\n",
        "   - **Random Irregular Masks**:\n",
        "     - Generated using freehand-like curves and random polygons.\n",
        "     - Often involve **random rotations** and **flipping** for augmentation.\n",
        "   - **Large-Hole Masks**:\n",
        "     - Created by sampling large continuous regions, ensuring high diversity in shape and size.\n",
        "   - **Tools and Libraries**:\n",
        "     - Python libraries like **OpenCV** and **NumPy** for procedural generation of irregular shapes.\n",
        "     - **External mask datasets** for additional diversity, e.g., Mask datasets from previous works such as [29].\n",
        "\n",
        "---\n",
        "\n",
        "### **Model Architecture: BIDS-Net**\n",
        "\n",
        "#### 1. **Overall Structure**\n",
        "   - A **dual-stream network** combining **CNN** and **Transformer** models in a parallel design.\n",
        "   - Built on a **U-shaped encoder-decoder structure** for multi-scale feature extraction.\n",
        "\n",
        "#### 2. **Key Components**\n",
        "   - **CNN Stream**:\n",
        "     - Focus: Capturing **local patterns** for texture refinement.\n",
        "     - Built with **pre-activation residual blocks** for efficient and robust feature learning.\n",
        "   - **Transformer Stream**:\n",
        "     - Focus: Modeling **long-range contextual correlations**.\n",
        "     - Uses **fast global self-attention** for scalability and reduced computational overhead.\n",
        "   - **Bidirectional Feature Interaction (BFI)**:\n",
        "     - Bridges the CNN and Transformer streams with **feature alignment** and **adaptive fusion**.\n",
        "\n",
        "#### 3. **Detailed Implementation Steps**\n",
        "   - **Input Projection**:\n",
        "     - Corrupted images and masks are projected into separate feature spaces for the CNN and Transformer streams.\n",
        "     - Transformer features are downsampled to balance computational cost and performance.\n",
        "   - **Encoding Stage**:\n",
        "     - Each stream extracts features using **convolutional blocks (CNN)** and **Transformer blocks**.\n",
        "     - Features are fused bidirectionally via the **BFI module**.\n",
        "   - **Bottleneck Stage**:\n",
        "     - Features from both streams interact for enhanced context reasoning at the lowest spatial resolution.\n",
        "   - **Decoding Stage**:\n",
        "     - Outputs from both streams are upsampled and concatenated for final refinement.\n",
        "   - **Output Projection**:\n",
        "     - Combined features are transformed back to the image space for inpainting results.\n",
        "\n",
        "---\n",
        "\n",
        "### **Relevant Techniques and Algorithms**\n",
        "\n",
        "#### 1. **Fast Global Self-Attention**\n",
        "   - Reduces standard attention's quadratic complexity to linear using:\n",
        "     - **Kernelizable Attention**: Positive orthogonal random features replace softmax attention.\n",
        "     - Ensures **scalability** and efficiency for high-resolution images.\n",
        "\n",
        "#### 2. **Selective Feature Fusion (SFF)**\n",
        "   - Adapts weights for each channel during fusion, ensuring:\n",
        "     - CNN benefits from Transformer’s global context.\n",
        "     - Transformer incorporates CNN’s local details.\n",
        "   - Based on the **Selective Kernel Convolution** technique.\n",
        "\n",
        "#### 3. **Loss Functions**\n",
        "   - **Pixel-wise Reconstruction Loss**: Ensures pixel-level consistency.\n",
        "   - **Adversarial Loss**: Improves texture realism by incorporating a discriminator network.\n",
        "   - **Perceptual Loss**: Derived from a pre-trained VGG-19, enhancing perceptual similarity.\n",
        "   - **Style Loss**: Preserves stylistic details using Gram matrices.\n",
        "\n",
        "---\n",
        "\n",
        "### **Tools and Libraries**\n",
        "   - **Frameworks**: PyTorch (1.10.1), TensorFlow for alternate implementations.\n",
        "   - **Visualization**: Matplotlib or OpenCV for displaying masks and inpainted results.\n",
        "   - **GPU Hardware**: Tested on NVIDIA GeForce RTX 3090 for performance.\n",
        "\n",
        "---\n",
        "\n",
        "### **Considerations**\n",
        "   - **Mask Diversity**: Critical for generalization across various corruption scenarios.\n",
        "   - **Computational Efficiency**: Striking a balance between accuracy and runtime, particularly with Transformer integration.\n",
        "   - **Evaluation Metrics**:\n",
        "     - Quantitative: PSNR, SSIM, FID, LPIPS.\n",
        "     - Qualitative: Visual coherence and texture consistency.\n",
        "\n"
      ],
      "metadata": {
        "id": "8WqwVU4dfuIh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset"
      ],
      "metadata": {
        "id": "Ax71wBu70bTv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets -q\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sFeJjh1M0dJO",
        "outputId": "f92c19be-e536-4cda-be16-1b47cbd07886"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/480.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m471.0/480.6 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/116.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/179.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import random\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset,random_split\n",
        "from datasets import Dataset, DatasetDict\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "5BNpU_sl0joJ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name=\"caltech256-BIDS\"\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "\n",
        "CHECKPOINTS_DIR = '/content/drive/MyDrive/ckpts'\n",
        "\n",
        "def save_checkpoint(model, optimizer, epoch):\n",
        "    os.makedirs(CHECKPOINTS_DIR, exist_ok=True)\n",
        "    checkpoint_path = f'{CHECKPOINTS_DIR}/{model_name}.pth'\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "    }, checkpoint_path)\n",
        "    print(f\"ckpt saved for {model_name} at epoch {epoch}.\")\n",
        "\n",
        "def load_checkpoint(model, optimizer):\n",
        "    ckpt_path = f'{CHECKPOINTS_DIR}/{model_name}.pth'\n",
        "    if not os.path.exists(ckpt_path):\n",
        "        print(f\"no ckpt found for {model_name} starting from epoch 0.\")\n",
        "        return 0\n",
        "\n",
        "    checkpoint = torch.load(ckpt_path)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    start_epoch = checkpoint['epoch'] + 1\n",
        "    print(f\"ckpt loaded for {model_name} from {ckpt_path}. resuming from epoch {start_epoch}.\")\n",
        "\n",
        "    return start_epoch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IN73lAG-0ux8",
        "outputId": "56cb0cb7-1b19-4165-a7e8-be3feb5ed602"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"jessicali9530/caltech256\")\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hdg8NxId0utK",
        "outputId": "4200b806-95d8-4ee5-8a8d-d88f0b6ca29f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/jessicali9530/caltech256?dataset_version_number=2...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2.12G/2.12G [00:28<00:00, 79.4MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/jessicali9530/caltech256/versions/2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Complete Image Inpainting Implementation\n",
        "\n"
      ],
      "metadata": {
        "id": "6PmoDqKU5X76"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "\n",
        "# 1. Constants and Configurations\n",
        "CONFIG = {\n",
        "    'BATCH_SIZE': 16,\n",
        "    'EPOCHS': 100,\n",
        "    'LEARNING_RATE': 0.001,\n",
        "    'IMAGE_SIZE': 256,\n",
        "    'TRAIN_SPLIT': 0.8,\n",
        "    'SAVE_INTERVAL': 5\n",
        "}\n",
        "\n",
        "# 2. Dataset and Mask Generation\n",
        "class Caltech256Dataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.image_paths = glob.glob(os.path.join(root_dir, \"**/*.jpg\"), recursive=True)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        mask = generate_random_mask(image.shape[1], image.shape[2])\n",
        "        mask = torch.from_numpy(mask).float()\n",
        "\n",
        "        return image, mask\n",
        "\n",
        "def generate_random_mask(height, width):\n",
        "    if random.random() > 0.5:\n",
        "        return generate_irregular_mask(height, width)\n",
        "    return generate_large_hole_mask(height, width)\n",
        "\n",
        "def generate_irregular_mask(height, width, max_vertices=12, max_brush_width=50):\n",
        "    mask = np.zeros((height, width), dtype=np.uint8)\n",
        "    num_vertices = random.randint(3, max_vertices)\n",
        "    vertices = np.array([[\n",
        "        random.randint(0, width),\n",
        "        random.randint(0, height)\n",
        "    ] for _ in range(num_vertices)], dtype=np.int32)\n",
        "    cv2.fillPoly(mask, [vertices], 1)\n",
        "\n",
        "    for _ in range(random.randint(1, 5)):\n",
        "        start_point = (random.randint(0, width), random.randint(0, height))\n",
        "        end_point = (random.randint(0, width), random.randint(0, height))\n",
        "        thickness = random.randint(10, max_brush_width)\n",
        "        cv2.line(mask, start_point, end_point, 1, thickness)\n",
        "\n",
        "    return mask\n",
        "\n",
        "def generate_large_hole_mask(height, width, min_size=0.3, max_size=0.6):\n",
        "    mask = np.zeros((height, width), dtype=np.uint8)\n",
        "    hole_size = random.uniform(min_size, max_size)\n",
        "    hole_height = int(height * hole_size)\n",
        "    hole_width = int(width * hole_size)\n",
        "\n",
        "    x = random.randint(0, width - hole_width)\n",
        "    y = random.randint(0, height - hole_height)\n",
        "    cv2.rectangle(mask, (x, y), (x + hole_width, y + hole_height), 1, -1)\n",
        "\n",
        "    return mask\n",
        "\n",
        "# 3. Model Architecture\n",
        "class PreActResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(PreActResidualBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
        "        self.norm1 = nn.BatchNorm2d(in_channels)\n",
        "        self.norm2 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        # Skip connection for channel dimension mismatch\n",
        "        self.skip = nn.Conv2d(in_channels, out_channels, kernel_size=1) if in_channels != out_channels else nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = self.skip(x)\n",
        "\n",
        "        out = self.norm1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv1(out)\n",
        "\n",
        "        out = self.norm2(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "\n",
        "        return out + identity\n",
        "\n",
        "class FastGlobalSelfAttention(nn.Module):\n",
        "    def __init__(self, dim, num_heads):\n",
        "        super(FastGlobalSelfAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.scale = (dim // num_heads) ** -0.5\n",
        "        self.to_qkv = nn.Linear(dim, dim * 3)\n",
        "        self.to_out = nn.Linear(dim, dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        x = x.flatten(2).transpose(1, 2)  # (B, H*W, C)\n",
        "\n",
        "        qkv = self.to_qkv(x).chunk(3, dim=-1)\n",
        "        q, k, v = map(lambda t: t.view(B, -1, self.num_heads, C // self.num_heads).transpose(1, 2), qkv)\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "\n",
        "        out = (attn @ v).transpose(1, 2).reshape(B, H * W, C)\n",
        "        out = self.to_out(out)\n",
        "        out = out.transpose(1, 2).view(B, C, H, W)\n",
        "\n",
        "        return out\n",
        "\n",
        "class BFI(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super(BFI, self).__init__()\n",
        "        self.cnn_norm = nn.BatchNorm2d(channels)\n",
        "        self.trans_norm = nn.BatchNorm2d(channels)\n",
        "        self.fusion = nn.Conv2d(channels * 2, channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, cnn_feat, trans_feat):\n",
        "        cnn_feat = self.cnn_norm(cnn_feat)\n",
        "        trans_feat = self.trans_norm(trans_feat)\n",
        "        fused = torch.cat([cnn_feat, trans_feat], dim=1)\n",
        "        return self.fusion(fused)\n",
        "\n",
        "class BIDS_Net(nn.Module):\n",
        "    def __init__(self, in_channels=3, out_channels=3, num_heads=8):\n",
        "        super(BIDS_Net, self).__init__()\n",
        "\n",
        "        # CNN Stream\n",
        "        self.cnn_stream = nn.ModuleList([\n",
        "            PreActResidualBlock(in_channels, 64),\n",
        "            PreActResidualBlock(64, 128),\n",
        "            PreActResidualBlock(128, 256)\n",
        "        ])\n",
        "\n",
        "        # Transformer Stream\n",
        "        self.trans_proj = nn.Conv2d(in_channels, 64, kernel_size=1)\n",
        "        self.trans_stream = nn.ModuleList([\n",
        "            FastGlobalSelfAttention(64, num_heads),\n",
        "            FastGlobalSelfAttention(128, num_heads),\n",
        "            FastGlobalSelfAttention(256, num_heads)\n",
        "        ])\n",
        "\n",
        "        # Channel Adjustments for Transformer Stream\n",
        "        self.trans_ch_adj = nn.ModuleList([\n",
        "            nn.Conv2d(64, 128, kernel_size=1),\n",
        "            nn.Conv2d(128, 256, kernel_size=1)\n",
        "        ])\n",
        "\n",
        "        # BFI Modules\n",
        "        self.bfi1 = BFI(64)\n",
        "        self.bfi2 = BFI(128)\n",
        "        self.bfi3 = BFI(256)\n",
        "\n",
        "        # Output Projection\n",
        "        self.output = nn.Sequential(\n",
        "            nn.Conv2d(256, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, out_channels, kernel_size=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        # CNN Stream\n",
        "        cnn_feat1 = self.cnn_stream[0](x)\n",
        "        cnn_feat2 = self.cnn_stream[1](cnn_feat1)\n",
        "        cnn_feat3 = self.cnn_stream[2](cnn_feat2)\n",
        "\n",
        "        # Transformer Stream\n",
        "        trans_feat = self.trans_proj(x)\n",
        "        trans_feat1 = self.trans_stream[0](trans_feat)\n",
        "\n",
        "        trans_feat = self.trans_ch_adj[0](trans_feat1)\n",
        "        trans_feat2 = self.trans_stream[1](trans_feat)\n",
        "\n",
        "        trans_feat = self.trans_ch_adj[1](trans_feat2)\n",
        "        trans_feat3 = self.trans_stream[2](trans_feat)\n",
        "\n",
        "        # BFI Fusion\n",
        "        fused1 = self.bfi1(cnn_feat1, trans_feat1)\n",
        "        fused2 = self.bfi2(cnn_feat2, trans_feat2)\n",
        "        fused3 = self.bfi3(cnn_feat3, trans_feat3)\n",
        "\n",
        "        # Output\n",
        "        out = self.output(fused3)\n",
        "        return out * mask + x * (1 - mask)\n",
        "\n",
        "# 4. Training and Evaluation Functions\n",
        "def train_one_epoch(model, dataloader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    pbar = tqdm(dataloader, desc='Training')\n",
        "\n",
        "    for images, masks in pbar:\n",
        "        images = images.to(device)\n",
        "        masks = masks.to(device)\n",
        "\n",
        "        masked_images = images * (1 - masks.unsqueeze(1))\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(masked_images, masks.unsqueeze(1))\n",
        "        loss = criterion(outputs, images)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        pbar.set_postfix({'loss': loss.item()})\n",
        "\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "def validate(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, masks in dataloader:\n",
        "            images = images.to(device)\n",
        "            masks = masks.to(device)\n",
        "\n",
        "            masked_images = images * (1 - masks.unsqueeze(1))\n",
        "            outputs = model(masked_images, masks.unsqueeze(1))\n",
        "            loss = criterion(outputs, images)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "# 5. Checkpoint Functions\n",
        "def save_checkpoint(model, optimizer, epoch, best_loss, path):\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'best_loss': best_loss\n",
        "    }, path)\n",
        "\n",
        "def load_checkpoint(model, optimizer, path):\n",
        "    if not os.path.exists(path):\n",
        "        return 0, float('inf')\n",
        "\n",
        "    checkpoint = torch.load(path)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    return checkpoint['epoch'] + 1, checkpoint['best_loss']\n",
        "\n",
        "# 6. Visualization Functions\n",
        "def visualize_results(model, dataloader, device, save_path=None):\n",
        "    model.eval()\n",
        "    denorm = transforms.Normalize(\n",
        "        mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],\n",
        "        std=[1/0.229, 1/0.224, 1/0.225]\n",
        "    )\n",
        "\n",
        "    with torch.no_grad():\n",
        "        images, masks = next(iter(dataloader))\n",
        "        images = images[:5].to(device)  # Take first 5 samples\n",
        "        masks = masks[:5].to(device)\n",
        "\n",
        "        masked_images = images * (1 - masks.unsqueeze(1))\n",
        "        outputs = model(masked_images, masks.unsqueeze(1))\n",
        "\n",
        "        # Denormalize images\n",
        "        images = denorm(images).cpu()\n",
        "        masked_images = denorm(masked_images).cpu()\n",
        "        outputs = denorm(outputs).cpu()\n",
        "\n",
        "        # Create visualization\n",
        "        fig, axes = plt.subplots(5, 3, figsize=(12, 20))\n",
        "\n",
        "        for i in range(5):\n",
        "            axes[i, 0].imshow(images[i].permute(1, 2, 0).clip(0, 1))\n",
        "            axes[i, 0].set_title('Original')\n",
        "            axes[i, 1].imshow(masked_images[i].permute(1, 2, 0).clip(0, 1))\n",
        "            axes[i, 1].set_title('Masked')\n",
        "            axes[i, 2].imshow(outputs[i].permute(1, 2, 0).clip(0, 1))\n",
        "            axes[i, 2].set_title('Inpainted')\n",
        "\n",
        "            for ax in axes[i]:\n",
        "                ax.axis('off')\n",
        "\n",
        "        plt.tight_layout()\n",
        "\n",
        "        if save_path:\n",
        "            plt.savefig(save_path)\n",
        "            plt.close()\n",
        "        else:\n",
        "            plt.show()\n",
        "\n",
        "# 7. Main Training Loop\n",
        "def main():\n",
        "    # Initialize wandb\n",
        "    wandb.init(project=\"image-inpainting\", name=\"BIDS-Net-training\")\n",
        "\n",
        "    # Hyperparameters\n",
        "    config = {\n",
        "        'batch_size': 16,\n",
        "        'epochs': 100,\n",
        "        'learning_rate': 0.001,\n",
        "        'image_size': 256,\n",
        "        'num_workers': 4,\n",
        "        'checkpoint_dir': 'checkpoints',\n",
        "        'results_dir': 'results'\n",
        "    }\n",
        "    wandb.config.update(config)\n",
        "\n",
        "    # Create directories\n",
        "    os.makedirs(config['checkpoint_dir'], exist_ok=True)\n",
        "    os.makedirs(config['results_dir'], exist_ok=True)\n",
        "\n",
        "    # Set device\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Data transforms\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((config['image_size'], config['image_size'])),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                           std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    # Create datasets\n",
        "    dataset = Caltech256Dataset(\n",
        "        root_dir=\"path_to_caltech256\",\n",
        "        transform=transform,\n",
        "        image_size=config['image_size']\n",
        "    )\n",
        "\n",
        "    # Split dataset\n",
        "    train_size = int(0.8 * len(dataset))\n",
        "    val_size = len(dataset) - train_size\n",
        "    train_dataset, val_dataset = random_split(\n",
        "        dataset, [train_size, val_size]\n",
        "    )\n",
        "\n",
        "    # Create dataloaders\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=config['batch_size'],\n",
        "        shuffle=True,\n",
        "        num_workers=config['num_workers'],\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=config['batch_size'],\n",
        "        shuffle=False,\n",
        "        num_workers=config['num_workers'],\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    # Initialize model, optimizer, and loss\n",
        "    model = BIDS_Net().to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
        "    criterion = nn.L1Loss()\n",
        "\n",
        "    # Learning rate scheduler\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
        "    )\n",
        "\n",
        "    # Load checkpoint if exists\n",
        "    start_epoch = 0\n",
        "    best_val_loss = float('inf')\n",
        "    checkpoint_path = os.path.join(config['checkpoint_dir'], 'latest.pth')\n",
        "    if os.path.exists(checkpoint_path):\n",
        "        checkpoint = torch.load(checkpoint_path)\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        start_epoch = checkpoint['epoch'] + 1\n",
        "        best_val_loss = checkpoint['best_val_loss']\n",
        "        print(f\"Loaded checkpoint from epoch {start_epoch-1}\")\n",
        "\n",
        "    # Training loop\n",
        "    print(\"Starting training...\")\n",
        "    for epoch in range(start_epoch, config['epochs']):\n",
        "        print(f\"\\nEpoch {epoch+1}/{config['epochs']}\")\n",
        "\n",
        "        # Train for one epoch\n",
        "        train_loss = train_epoch(\n",
        "            model=model,\n",
        "            train_loader=train_loader,\n",
        "            criterion=criterion,\n",
        "            optimizer=optimizer,\n",
        "            device=device,\n",
        "            epoch=epoch\n",
        "        )\n",
        "\n",
        "        # Validate\n",
        "        val_loss = validate(\n",
        "            model=model,\n",
        "            val_loader=val_loader,\n",
        "            criterion=criterion,\n",
        "            device=device\n",
        "        )\n",
        "\n",
        "        # Update learning rate\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        # Print epoch results\n",
        "        print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "        # Save checkpoint if best model\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'best_val_loss': best_val_loss\n",
        "            }, os.path.join(config['checkpoint_dir'], 'best_model.pth'))\n",
        "            print(\"Saved best model checkpoint\")\n",
        "\n",
        "        # Save latest checkpoint\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'best_val_loss': best_val_loss\n",
        "        }, os.path.join(config['checkpoint_dir'], 'latest.pth'))\n",
        "\n",
        "        # Visualize results every 5 epochs\n",
        "        if (epoch + 1) % 5 == 0:\n",
        "            visualize_results(\n",
        "                model=model,\n",
        "                val_loader=val_loader,\n",
        "                device=device,\n",
        "                epoch=epoch,\n",
        "                save_dir=config['results_dir']\n",
        "            )\n",
        "\n",
        "    wandb.finish()\n",
        "    print(\"Training completed!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "JAeq1Awg5bwo",
        "outputId": "690475c7-7084-45ee-bd93-415599d78ca5"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'wandb' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-670650cd3750>\u001b[0m in \u001b[0;36m<cell line: 455>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-14-670650cd3750>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[0;31m# Initialize wandb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m     \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproject\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"image-inpainting\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"BIDS-Net-training\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[0;31m# Hyperparameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'wandb' is not defined"
          ]
        }
      ]
    }
  ]
}