{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM1DuqXjJrjkde84GnxMzWB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fjadidi2001/Image_Inpaint/blob/main/CM_GAN_Jan5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The architecture of CM-GAN focuses on image inpainting, specifically designed to fill in missing or corrupted regions of images with realistic content. While\n",
        "### **1. Generator Architecture**\n",
        "The generator is responsible for creating realistic inpainted images. CM-GAN uses **cascaded modulation** to process inputs. A common flow:\n",
        "\n",
        "#### Input:\n",
        "- An **image** with missing regions (masked image).\n",
        "- A **binary mask** representing the missing areas (1 for missing, 0 for existing pixels).\n",
        "\n",
        "#### Layers:\n",
        "1. **Convolutional Layers with Mask Concatenation**:\n",
        "   - Initial layers concatenate the image with the binary mask.\n",
        "   - Convolutions extract features from the masked regions.\n",
        "\n",
        "   **Purpose**: Learn the structure and surrounding context of the image.\n",
        "\n",
        "2. **Cascaded Modulation Block**:\n",
        "   - Combines **global modulation** (to understand overall image semantics) with **spatially adaptive modulation** (to handle local details).\n",
        "   - Global modulation uses a feature map that spans the entire image.\n",
        "   - Adaptive modulation applies location-specific adjustments.\n",
        "\n",
        "   **Purpose**: Balance global coherence and local realism.\n",
        "\n",
        "3. **Feature Propagation via Attention Mechanisms**:\n",
        "   - **Enhanced Attention** to propagate contextual information from known to unknown areas.\n",
        "\n",
        "   **Purpose**: Ensures accurate filling of missing regions based on surrounding context.\n",
        "\n",
        "4. **Output Layers**:\n",
        "   - A final set of convolutions or deconvolutions reconstructs the inpainted image.\n",
        "\n",
        "   **Purpose**: Generate the final high-quality inpainted output.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Discriminator Architecture**\n",
        "The discriminator evaluates the inpainted images for realism.\n",
        "\n",
        "1. **Input**:\n",
        "   - The inpainted image (from the generator).\n",
        "   - The corresponding ground truth image (actual image without missing areas).\n",
        "\n",
        "2. **Layers**:\n",
        "   - Convolutional layers extract features.\n",
        "   - Outputs a **realism score**, indicating how realistic the inpainted image is.\n",
        "\n",
        "3. **Loss Function**:\n",
        "   - Often uses an **adversarial loss** (e.g., Wasserstein or hinge loss) to train the generator and discriminator in a competitive manner.\n",
        "\n",
        "---\n",
        "\n",
        "### **Key Components of CM-GAN**\n",
        "1. **Object-Aware Training**:\n",
        "   - Focuses on challenging regions, like objects, using annotations (e.g., panoptic segmentation).\n",
        "   - Ensures that the generator fills object regions more realistically.\n",
        "\n",
        "2. **Mask-Aware Encoding**:\n",
        "   - Explicitly considers the mask during feature extraction.\n",
        "   - Helps the generator learn to handle varied mask sizes and shapes.\n",
        "\n",
        "3. **Enhanced Attention**:\n",
        "   - Propagates information from visible areas to missing areas.\n",
        "   - Improves inpainting quality for complex patterns.\n",
        "\n",
        "---\n",
        "\n",
        "### **How the Architecture Works**\n",
        "1. **Training**:\n",
        "   - The generator creates inpainted images.\n",
        "   - The discriminator evaluates their realism.\n",
        "   - Both networks are updated iteratively to improve their performance.\n",
        "\n",
        "2. **Inference**:\n",
        "   - Given an input image and a mask, the generator fills the missing regions.\n",
        "   - No discriminator is needed during inference.\n"
      ],
      "metadata": {
        "id": "qSeics-fhrmJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Gather the Dataset\n"
      ],
      "metadata": {
        "id": "D5Wl0pJ4jriZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://data.csail.mit.edu/places/places365/train_large_places365standard.tar\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vA6ohVVspUws",
        "outputId": "42ff7276-b1f7-4886-c04f-61bfce1ffcb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-01-05 18:08:34--  http://data.csail.mit.edu/places/places365/train_large_places365standard.tar\n",
            "Resolving data.csail.mit.edu (data.csail.mit.edu)... 128.52.131.233\n",
            "Connecting to data.csail.mit.edu (data.csail.mit.edu)|128.52.131.233|:80... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://data.csail.mit.edu/places/places365/train_large_places365standard.tar [following]\n",
            "--2025-01-05 18:08:35--  https://data.csail.mit.edu/places/places365/train_large_places365standard.tar\n",
            "Connecting to data.csail.mit.edu (data.csail.mit.edu)|128.52.131.233|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 112598435840 (105G) [application/x-tar]\n",
            "Saving to: ‘train_large_places365standard.tar’\n",
            "\n",
            "d.tar                 2%[                    ]   2.10G  15.4MB/s    eta 1h 55m "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xvf train_large_places365standard.tar"
      ],
      "metadata": {
        "id": "5faLa2eSqhYR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "f9ZfAbSpqz_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load and Use the Dataset\n"
      ],
      "metadata": {
        "id": "lT70RwhUq8O-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Organize the Dataset"
      ],
      "metadata": {
        "id": "rrcW9k6rrFjp"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K_AbgCKMr_Sz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use the Dataset in a Machine Learning Framework"
      ],
      "metadata": {
        "id": "u70-TMFxrSfo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task"
      ],
      "metadata": {
        "id": "6fE1YaHCrvQA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "### **Step-by-Step Guide**\n",
        "\n",
        "---\n",
        "\n",
        "### **Step 1: Prepare Your Dataset**\n",
        "1. **Organize Your Dataset**:\n",
        "   - Place all your images in a single folder (e.g., `./places2`).\n",
        "   - Ensure the images are in common formats like `.jpg`, `.png`, or `.jpeg`.\n",
        "\n",
        "2. **Update the Code**:\n",
        "   - Use the provided code to load, preprocess, and generate masks for your dataset.\n",
        "   - Set the `DATA_DIR` variable to the path of your dataset folder.\n",
        "\n",
        "3. **Run the Dataset Preparation Code**:\n",
        "   - Execute the code to create the `train_dataset` and `val_dataset`.\n",
        "   - Verify that the dataset is created successfully by visualizing a sample batch.\n",
        "\n",
        "---\n",
        "\n",
        "### **Step 2: Design the Generator**\n",
        "The generator is responsible for filling in the missing regions of the image. Here's how to design it:\n",
        "\n",
        "1. **Input**:\n",
        "   - Concatenate the masked image and binary mask as input.\n",
        "\n",
        "2. **Encoder**:\n",
        "   - Use convolutional layers to extract features from the input.\n",
        "   - Apply instance normalization and ReLU activation.\n",
        "\n",
        "3. **Cascaded Modulation Blocks**:\n",
        "   - Implement global and spatially adaptive modulation to balance global coherence and local details.\n",
        "\n",
        "4. **Attention Mechanism**:\n",
        "   - Use self-attention or non-local blocks to propagate information from known to unknown regions.\n",
        "\n",
        "5. **Decoder**:\n",
        "   - Use transposed convolutions or upsampling layers to reconstruct the image.\n",
        "   - Apply skip connections from the encoder to preserve details.\n",
        "\n",
        "6. **Output Layer**:\n",
        "   - Use a convolutional layer with a `tanh` activation to generate the final image.\n",
        "\n",
        "---\n",
        "\n",
        "### **Step 3: Design the Discriminator**\n",
        "The discriminator evaluates the realism of the inpainted images.\n",
        "\n",
        "1. **Input**:\n",
        "   - Concatenate the inpainted image and ground truth image (for conditional GANs).\n",
        "\n",
        "2. **Layers**:\n",
        "   - Use convolutional layers with leaky ReLU activation.\n",
        "   - Apply spectral normalization for stable training.\n",
        "\n",
        "3. **Output**:\n",
        "   - Use a fully connected layer to output a realism score.\n",
        "\n",
        "---\n",
        "\n",
        "### **Step 4: Define Loss Functions**\n",
        "1. **Adversarial Loss**:\n",
        "   - Use Wasserstein loss or hinge loss for stable training.\n",
        "\n",
        "2. **Reconstruction Loss**:\n",
        "   - Use L1 or L2 loss to ensure pixel-level accuracy.\n",
        "\n",
        "3. **Perceptual Loss**:\n",
        "   - Use a pre-trained VGG network to compare high-level features.\n",
        "\n",
        "4. **Total Loss**:\n",
        "   - Combine all losses with appropriate weights.\n",
        "\n",
        "---\n",
        "\n",
        "### **Step 5: Train the Model**\n",
        "1. **Optimizers**:\n",
        "   - Use Adam optimizer for both the generator and discriminator.\n",
        "\n",
        "2. **Training Loop**:\n",
        "   - Alternate between training the generator and discriminator.\n",
        "   - Use gradient penalty for Wasserstein GANs.\n",
        "\n",
        "3. **Evaluation**:\n",
        "   - Visualize inpainted images during training.\n",
        "   - Use metrics like PSNR and SSIM for quantitative evaluation.\n",
        "\n",
        "---\n",
        "\n",
        "### **Step 6: Deploy the Model**\n",
        "1. **Save the Trained Model**:\n",
        "   - Save the generator for inference.\n",
        "\n",
        "2. **Inference**:\n",
        "   - Load the generator and use it to inpaint new images.\n"
      ],
      "metadata": {
        "id": "NowDdYzorwVX"
      }
    }
  ]
}