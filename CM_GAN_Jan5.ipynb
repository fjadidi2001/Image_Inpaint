{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOXyMrHGoe2JTs9lHysH9pD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fjadidi2001/Image_Inpaint/blob/main/CM_GAN_Jan5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The architecture of CM-GAN focuses on image inpainting, specifically designed to fill in missing or corrupted regions of images with realistic content. While\n",
        "### **1. Generator Architecture**\n",
        "The generator is responsible for creating realistic inpainted images. CM-GAN uses **cascaded modulation** to process inputs. A common flow:\n",
        "\n",
        "#### Input:\n",
        "- An **image** with missing regions (masked image).\n",
        "- A **binary mask** representing the missing areas (1 for missing, 0 for existing pixels).\n",
        "\n",
        "#### Layers:\n",
        "1. **Convolutional Layers with Mask Concatenation**:\n",
        "   - Initial layers concatenate the image with the binary mask.\n",
        "   - Convolutions extract features from the masked regions.\n",
        "\n",
        "   **Purpose**: Learn the structure and surrounding context of the image.\n",
        "\n",
        "2. **Cascaded Modulation Block**:\n",
        "   - Combines **global modulation** (to understand overall image semantics) with **spatially adaptive modulation** (to handle local details).\n",
        "   - Global modulation uses a feature map that spans the entire image.\n",
        "   - Adaptive modulation applies location-specific adjustments.\n",
        "\n",
        "   **Purpose**: Balance global coherence and local realism.\n",
        "\n",
        "3. **Feature Propagation via Attention Mechanisms**:\n",
        "   - **Enhanced Attention** to propagate contextual information from known to unknown areas.\n",
        "\n",
        "   **Purpose**: Ensures accurate filling of missing regions based on surrounding context.\n",
        "\n",
        "4. **Output Layers**:\n",
        "   - A final set of convolutions or deconvolutions reconstructs the inpainted image.\n",
        "\n",
        "   **Purpose**: Generate the final high-quality inpainted output.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Discriminator Architecture**\n",
        "The discriminator evaluates the inpainted images for realism.\n",
        "\n",
        "1. **Input**:\n",
        "   - The inpainted image (from the generator).\n",
        "   - The corresponding ground truth image (actual image without missing areas).\n",
        "\n",
        "2. **Layers**:\n",
        "   - Convolutional layers extract features.\n",
        "   - Outputs a **realism score**, indicating how realistic the inpainted image is.\n",
        "\n",
        "3. **Loss Function**:\n",
        "   - Often uses an **adversarial loss** (e.g., Wasserstein or hinge loss) to train the generator and discriminator in a competitive manner.\n",
        "\n",
        "---\n",
        "\n",
        "### **Key Components of CM-GAN**\n",
        "1. **Object-Aware Training**:\n",
        "   - Focuses on challenging regions, like objects, using annotations (e.g., panoptic segmentation).\n",
        "   - Ensures that the generator fills object regions more realistically.\n",
        "\n",
        "2. **Mask-Aware Encoding**:\n",
        "   - Explicitly considers the mask during feature extraction.\n",
        "   - Helps the generator learn to handle varied mask sizes and shapes.\n",
        "\n",
        "3. **Enhanced Attention**:\n",
        "   - Propagates information from visible areas to missing areas.\n",
        "   - Improves inpainting quality for complex patterns.\n",
        "\n",
        "---\n",
        "\n",
        "### **How the Architecture Works**\n",
        "1. **Training**:\n",
        "   - The generator creates inpainted images.\n",
        "   - The discriminator evaluates their realism.\n",
        "   - Both networks are updated iteratively to improve their performance.\n",
        "\n",
        "2. **Inference**:\n",
        "   - Given an input image and a mask, the generator fills the missing regions.\n",
        "   - No discriminator is needed during inference.\n"
      ],
      "metadata": {
        "id": "qSeics-fhrmJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Gather the Dataset\n"
      ],
      "metadata": {
        "id": "D5Wl0pJ4jriZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "A0C__2XydX4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "outputId": "62af905d-4311-472d-aa26-f46d3ad9b1ff"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: './places2'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-89fdf4756684>\u001b[0m in \u001b[0;36m<cell line: 71>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;31m# Create the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIMG_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMASK_TYPE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;31m# Visualize a sample batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-89fdf4756684>\u001b[0m in \u001b[0;36mcreate_dataset\u001b[0;34m(data_dir, img_size, batch_size, mask_type)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcreate_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"random\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;31m# Get list of image paths\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     image_paths = [os.path.join(data_dir, img_name) for img_name in os.listdir(data_dir)\n\u001b[0m\u001b[1;32m     39\u001b[0m                    if img_name.endswith(('.jpg', '.png', '.jpeg'))]\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './places2'"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from skimage.draw import random_shapes\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Parameters\n",
        "DATA_DIR = \"./places2\"  # Path to your dataset\n",
        "IMG_SIZE = (128, 128)  # Target image size\n",
        "BATCH_SIZE = 32\n",
        "MASK_TYPE = \"random\"  # Options: \"random\", \"rectangle\"\n",
        "\n",
        "# Generate a binary mask\n",
        "def generate_mask(img_size, mask_type=\"random\"):\n",
        "    if mask_type == \"random\":\n",
        "        # Generate random irregular mask\n",
        "        mask, _ = random_shapes(img_size, max_shapes=5, min_size=50, max_size=100, multichannel=False)\n",
        "        mask = (mask == 255).astype(np.float32)  # Convert to binary mask\n",
        "    elif mask_type == \"rectangle\":\n",
        "        # Generate rectangular mask\n",
        "        mask = np.ones(img_size, dtype=np.float32)\n",
        "        x1, y1 = np.random.randint(0, img_size[0] // 2), np.random.randint(0, img_size[1] // 2)\n",
        "        x2, y2 = np.random.randint(x1, img_size[0]), np.random.randint(y1, img_size[1])\n",
        "        mask[x1:x2, y1:y2] = 0\n",
        "    return mask\n",
        "\n",
        "# Load and preprocess a single image\n",
        "def load_and_preprocess_image(img_path, img_size):\n",
        "    img = load_img(img_path, target_size=img_size)\n",
        "    img = img_to_array(img) / 255.0  # Normalize to [0, 1]\n",
        "    return img\n",
        "\n",
        "# Create a TensorFlow Dataset\n",
        "def create_dataset(data_dir, img_size, batch_size, mask_type=\"random\"):\n",
        "    # Get list of image paths\n",
        "    image_paths = [os.path.join(data_dir, img_name) for img_name in os.listdir(data_dir)\n",
        "                   if img_name.endswith(('.jpg', '.png', '.jpeg'))]\n",
        "\n",
        "    # Split into training and validation sets\n",
        "    train_paths, val_paths = train_test_split(image_paths, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Function to load and preprocess images and masks\n",
        "    def process_image(img_path):\n",
        "        # Load and preprocess image\n",
        "        img = tf.numpy_function(load_and_preprocess_image, [img_path, img_size], tf.float32)\n",
        "        img.set_shape(img_size + (3,))  # Set shape explicitly\n",
        "\n",
        "        # Generate mask\n",
        "        mask = tf.numpy_function(generate_mask, [img_size, mask_type], tf.float32)\n",
        "        mask.set_shape(img_size)  # Set shape explicitly\n",
        "\n",
        "        # Apply mask to image\n",
        "        masked_img = img * tf.expand_dims(mask, axis=-1)\n",
        "\n",
        "        return masked_img, img, mask\n",
        "\n",
        "    # Create TensorFlow Dataset\n",
        "    train_dataset = tf.data.Dataset.from_tensor_slices(train_paths)\n",
        "    train_dataset = train_dataset.map(process_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    train_dataset = train_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    val_dataset = tf.data.Dataset.from_tensor_slices(val_paths)\n",
        "    val_dataset = val_dataset.map(process_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    val_dataset = val_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    return train_dataset, val_dataset\n",
        "\n",
        "# Create the dataset\n",
        "train_dataset, val_dataset = create_dataset(DATA_DIR, IMG_SIZE, BATCH_SIZE, MASK_TYPE)\n",
        "\n",
        "# Visualize a sample batch\n",
        "def visualize_batch(dataset):\n",
        "    for masked_images, original_images, masks in dataset.take(1):\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        for i in range(3):  # Display 3 samples\n",
        "            plt.subplot(3, 3, i * 3 + 1)\n",
        "            plt.title(\"Masked Image\")\n",
        "            plt.imshow(masked_images[i])\n",
        "            plt.axis(\"off\")\n",
        "\n",
        "            plt.subplot(3, 3, i * 3 + 2)\n",
        "            plt.title(\"Original Image\")\n",
        "            plt.imshow(original_images[i])\n",
        "            plt.axis(\"off\")\n",
        "\n",
        "            plt.subplot(3, 3, i * 3 + 3)\n",
        "            plt.title(\"Mask\")\n",
        "            plt.imshow(masks[i], cmap='gray')\n",
        "            plt.axis(\"off\")\n",
        "        plt.show()\n",
        "\n",
        "visualize_batch(train_dataset)"
      ]
    }
  ]
}