{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fjadidi2001/Image_Inpaint/blob/main/FastInpaint_Jan14.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries"
      ],
      "metadata": {
        "id": "oUj1jZcEwJSQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, random_split, DataLoader\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import kagglehub\n",
        "from google.colab import drive\n",
        "import torchvision.datasets as datasets\n"
      ],
      "metadata": {
        "id": "4Xjz9lK6ljsc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb614685-75d1-49ab-9894-1d5ec62649f9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.11)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.27.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.12.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.2.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.2.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"jessicali9530/caltech256\")\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WzStjkBJmH3G",
        "outputId": "ce3541fd-5f46-400a-a829-cbbe79391aee"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/jessicali9530/caltech256?dataset_version_number=2...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2.12G/2.12G [00:17<00:00, 133MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/jessicali9530/caltech256/versions/2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model name and setup\n",
        "model_name = \"caltech256-fastInpaint\"\n",
        "drive.mount('/content/drive')\n",
        "CHECKPOINTS_DIR = '/content/drive/MyDrive/ckpts'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LNTseq9-ngH-",
        "outputId": "83f376e6-2a59-41b6-8239-81056bb7c6e9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_data(img_size=256, batch_size=8):\n",
        "    \"\"\"Setup data loaders for Caltech256\"\"\"\n",
        "    # Download dataset\n",
        "    base_path = kagglehub.dataset_download(\"jessicali9530/caltech256\")\n",
        "    image_dir = os.path.join(base_path, '256_ObjectCategories')\n",
        "\n",
        "    print(f\"Base path: {base_path}\")\n",
        "    print(f\"Image directory: {image_dir}\")\n",
        "\n",
        "    # Define transforms\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((img_size, img_size)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "    ])\n",
        "\n",
        "    # Create dataset\n",
        "    dataset = datasets.ImageFolder(root=image_dir, transform=transform)\n",
        "    print(f\"\\nLoaded dataset with {len(dataset)} images\")\n",
        "\n",
        "    # Split dataset\n",
        "    train_size = int(0.7 * len(dataset))\n",
        "    val_size = int(0.15 * len(dataset))\n",
        "    test_size = len(dataset) - train_size - val_size\n",
        "\n",
        "    # Create splits\n",
        "    train_dataset, val_dataset, test_dataset = random_split(\n",
        "        dataset, [train_size, val_size, test_size]\n",
        "    )\n",
        "\n",
        "    # Create data loaders\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=2\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=batch_size,\n",
        "        num_workers=2\n",
        "    )\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=batch_size,\n",
        "        num_workers=2\n",
        "    )\n",
        "\n",
        "    print(f\"Train size: {len(train_dataset)}\")\n",
        "    print(f\"Val size: {len(val_dataset)}\")\n",
        "    print(f\"Test size: {len(test_dataset)}\")\n",
        "\n",
        "    return train_loader, val_loader, test_loader\n"
      ],
      "metadata": {
        "id": "WlT-CzEPxIK9"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_batch(original_images, masked_images, outputs, epoch, batch_idx, save_dir='visualization'):\n",
        "    \"\"\"Visualize a batch of images: original, masked, and reconstructed\"\"\"\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    # Convert tensors to numpy arrays and move to CPU if needed\n",
        "    original_images = original_images.cpu().detach().numpy()\n",
        "    masked_images = masked_images.cpu().detach().numpy()\n",
        "    outputs = outputs.cpu().detach().numpy()\n",
        "\n",
        "    # Create a figure with three rows: original, masked, and reconstructed\n",
        "    fig, axes = plt.subplots(3, min(4, original_images.shape[0]), figsize=(15, 10))\n",
        "\n",
        "    for i in range(min(4, original_images.shape[0])):\n",
        "        # Original\n",
        "        axes[0, i].imshow(np.transpose(original_images[i], (1, 2, 0)) * 0.5 + 0.5)\n",
        "        axes[0, i].axis('off')\n",
        "        if i == 0:\n",
        "            axes[0, i].set_title('Original')\n",
        "\n",
        "        # Masked\n",
        "        axes[1, i].imshow(np.transpose(masked_images[i], (1, 2, 0)) * 0.5 + 0.5)\n",
        "        axes[1, i].axis('off')\n",
        "        if i == 0:\n",
        "            axes[1, i].set_title('Masked')\n",
        "\n",
        "        # Reconstructed\n",
        "        axes[2, i].imshow(np.transpose(outputs[i], (1, 2, 0)) * 0.5 + 0.5)\n",
        "        axes[2, i].axis('off')\n",
        "        if i == 0:\n",
        "            axes[2, i].set_title('Reconstructed')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{save_dir}/epoch_{epoch}_batch_{batch_idx}.png')\n",
        "    plt.close()"
      ],
      "metadata": {
        "id": "ZLYEOhmVnnFm"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_sample_batch(data_loader, save_path='sample_batch.png'):\n",
        "    \"\"\"Visualize and save a sample batch of images\"\"\"\n",
        "    # Get a batch of images\n",
        "    images = next(iter(data_loader))[0]  # [0] because ImageFolder returns (images, labels)\n",
        "\n",
        "    # Create figure\n",
        "    fig, axes = plt.subplots(2, 4, figsize=(15, 8))\n",
        "    axes = axes.ravel()\n",
        "\n",
        "    for idx, img in enumerate(images[:8]):  # Show first 8 images\n",
        "        # Convert tensor to numpy and denormalize\n",
        "        img_np = img.numpy().transpose(1, 2, 0) * 0.5 + 0.5\n",
        "        axes[idx].imshow(img_np)\n",
        "        axes[idx].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path)\n",
        "    plt.close()\n",
        "    print(f\"Sample batch visualization saved to {save_path}\")"
      ],
      "metadata": {
        "id": "qfFs334hx8e-"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_checkpoint(model, optimizer, epoch):\n",
        "    os.makedirs(CHECKPOINTS_DIR, exist_ok=True)\n",
        "    checkpoint_path = f'{CHECKPOINTS_DIR}/{model_name}.pth'\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "    }, checkpoint_path)\n",
        "    print(f\"Checkpoint saved for {model_name} at epoch {epoch}\")"
      ],
      "metadata": {
        "id": "vkpJrUKZqFyW"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_checkpoint(model, optimizer):\n",
        "    ckpt_path = f'{CHECKPOINTS_DIR}/{model_name}.pth'\n",
        "    if not os.path.exists(ckpt_path):\n",
        "        print(f\"No checkpoint found for {model_name}, starting from epoch 0\")\n",
        "        return 0\n",
        "\n",
        "    checkpoint = torch.load(ckpt_path)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    start_epoch = checkpoint['epoch'] + 1\n",
        "    print(f\"Checkpoint loaded for {model_name}, resuming from epoch {start_epoch}\")\n",
        "    return start_epoch"
      ],
      "metadata": {
        "id": "87WbbJl7qOvb"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(channels, channels, 3, padding=1)\n",
        "        self.norm1 = nn.InstanceNorm2d(channels)\n",
        "        self.conv2 = nn.Conv2d(channels, channels, 3, padding=1)\n",
        "        self.norm2 = nn.InstanceNorm2d(channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        x = F.relu(self.norm1(self.conv1(x)))\n",
        "        x = self.norm2(self.conv2(x))\n",
        "        x += residual\n",
        "        x = F.relu(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "WeTBJj7SyaDl"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class InpaintingNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(InpaintingNet, self).__init__()\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(6, 64, 7, padding=3),\n",
        "            nn.InstanceNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Conv2d(64, 128, 4, stride=2, padding=1),\n",
        "            nn.InstanceNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Conv2d(128, 256, 4, stride=2, padding=1),\n",
        "            nn.InstanceNorm2d(256),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        # Middle blocks\n",
        "        self.middle = nn.Sequential(*[ResidualBlock(256) for _ in range(6)])\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(256, 128, 4, stride=2, padding=1),\n",
        "            nn.InstanceNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),\n",
        "            nn.InstanceNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Conv2d(64, 3, 7, padding=3),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        x = torch.cat([x, mask], dim=1)\n",
        "        x = self.encoder(x)\n",
        "        x = self.middle(x)\n",
        "        x = self.decoder(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "7kjWjfXnqZRB"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_random_mask(image):\n",
        "    \"\"\"Create random rectangular masks\"\"\"\n",
        "    batch_size, _, height, width = image.shape\n",
        "    mask = torch.ones_like(image)\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        h = torch.randint(height//4, height//2, (1,)).item()\n",
        "        w = torch.randint(width//4, width//2, (1,)).item()\n",
        "\n",
        "        top = torch.randint(0, height - h, (1,)).item()\n",
        "        left = torch.randint(0, width - w, (1,)).item()\n",
        "\n",
        "        mask[i, :, top:top+h, left:left+w] = 0\n",
        "\n",
        "    return mask"
      ],
      "metadata": {
        "id": "Sbp52-hmqti6"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_losses(train_losses, val_losses, save_dir='visualization'):\n",
        "    \"\"\"Plot training and validation losses\"\"\"\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(train_losses, label='Training Loss')\n",
        "    plt.plot(val_losses, label='Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Validation Losses')\n",
        "    plt.legend()\n",
        "    plt.savefig(f'{save_dir}/losses.png')\n",
        "    plt.close()"
      ],
      "metadata": {
        "id": "IVmfbHD5q63Q"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, val_loader, num_epochs=30, device='cuda'):\n",
        "    model = model.to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    criterion = nn.L1Loss()\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3)\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    # Load checkpoint if exists\n",
        "    start_epoch = load_checkpoint(model, optimizer)\n",
        "\n",
        "    for epoch in range(start_epoch, num_epochs):\n",
        "        # Training\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        for batch_idx, (images, _) in enumerate(train_loader): # Access images directly from the tuple\n",
        "            images = images.to(device)\n",
        "            masks = create_random_mask(images).to(device)\n",
        "            masked_images = images * masks\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(masked_images, masks)\n",
        "            loss = criterion(outputs, images)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "            # Visualize every 100 batches\n",
        "            if batch_idx % 100 == 0:\n",
        "                visualize_batch(images, masked_images, outputs, epoch, batch_idx)\n",
        "\n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "        train_losses.append(avg_train_loss)\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, (images, _) in enumerate(val_loader): # unpack the batch\n",
        "                images = images.to(device) # images is the tensor you need\n",
        "                masks = create_random_mask(images).to(device)\n",
        "                masked_images = images * masks\n",
        "\n",
        "                outputs = model(masked_images, masks)\n",
        "                loss = criterion(outputs, images)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "        val_losses.append(avg_val_loss)\n",
        "\n",
        "        # Update learning rate\n",
        "        scheduler.step(avg_val_loss)\n",
        "\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}]')\n",
        "        print(f'Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n",
        "\n",
        "        # Save checkpoint and best model\n",
        "        save_checkpoint(model, optimizer, epoch)\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            torch.save(model.state_dict(), 'best_inpainting.pth')\n",
        "\n",
        "        # Plot losses\n",
        "        plot_losses(train_losses, val_losses)\n",
        "\n"
      ],
      "metadata": {
        "id": "fQXoYnyNrDFg"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # Setup device\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    try:\n",
        "        # Setup data\n",
        "        train_loader, val_loader, test_loader = setup_data(img_size=256, batch_size=8)\n",
        "\n",
        "        # Visualize sample batch\n",
        "        visualize_sample_batch(train_loader)\n",
        "\n",
        "        # Create and train model\n",
        "        model = InpaintingNet()\n",
        "        train_model(model, train_loader, val_loader, num_epochs=10, device=device)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in main: {str(e)}\")\n",
        "        raise"
      ],
      "metadata": {
        "id": "CB1WlcB9tb4R"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h4zJgRcJrRoE",
        "outputId": "f744f8fc-72b7-4e0e-8cab-034f376ec618"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Base path: /root/.cache/kagglehub/datasets/jessicali9530/caltech256/versions/2\n",
            "Image directory: /root/.cache/kagglehub/datasets/jessicali9530/caltech256/versions/2/256_ObjectCategories\n",
            "\n",
            "Loaded dataset with 30607 images\n",
            "Train size: 21424\n",
            "Val size: 4591\n",
            "Test size: 4592\n",
            "Sample batch visualization saved to sample_batch.png\n",
            "No checkpoint found for caltech256-fastInpaint, starting from epoch 0\n",
            "Epoch [1/10]\n",
            "Train Loss: 0.1731, Val Loss: 0.1107\n",
            "Checkpoint saved for caltech256-fastInpaint at epoch 0\n",
            "Epoch [2/10]\n",
            "Train Loss: 0.1025, Val Loss: 0.0902\n",
            "Checkpoint saved for caltech256-fastInpaint at epoch 1\n",
            "Epoch [3/10]\n",
            "Train Loss: 0.0873, Val Loss: 0.0858\n",
            "Checkpoint saved for caltech256-fastInpaint at epoch 2\n",
            "Epoch [4/10]\n",
            "Train Loss: 0.0800, Val Loss: 0.0743\n",
            "Checkpoint saved for caltech256-fastInpaint at epoch 3\n",
            "Epoch [5/10]\n",
            "Train Loss: 0.0741, Val Loss: 0.0701\n",
            "Checkpoint saved for caltech256-fastInpaint at epoch 4\n",
            "Epoch [6/10]\n",
            "Train Loss: 0.0701, Val Loss: 0.0709\n",
            "Checkpoint saved for caltech256-fastInpaint at epoch 5\n",
            "Epoch [7/10]\n",
            "Train Loss: 0.0671, Val Loss: 0.0663\n",
            "Checkpoint saved for caltech256-fastInpaint at epoch 6\n",
            "Epoch [8/10]\n",
            "Train Loss: 0.0665, Val Loss: 0.0645\n",
            "Checkpoint saved for caltech256-fastInpaint at epoch 7\n",
            "Epoch [9/10]\n",
            "Train Loss: 0.0633, Val Loss: 0.0603\n",
            "Checkpoint saved for caltech256-fastInpaint at epoch 8\n",
            "Epoch [10/10]\n",
            "Train Loss: 0.0610, Val Loss: 0.0610\n",
            "Checkpoint saved for caltech256-fastInpaint at epoch 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def visualize_batch(original_images, masked_images, outputs, epoch, batch_idx, save_dir='visualization'):\n",
        "    \"\"\"\n",
        "    Enhanced visualization function for displaying original, masked, and reconstructed images\n",
        "    with improved layout and titles\n",
        "    \"\"\"\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    # Denormalize images\n",
        "    def denormalize(tensor):\n",
        "        return tensor.cpu().detach().numpy() * 0.5 + 0.5\n",
        "\n",
        "    # Convert tensors to numpy arrays and denormalize\n",
        "    original_images = denormalize(original_images)\n",
        "    masked_images = denormalize(masked_images)\n",
        "    outputs = denormalize(outputs)\n",
        "\n",
        "    # Number of images to display\n",
        "    n_images = min(4, original_images.shape[0])\n",
        "\n",
        "    # Create a figure with better spacing and size\n",
        "    fig = plt.figure(figsize=(15, 12))\n",
        "    plt.subplots_adjust(hspace=0.3)\n",
        "\n",
        "    # Add a main title\n",
        "    fig.suptitle(f'Image Inpainting Results - Epoch {epoch}, Batch {batch_idx}',\n",
        "                 fontsize=16, y=0.95)\n",
        "\n",
        "    for idx in range(n_images):\n",
        "        # Original image\n",
        "        plt.subplot(3, n_images, idx + 1)\n",
        "        plt.imshow(np.transpose(original_images[idx], (1, 2, 0)))\n",
        "        plt.axis('off')\n",
        "        if idx == 0:\n",
        "            plt.title('Original Images', pad=10)\n",
        "\n",
        "        # Masked image\n",
        "        plt.subplot(3, n_images, n_images + idx + 1)\n",
        "        plt.imshow(np.transpose(masked_images[idx], (1, 2, 0)))\n",
        "        plt.axis('off')\n",
        "        if idx == 0:\n",
        "            plt.title('Masked Images', pad=10)\n",
        "\n",
        "        # Reconstructed image\n",
        "        plt.subplot(3, n_images, 2*n_images + idx + 1)\n",
        "        plt.imshow(np.transpose(outputs[idx], (1, 2, 0)))\n",
        "        plt.axis('off')\n",
        "        if idx == 0:\n",
        "            plt.title('Reconstructed Images', pad=10)\n",
        "\n",
        "    # Save with high DPI for better quality\n",
        "    plt.savefig(f'{save_dir}/inpainting_results_epoch_{epoch}_batch_{batch_idx}.png',\n",
        "                dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "def create_random_mask(image, min_size=0.2, max_size=0.4):\n",
        "    \"\"\"\n",
        "    Create random rectangular masks with configurable size ranges\n",
        "    \"\"\"\n",
        "    batch_size, _, height, width = image.shape\n",
        "    mask = torch.ones_like(image)\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        # Calculate mask size as a proportion of image dimensions\n",
        "        h = int(torch.randint(\n",
        "            int(height * min_size),\n",
        "            int(height * max_size),\n",
        "            (1,)\n",
        "        ).item())\n",
        "        w = int(torch.randint(\n",
        "            int(width * min_size),\n",
        "            int(width * max_size),\n",
        "            (1,)\n",
        "        ).item())\n",
        "\n",
        "        # Random position\n",
        "        top = torch.randint(0, height - h, (1,)).item()\n",
        "        left = torch.randint(0, width - w, (1,)).item()\n",
        "\n",
        "        # Apply mask\n",
        "        mask[i, :, top:top+h, left:left+w] = 0\n",
        "\n",
        "    return mask\n",
        "\n",
        "def plot_losses(train_losses, val_losses, save_dir='visualization'):\n",
        "    \"\"\"\n",
        "    Enhanced loss visualization with improved styling\n",
        "    \"\"\"\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # Plot losses\n",
        "    plt.plot(train_losses, 'b-', label='Training Loss', linewidth=2)\n",
        "    plt.plot(val_losses, 'r-', label='Validation Loss', linewidth=2)\n",
        "\n",
        "    # Add grid\n",
        "    plt.grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "    # Labels and title\n",
        "    plt.xlabel('Epoch', fontsize=12)\n",
        "    plt.ylabel('Loss', fontsize=12)\n",
        "    plt.title('Training and Validation Losses Over Time', fontsize=14, pad=20)\n",
        "\n",
        "    # Legend\n",
        "    plt.legend(fontsize=10)\n",
        "\n",
        "    # Save plot\n",
        "    plt.savefig(f'{save_dir}/loss_plot.png', dpi=300, bbox_inches='tight')\n",
        "    plt.close()"
      ],
      "metadata": {
        "id": "LmV4RWPz_Kdd"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "thKsZ5dQ_Sno",
        "outputId": "0637a105-e583-4d93-e784-b70ad8a7f6b1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Base path: /root/.cache/kagglehub/datasets/jessicali9530/caltech256/versions/2\n",
            "Image directory: /root/.cache/kagglehub/datasets/jessicali9530/caltech256/versions/2/256_ObjectCategories\n",
            "\n",
            "Loaded dataset with 30607 images\n",
            "Train size: 21424\n",
            "Val size: 4591\n",
            "Test size: 4592\n",
            "Sample batch visualization saved to sample_batch.png\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-ad4e4902d78a>:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(ckpt_path)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint loaded for caltech256-fastInpaint, resuming from epoch 10\n"
          ]
        }
      ]
    }
  ]
}