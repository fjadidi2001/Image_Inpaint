{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPjNpmsXzxPFQmvDxaZ9geA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fjadidi2001/Image_Inpaint/blob/main/FastInpaint_Jan14.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries"
      ],
      "metadata": {
        "id": "oUj1jZcEwJSQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, random_split, DataLoader\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import kagglehub\n",
        "from google.colab import drive\n",
        "import torchvision.datasets as datasets\n"
      ],
      "metadata": {
        "id": "4Xjz9lK6ljsc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b254f29-dd4e-4367-c18b-2f597cac03c0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.11)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.27.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.12.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"jessicali9530/caltech256\")\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WzStjkBJmH3G",
        "outputId": "a2cf8f3c-1284-45de-b11c-ebec07eb126b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/jessicali9530/caltech256?dataset_version_number=2...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2.12G/2.12G [00:30<00:00, 75.4MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/jessicali9530/caltech256/versions/2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model name and setup\n",
        "model_name = \"caltech256-fastInpaint\"\n",
        "drive.mount('/content/drive')\n",
        "CHECKPOINTS_DIR = '/content/drive/MyDrive/ckpts'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LNTseq9-ngH-",
        "outputId": "e52d5467-b6e8-4a12-cd90-69dbad7e1eaf"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_data(img_size=256, batch_size=8):\n",
        "    \"\"\"Setup data loaders for Caltech256\"\"\"\n",
        "    # Download dataset\n",
        "    base_path = kagglehub.dataset_download(\"jessicali9530/caltech256\")\n",
        "    image_dir = os.path.join(base_path, '256_ObjectCategories')\n",
        "\n",
        "    print(f\"Base path: {base_path}\")\n",
        "    print(f\"Image directory: {image_dir}\")\n",
        "\n",
        "    # Define transforms\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((img_size, img_size)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "    ])\n",
        "\n",
        "    # Create dataset\n",
        "    dataset = datasets.ImageFolder(root=image_dir, transform=transform)\n",
        "    print(f\"\\nLoaded dataset with {len(dataset)} images\")\n",
        "\n",
        "    # Split dataset\n",
        "    train_size = int(0.7 * len(dataset))\n",
        "    val_size = int(0.15 * len(dataset))\n",
        "    test_size = len(dataset) - train_size - val_size\n",
        "\n",
        "    # Create splits\n",
        "    train_dataset, val_dataset, test_dataset = random_split(\n",
        "        dataset, [train_size, val_size, test_size]\n",
        "    )\n",
        "\n",
        "    # Create data loaders\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=2\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=batch_size,\n",
        "        num_workers=2\n",
        "    )\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=batch_size,\n",
        "        num_workers=2\n",
        "    )\n",
        "\n",
        "    print(f\"Train size: {len(train_dataset)}\")\n",
        "    print(f\"Val size: {len(val_dataset)}\")\n",
        "    print(f\"Test size: {len(test_dataset)}\")\n",
        "\n",
        "    return train_loader, val_loader, test_loader\n"
      ],
      "metadata": {
        "id": "WlT-CzEPxIK9"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_batch(original_images, masked_images, outputs, epoch, batch_idx, save_dir='visualization'):\n",
        "    \"\"\"Visualize a batch of images: original, masked, and reconstructed\"\"\"\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    # Convert tensors to numpy arrays and move to CPU if needed\n",
        "    original_images = original_images.cpu().detach().numpy()\n",
        "    masked_images = masked_images.cpu().detach().numpy()\n",
        "    outputs = outputs.cpu().detach().numpy()\n",
        "\n",
        "    # Create a figure with three rows: original, masked, and reconstructed\n",
        "    fig, axes = plt.subplots(3, min(4, original_images.shape[0]), figsize=(15, 10))\n",
        "\n",
        "    for i in range(min(4, original_images.shape[0])):\n",
        "        # Original\n",
        "        axes[0, i].imshow(np.transpose(original_images[i], (1, 2, 0)) * 0.5 + 0.5)\n",
        "        axes[0, i].axis('off')\n",
        "        if i == 0:\n",
        "            axes[0, i].set_title('Original')\n",
        "\n",
        "        # Masked\n",
        "        axes[1, i].imshow(np.transpose(masked_images[i], (1, 2, 0)) * 0.5 + 0.5)\n",
        "        axes[1, i].axis('off')\n",
        "        if i == 0:\n",
        "            axes[1, i].set_title('Masked')\n",
        "\n",
        "        # Reconstructed\n",
        "        axes[2, i].imshow(np.transpose(outputs[i], (1, 2, 0)) * 0.5 + 0.5)\n",
        "        axes[2, i].axis('off')\n",
        "        if i == 0:\n",
        "            axes[2, i].set_title('Reconstructed')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{save_dir}/epoch_{epoch}_batch_{batch_idx}.png')\n",
        "    plt.close()"
      ],
      "metadata": {
        "id": "ZLYEOhmVnnFm"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_sample_batch(data_loader, save_path='sample_batch.png'):\n",
        "    \"\"\"Visualize and save a sample batch of images\"\"\"\n",
        "    # Get a batch of images\n",
        "    images = next(iter(data_loader))[0]  # [0] because ImageFolder returns (images, labels)\n",
        "\n",
        "    # Create figure\n",
        "    fig, axes = plt.subplots(2, 4, figsize=(15, 8))\n",
        "    axes = axes.ravel()\n",
        "\n",
        "    for idx, img in enumerate(images[:8]):  # Show first 8 images\n",
        "        # Convert tensor to numpy and denormalize\n",
        "        img_np = img.numpy().transpose(1, 2, 0) * 0.5 + 0.5\n",
        "        axes[idx].imshow(img_np)\n",
        "        axes[idx].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path)\n",
        "    plt.close()\n",
        "    print(f\"Sample batch visualization saved to {save_path}\")"
      ],
      "metadata": {
        "id": "qfFs334hx8e-"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_checkpoint(model, optimizer, epoch):\n",
        "    os.makedirs(CHECKPOINTS_DIR, exist_ok=True)\n",
        "    checkpoint_path = f'{CHECKPOINTS_DIR}/{model_name}.pth'\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "    }, checkpoint_path)\n",
        "    print(f\"Checkpoint saved for {model_name} at epoch {epoch}\")"
      ],
      "metadata": {
        "id": "vkpJrUKZqFyW"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_checkpoint(model, optimizer):\n",
        "    ckpt_path = f'{CHECKPOINTS_DIR}/{model_name}.pth'\n",
        "    if not os.path.exists(ckpt_path):\n",
        "        print(f\"No checkpoint found for {model_name}, starting from epoch 0\")\n",
        "        return 0\n",
        "\n",
        "    checkpoint = torch.load(ckpt_path)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    start_epoch = checkpoint['epoch'] + 1\n",
        "    print(f\"Checkpoint loaded for {model_name}, resuming from epoch {start_epoch}\")\n",
        "    return start_epoch"
      ],
      "metadata": {
        "id": "87WbbJl7qOvb"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(channels, channels, 3, padding=1)\n",
        "        self.norm1 = nn.InstanceNorm2d(channels)\n",
        "        self.conv2 = nn.Conv2d(channels, channels, 3, padding=1)\n",
        "        self.norm2 = nn.InstanceNorm2d(channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        x = F.relu(self.norm1(self.conv1(x)))\n",
        "        x = self.norm2(self.conv2(x))\n",
        "        x += residual\n",
        "        x = F.relu(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "WeTBJj7SyaDl"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class InpaintingNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(InpaintingNet, self).__init__()\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(6, 64, 7, padding=3),\n",
        "            nn.InstanceNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Conv2d(64, 128, 4, stride=2, padding=1),\n",
        "            nn.InstanceNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Conv2d(128, 256, 4, stride=2, padding=1),\n",
        "            nn.InstanceNorm2d(256),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        # Middle blocks\n",
        "        self.middle = nn.Sequential(*[ResidualBlock(256) for _ in range(6)])\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(256, 128, 4, stride=2, padding=1),\n",
        "            nn.InstanceNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),\n",
        "            nn.InstanceNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Conv2d(64, 3, 7, padding=3),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        x = torch.cat([x, mask], dim=1)\n",
        "        x = self.encoder(x)\n",
        "        x = self.middle(x)\n",
        "        x = self.decoder(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "7kjWjfXnqZRB"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_random_mask(image):\n",
        "    \"\"\"Create random rectangular masks\"\"\"\n",
        "    batch_size, _, height, width = image.shape\n",
        "    mask = torch.ones_like(image)\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        h = torch.randint(height//4, height//2, (1,)).item()\n",
        "        w = torch.randint(width//4, width//2, (1,)).item()\n",
        "\n",
        "        top = torch.randint(0, height - h, (1,)).item()\n",
        "        left = torch.randint(0, width - w, (1,)).item()\n",
        "\n",
        "        mask[i, :, top:top+h, left:left+w] = 0\n",
        "\n",
        "    return mask"
      ],
      "metadata": {
        "id": "Sbp52-hmqti6"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_losses(train_losses, val_losses, save_dir='visualization'):\n",
        "    \"\"\"Plot training and validation losses\"\"\"\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(train_losses, label='Training Loss')\n",
        "    plt.plot(val_losses, label='Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Validation Losses')\n",
        "    plt.legend()\n",
        "    plt.savefig(f'{save_dir}/losses.png')\n",
        "    plt.close()"
      ],
      "metadata": {
        "id": "IVmfbHD5q63Q"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, val_loader, num_epochs=30, device='cuda'):\n",
        "    model = model.to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    criterion = nn.L1Loss()\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3)\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    # Load checkpoint if exists\n",
        "    start_epoch = load_checkpoint(model, optimizer)\n",
        "\n",
        "    for epoch in range(start_epoch, num_epochs):\n",
        "        # Training\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        for batch_idx, (images, _) in enumerate(train_loader): # Access images directly from the tuple\n",
        "            images = images.to(device)\n",
        "            masks = create_random_mask(images).to(device)\n",
        "            masked_images = images * masks\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(masked_images, masks)\n",
        "            loss = criterion(outputs, images)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "            # Visualize every 100 batches\n",
        "            if batch_idx % 100 == 0:\n",
        "                visualize_batch(images, masked_images, outputs, epoch, batch_idx)\n",
        "\n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "        train_losses.append(avg_train_loss)\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, (images, _) in enumerate(val_loader): # unpack the batch\n",
        "                images = images.to(device) # images is the tensor you need\n",
        "                masks = create_random_mask(images).to(device)\n",
        "                masked_images = images * masks\n",
        "\n",
        "                outputs = model(masked_images, masks)\n",
        "                loss = criterion(outputs, images)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "        val_losses.append(avg_val_loss)\n",
        "\n",
        "        # Update learning rate\n",
        "        scheduler.step(avg_val_loss)\n",
        "\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}]')\n",
        "        print(f'Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n",
        "\n",
        "        # Save checkpoint and best model\n",
        "        save_checkpoint(model, optimizer, epoch)\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            torch.save(model.state_dict(), 'best_inpainting.pth')\n",
        "\n",
        "        # Plot losses\n",
        "        plot_losses(train_losses, val_losses)\n",
        "\n"
      ],
      "metadata": {
        "id": "fQXoYnyNrDFg"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # Setup device\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    try:\n",
        "        # Setup data\n",
        "        train_loader, val_loader, test_loader = setup_data(img_size=256, batch_size=8)\n",
        "\n",
        "        # Visualize sample batch\n",
        "        visualize_sample_batch(train_loader)\n",
        "\n",
        "        # Create and train model\n",
        "        model = InpaintingNet()\n",
        "        train_model(model, train_loader, val_loader, num_epochs=10, device=device)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in main: {str(e)}\")\n",
        "        raise"
      ],
      "metadata": {
        "id": "CB1WlcB9tb4R"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "id": "h4zJgRcJrRoE",
        "outputId": "5ba14b79-d9f2-4b13-f6d8-705b2f765198"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Base path: /root/.cache/kagglehub/datasets/jessicali9530/caltech256/versions/2\n",
            "Image directory: /root/.cache/kagglehub/datasets/jessicali9530/caltech256/versions/2/256_ObjectCategories\n",
            "\n",
            "Loaded dataset with 30607 images\n",
            "Train size: 21424\n",
            "Val size: 4591\n",
            "Test size: 4592\n",
            "Sample batch visualization saved to sample_batch.png\n",
            "No checkpoint found for caltech256-fastInpaint, starting from epoch 0\n",
            "Error in main: list indices must be integers or slices, not str\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "list indices must be integers or slices, not str",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-972361fa1b80>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-25-ef4f4b01e4a1>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m# Create and train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInpaintingNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-2e49063879f6>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, num_epochs, device)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m                 \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pixel_values'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m                 \u001b[0mmasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_random_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m                 \u001b[0mmasked_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
          ]
        }
      ]
    }
  ]
}